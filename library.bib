@article{Badin2017,
   abstract = {“Neuronal assemblies” are defined here as coalitions within the brain of millions of neurons extending in space up to 1-2 mm, and lasting for hundreds of milliseconds: as such they could potentially link bottom-up, micro-scale with top-down, macro-scale events. The perspective first compares the features in vitro versus in vivo of this underappreciated “meso-scale” level of brain processing, secondly considers the various diverse functions in which assemblies may play a pivotal part, and thirdly analyses whether the surprisingly spatially extensive and prolonged temporal properties of assemblies can be described exclusively in terms of classic synaptic transmission or whether additional, different types of signaling systems are likely to operate. Based on our own voltage-sensitive dye imaging (VSDI) data acquired in vitro we show how restriction to only one signaling process, i.e., synaptic transmission, is unlikely to be adequate for modeling the full profile of assemblies. Based on observations from VSDI with its protracted spatio-temporal scales, we suggest that two other, distinct processes are likely to play a significant role in assembly dynamics: “volume” transmission (the passive diffusion of diverse bioactive transmitters, hormones, and modulators), as well as electrotonic spread via gap junctions. We hypothesize that a combination of all three processes has the greatest potential for deriving a realistic model of assemblies and hence elucidating the various complex brain functions that they may mediate.},
   author = {Antoine Scott Badin and Francesco Fermani and Susan A. Greenfield},
   doi = {10.3389/fncir.2016.00114},
   issn = {16625110},
   journal = {Frontiers in Neural Circuits},
   keywords = {Gap junctions,Neuronal assemblies,Synaptic transmission,Volume transmission},
   month = {1},
   pmid = {28119576},
   publisher = {Frontiers Media S.A.},
   title = {The features and functions of neuronal assemblies: Possible dependency on mechanisms beyond synaptic transmission},
   volume = {10},
   year = {2017},
}
@article{,
   abstract = {The architecture of cortex is flexible, permitting neuronal networks to store recent sensory experiences as specific synaptic connectivity patterns. However, it is unclear how these patterns are maintained in the face of the high spike time variability associated with cortex. Here we demonstrate, using a large-scale cortical network model, that realistic synaptic plasticity rules coupled with homeostatic mechanisms lead to the formation of neuronal assemblies that reflect previously experienced stimuli. Further, reverberation of past evoked states in spontaneous spiking activity stabilizes, rather than erases, this learned architecture. Spontaneous and evoked spiking activity contains a signature of learned assembly structures, leading to testable predictions about the effect of recent sensory experience on spike train statistics. Our work outlines requirements for synaptic plasticity rules capable of modifying spontaneous dynamics and shows that this modification is beneficial for stability of learned network architectures.},
   author = {Ashok Litwin-Kumar and Brent Doiron},
   doi = {10.1038/ncomms6319},
   issn = {20411723},
   journal = {Nature Communications},
   month = {11},
   pmid = {25395015},
   publisher = {Nature Publishing Group},
   title = {Formation and maintenance of neuronal assemblies through synaptic plasticity},
   volume = {5},
   year = {2014},
}
@report{,
   abstract = {We study theoretically how an interaction between assemblies of neu-ronal oscillators can be modulated by the pattern of external stimuli. It is shown that spatial variations in the stimuli can control the magnitude and phase of the synchronization between the output of neurons with different receptive fields. This modulation emerges from cooperative dynamics in the network, without the need for specialized, activity-dependent synapses. Our results further suggest that the modulation of neuronal interactions by extended features of a stimulus may give rise to complex spatiotemporal fluctuations in the phases of neuronal oscillations.},
   author = {E R Grannan D Kleinfeld and H Sompolinsky},
   title = {Stimulus-Dependent Synchronization of Neuronal Assemblies},
   url = {http://direct.mit.edu/neco/article-pdf/5/4/550/812503/neco.1993.5.4.550.pdf},
}
@report{Grinvald2003,
   abstract = {Spontaneous cortical activity of single neurons is often either dismissed as noise, or is regarded as carrying no functional significance and hence is ignored. Our findings suggest that such concepts should be revised. We explored the coherent population activity of neuronal assemblies in primary sensory area in the absence of a sensory input. Recent advances in real-time optical imaging based on voltage-sensitive dyes (VSDI) have facilitated exploration of population activity and its intimate relationship to the activity of individual cortical neurons. It has been shown by in vivo intracellular recordings that the dye signal measures the sum of the membrane potential changes in all the neuronal elements in the imaged area, emphasizing subthreshold synaptic potentials and dendritic action potentials in neuronal arborizations originating from neurons in all cortical layers whose dendrites reach the superficial cortical layers. Thus, the VSDI has allowed us to image the rather illusive activity in neuronal dendrites that cannot be readily explored by single unit recordings. Surprisingly, we found that the amplitude of this type of ongoing subthreshold activity is of the same order of magnitude as evoked activity. We also found that this ongoing activity exhibited high synchronization over many millimeters of cortex. We then investigated the influence of ongoing activity on the evoked response, and showed that the two interact strongly. Furthermore, we found that cortical states that were previously associated only with evoked activity can actually be observed also in the absence of stimulation, for example, the cortical representation of a given *This review, a summary of my lecture "Seeing the Brain in Action" at the Memorial, is dedicated to the memory of Prof. Shneior Lifson. Professor Lifson was the Chairman when I was a Ph.D. student in the Department of Chemical Physics; I enjoyed his unique personality and leadership. Shneior's exceptional behavior, charm, and unique personality originated from his unique brain. His mentor image has continued to be a source of inspiration in our studies of the mysteries underlying brain function. In recent years Prof. Lifson has studied evolution, and during our scientific interactions he was fascinated with the most impressive of evolution's product, the human brain with its remarkable performance. Thus, I dedicate this review on the brain to Shneior Lifson, who had a unique brain; therefore, his memory will vividly remain in our minds forever.},
   author = {Amiram Grinvald and Amos Arieli and Misha Tsodyks and Tal Kenet},
   journal = {Biopolymers},
   pages = {422-436},
   title = {Neuronal Assemblies: Single Cortical Neurons Are Obedient Members of a Huge Orchestra*},
   volume = {68},
   year = {2003},
}

@article{Singer1995,
   author = {W Singer and C M Gray},
   doi = {10.1146/annurev.ne.18.030195.003011},
   issn = {0147-006X},
   issue = {1},
   journal = {Annual Review of Neuroscience},
   month = {3},
   pages = {555-586},
   title = {Visual Feature Integration and the Temporal Correlation Hypothesis},
   volume = {18},
   url = {http://www.annualreviews.org/doi/10.1146/annurev.ne.18.030195.003011},
   year = {1995},
}
@article{Singer1999,
   abstract = {Deutschordenstrasse 46 60528 Frankfurt from multiple units are considered. These include multi-Federal Republic of Germany electrode recordings from multiple individual cells, but also measurements of local field potentials (LFPs) and electroencephalographic (EEG) or magnetoencephalo-graphic (MEG) recordings. The signals of these latter Most of our knowledge about the functional organization methods reflect the average activity of large cell popula-of neuronal systems is based on the analysis of the firing tions. Because this activity leads to measurable signal patterns of individual neurons that have been recorded fluctuations only if it is sufficiently synchronized, these one by one in succession. This approach permits as-global recordings provide valuable information about sessment of event-related variations in discharge rate, the temporal relations between responses. but it precludes detection of any covariations in the amplitude or timing of distributed responses if these Two Complementary Binding Strategies covariations result from internal neuronal interactions Discussions about the putative functional role of syn-rather than from time locking to stimulus or motor chrony focus on the question of whether it can serve as events. a mechanism to bind distributed neuronal activity. To As it is likely that internal coordination of distributed provide an adequate background for the examination of responses is functionally as relevant as stimulus-this question, I shall first deal with some general, imple-induced coordination, multielectrode recordings are in-mentation-independent aspects of binding operations. creasingly being used to analyze internally generated As the Gestalt psychologists pointed out, our cogni-covariations of firing patterns. More than a decade ago, we used this method to reveal that neurons in the visual tive systems have the tendency to interpret objects and cortex tend to synchronize their discharges with a preci-events as related if they are contiguous in space or time, sion in the millisecond range when activated with a sin-or if they exhibit similarities in certain feature domains. gle contour (Gray and Singer, 1987, Soc. Neurosci., ab-Thus, contours that touch one another, have similar con-stract; 1989), whereas they fail to do so when activated trast, or move with the same speed in the same direction by different contours moving in different directions (Gray (common fate) are more likely to be perceived as compo-et al., 1989; Engel et al., 1991c). In addition, these stimu-nents of the same object than spatially distant contours lus-induced, context-dependent synchronization phe-or contours that have no features in common. Likewise, nomena were found to be associated with a conspicu-events that coincide in time are interpreted with greater ous oscillatory modulation of cell firing in a frequency probability as related than events separated in time. At range between 30 and 50 Hz, the so-called frequency early stages of sensory processing, spatial relations and range. Two aspects make this synchronization interest-relations in feature space are represented by the ampli-ing. First, it results from internal coordination of spike tude and the topological relations of activation foci in timing and is not simply caused by stimulus-locked ordered maps. Temporal relations, however, are repre-changes in discharge rate. Second, synchronization sented by the relative timing of responses. In order to probability changes in a systematic way when the per-accomplish perceptual grouping, the distributed re-ceptual coherence of stimulus constellations is modified. sponses of feature-selective cells need to be bound Thus, this type of synchrony is not a trivial reflection of together at some stage of processing. Evidence indi-anatomical connectivity such as shared input through bi-cates that this is achieved in two complementary ways. furcating axons, but instead results from context-depen-One strategy is binding of responses by convergence dent, dynamic interactions within the cortical network. of axonal projections. Axons of cells whose responses The evidence for an internal coordination of spike should be bound are made to converge onto a common timing raises the question of whether it serves a function target cell at the next-higher processing level. If the in cortical processing or whether it is merely an epiphe-threshold of this binding unit is appropriately adjusted, nomenon. The goal of this paper is to review theoretical its response signals the specific conjunction of features arguments and data that relate to this issue. The first to which the feeding cells are tuned. We shall address part examines how well the nervous system, and in par-this grouping strategy as "binding by convergence" or ticular the cerebral cortex, can distinguish between syn-"binding by conjunction cells." This coding principle is chronous and asynchronous responses, and whether also known as "labeled line coding" because the re-any significance is attributed to precisely synchronized sponses of a given unit have a fixed label attached to discharge patterns when these are coordinated by ex-them; they always signal the same conjunction of input ternal events, e.g., by the synchronous onset of sensory signals. The complementary strategy for response bind-stimuli. In the second part, data are reviewed from ex-ing relies on dynamic selection and grouping of re-periments that were designed to examine putative func-sponses. Here, responses are bound by jointly enhanc-tions of internally generated synchronization. As the assessment of internally generated, non-stimulus-locked ing their saliency relative to other, nonbound responses. Enhanced responses have a stronger impact on downstream processes than nonenhanced responses and * E-mail: singer@mpih-frankfurt.mpg.de.},
   author = {Wolf Singer},
   doi = {10.1016/S0896-6273(00)80821-1},
   issn = {08966273},
   issue = {1},
   journal = {Neuron},
   month = {9},
   pages = {49-65},
   title = {Neuronal Synchrony: A Versatile Code for the Definition of Relations?},
   volume = {24},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627300808211},
   year = {1999},
}

@article{Lestienne1996,
   author = {Rémy Lestienne},
   doi = {10.1007/BF00199137},
   issn = {0340-1200},
   issue = {1},
   journal = {Biological Cybernetics},
   month = {1},
   title = {Determination of the precision of spike timing in the visual cortex of anaesthetised cats},
   volume = {74},
   year = {1996},
}

@article{Bialek1991,
   abstract = {Traditional approaches to neural coding characterize the encoding of known stimuli in average neural responses. Organisms face nearly the opposite task-extracting information about an unknown time-dependent stimulus from short segments of a spike train. Here the neural code was characterized from the point of view of the organism, clminating in algorithms for real-time stimulus estimation based on a single example of the spike train. These methods were applied to an identified movement-sensitive neuron in the fly visual system. Such decoding experiments determined the effective noise level and fault tolerance of neural computation, and the structure of the decoding algorithms suggested a simple model for real-time analog signal processing with spiking neurons.},
   author = {William Bialek and Fred Rieke and Rob R. De Ruyter Van Steveninck and David Warland},
   doi = {10.1126/science.2063199},
   issn = {00368075},
   issue = {5014},
   journal = {Science},
   pages = {1854-1857},
   pmid = {2063199},
   title = {Reading a neural code},
   volume = {252},
   year = {1991},
}

@article{buracas1998,
   author = {Giedrius T. Burac̆as and Anthony M. Zador and Michael R. DeWeese and Thomas D. Albright},
   doi = {10.1016/S0896-6273(00)80477-8},
   issn = {08966273},
   issue = {5},
   journal = {Neuron},
   month = {5},
   title = {Efficient Discrimination of Temporal Patterns by Motion-Sensitive Neurons in Primate Visual Cortex},
   volume = {20},
   year = {1998},
}
@article{Bair1996,
   abstract = {<p>How reliably do action potentials in cortical neurons encode information about a visual stimulus? Most physiological studies do not weigh the occurrences of particular action potentials as significant but treat them only as reflections of average neuronal excitation. We report that single neurons recorded in a previous study by Newsome et al. (1989; see also Britten et al. 1992) from cortical area MT in the behaving monkey respond to dynamic and unpredictable motion stimuli with a markedly reproducible temporal modulation that is precise to a few milliseconds. This temporal modulation is stimulus dependent, being present for highly dynamic random motion but absent when the stimulus translates rigidly.</p>},
   author = {Wyeth Bair and Christof Koch},
   doi = {10.1162/neco.1996.8.6.1185},
   issn = {0899-7667},
   issue = {6},
   journal = {Neural Computation},
   month = {8},
   title = {Temporal Precision of Spike Trains in Extrastriate Cortex of the Behaving Macaque Monkey},
   volume = {8},
   year = {1996},
}
@article{Tovee1993,
   abstract = {<p>1. The possibility of temporal encoding in the spike trains of single neurons recorded in the temporal lobe visual cortical areas of rhesus macaques was analyzed with the use of principal component and information theory analyses of smoothed spike trains. The neurons analyzed had responses selective for faces. 2. Provided that a correction was applied to earlier methods of principal component analysis used for neuronal spike trains, it was shown that the first principal component provides by a great extent the most information, with the second and third adding only small proportions (on average 18.8 and 8.4%, respectively). 3. It was shown that the magnitude of the second and higher principal components is even smaller if the spike train analysis is started after the onset of the neuronal response, instead of before the neuronal response has started. This suggests that variations in response latency are at least a part of what is reflected by the second and higher principal components. 4. The first principal component was correlated with the mean firing rate of the neurons. The second and higher principal components reflected at least partly the onset properties of the neuronal responses, such as response latency differences between the stimuli. 5. A considerable proportion of the information available from principal components 1-3 is available in the firing rate of the neuron. 6. Periods of the firing rate of as little as 50 or even 20 ms are sufficient to give a reasonable estimate of the firing rate of the neuron. 7. Information theory analysis showed that in short epochs (e.g., 50 ms) the information available from the firing rate can be as high, on average, as 84.4% of that available from the firing rate calculated over 400 ms, and 52.0% of that available from principal components 1-3 in the 400-ms period. It was also found that 44.0% of the information calculated from the first three principal components is available in the firing rates calculated over epochs as short as 20 ms. 8. More information was available near the start of the neuronal response, and the information available from short epochs became less later in the neuronal response. 9. Taken together, these analyses provide evidence that a short period of firing taken close to the start of the neuronal response provides a reasonable proportion of the total information that would be available if a long period of neuronal firing (e.g., 400 ms) were utilized to extract it, even if temporal encoding were used.(ABSTRACT TRUNCATED AT 400 WORDS)</p>},
   author = {M. J. Tovee and E. T. Rolls and A. Treves and R. P. Bellis},
   doi = {10.1152/jn.1993.70.2.640},
   issn = {0022-3077},
   issue = {2},
   journal = {Journal of Neurophysiology},
   month = {8},
   title = {Information encoding and the responses of single neurons in the primate temporal visual cortex},
   volume = {70},
   year = {1993},
}
@article{Salzman1992,
   author = {CD Salzman and CM Murasugi and KH Britten and WT Newsome},
   doi = {10.1523/JNEUROSCI.12-06-02331.1992},
   issn = {0270-6474},
   issue = {6},
   journal = {The Journal of Neuroscience},
   month = {6},
   title = {Microstimulation in visual area MT: effects on direction discrimination performance},
   volume = {12},
   year = {1992},
}

@report{Koch1994,
   author = {Christof Koch and Joel L Davis and A Bradford Book and Patricia S Churchland and V S Ramachandran and Terrence J Sejnowski},
   title = {Large-Scale Neuronal Theories of the Brain edited by 2 A Critique of Pure Vision'},
   year = {1994},
}

@article{Iacaruso2017,
   abstract = {How a sensory stimulus is processed and perceived depends on the surrounding sensory scene. In the visual cortex, contextual signals can be conveyed by an extensive network of intra- and inter-areal excitatory connections that link neurons representing stimulus features separated in visual space. However, the connectional logic of visual contextual inputs remains unknown; it is not clear what information individual neurons receive from different parts of the visual field, nor how this input relates to the visual features that a neuron encodes, defined by its spatial receptive field. Here we determine the organization of excitatory synaptic inputs responding to different locations in the visual scene by mapping spatial receptive fields in dendritic spines of mouse visual cortex neurons using two-photon calcium imaging. We find that neurons receive functionally diverse inputs from extended regions of visual space. Inputs representing similar visual features from the same location in visual space are more likely to cluster on neighbouring spines. Inputs from visual field regions beyond the receptive field of the postsynaptic neuron often synapse on higher-order dendritic branches. These putative long-range inputs are more frequent and more likely to share the preference for oriented edges with the postsynaptic neuron when the receptive field of the input is spatially displaced along the axis of the receptive field orientation of the postsynaptic neuron. Therefore, the connectivity between neurons with displaced receptive fields obeys a specific rule, whereby they connect preferentially when their receptive fields are co-oriented and co-axially aligned. This organization of synaptic connectivity is ideally suited for the amplification of elongated edges, which are enriched in the visual environment, and thus provides a potential substrate for contour integration and object grouping.},
   author = {M. Florencia Iacaruso and Ioana T. Gasler and Sonja B. Hofer},
   doi = {10.1038/nature23019},
   issn = {14764687},
   issue = {7664},
   journal = {Nature},
   month = {7},
   pages = {449-452},
   pmid = {28700575},
   publisher = {Nature Publishing Group},
   title = {Synaptic organization of visual space in primary visual cortex},
   volume = {547},
   year = {2017},
}

@generic{London2005,
   abstract = {One of the central questions in neuroscience is how particular tasks, or computations, are implemented by neural networks to generate behavior. The prevailing view has been that information processing in neural networks results primarily from the properties of synapses and the connectivity of neurons within the network, with the intrinsic excitability of single neurons playing a lesser role. As a consequence, the contribution of single neurons to computation in the brain has long been underestimated. Here we review recent work showing that neuronal dendrites exhibit a range of linear and nonlinear mechanisms that allow them to implement elementary computations. We discuss why these dendritic properties may be essential for the computations performed by the neuron and the network and provide theoretical and experimental examples to support this view. Copyright © 2005 by Annual Reviews. All rights reserved.},
   author = {Michael London and Michael Häusser},
   doi = {10.1146/annurev.neuro.28.061604.135703},
   issn = {0147006X},
   journal = {Annual Review of Neuroscience},
   keywords = {Coding,Dendrites,Ion channels,Spikes,Synaptic integration},
   pages = {503-532},
   pmid = {16033324},
   title = {Dendritic computation},
   volume = {28},
   year = {2005},
}

@article{Peinado1993,
   author = {Alejandro Peinado and Rafael Yuste and Lawrence C. Katz},
   doi = {10.1093/cercor/3.5.488},
   issn = {1047-3211},
   issue = {5},
   journal = {Cerebral Cortex},
   title = {Gap Junctional Communication and the Development of Local Circuits in Neocortex},
   volume = {3},
   year = {1993},
}
@article{Yuste1992,
   author = {R Yuste and A Peinado and L. Katz},
   doi = {10.1126/science.1496379},
   issn = {0036-8075},
   issue = {5070},
   journal = {Science},
   month = {7},
   title = {Neuronal domains in developing neocortex},
   volume = {257},
   year = {1992},
}
@article{Fischbach1972,
   author = {Gerald D. Fischbach},
   doi = {10.1016/0012-1606(72)90023-1},
   issn = {00121606},
   issue = {2},
   journal = {Developmental Biology},
   month = {6},
   title = {Synapse formation between dissociated nerve and muscle cells in low density cell cultures},
   volume = {28},
   year = {1972},
}

@article{Furshpan1959,
   author = {E. J. Furshpan and D. D. Potter},
   doi = {10.1113/jphysiol.1959.sp006143},
   issn = {00223751},
   issue = {2},
   journal = {The Journal of Physiology},
   month = {3},
   title = {Transmission at the giant motor synapses of the crayfish},
   volume = {145},
   year = {1959},
}

@report{garcialopezp2010,
   abstract = {Ramon y Cajal's studies in the field of neuroscience provoked a radical change in the course of its history. For this reason he is considered as the father of modern neuroscience. Some of his original preparations are housed at the Cajal Museum (Cajal Institute, CSIC, Madrid, Spain). In this article, we catalogue and analyse more than 4,500 of Cajal's histological preparations, the same preparations he used during his scientific career. Furthermore, we catalogued Cajal's original correspondence, both manuscripts and personal letters, drawings and plates. This is the first time anyone has compiled an account of Cajal's enormous scientific production, offering some curious insights into his work and his legacy. © 2010 Garcia-Lopez, Garcia-Marin and Freire.},
   author = {Pablo Garcia-Lopez and Virginia Garcia-Marin and Miguel Freire},
   doi = {10.3389/neuro.05.009.2010},
   issn = {16625129},
   issue = {MARCH},
   journal = {Frontiers in Neuroanatomy},
   keywords = {Cajal,Drawings,Histological preparations},
   month = {3},
   title = {The histological slides and drawings of Cajal},
   year = {2010},
}
@book{Gerstner2014,
   author = {Wulfram Gerstner and Werner M. Kistler and Richard Naud and Liam Paninski},
   city = {Cambridge},
   doi = {10.1017/CBO9781107447615},
   isbn = {9781107447615},
   publisher = {Cambridge University Press},
   title = {Neuronal Dynamics},
   year = {2014},
}
@article{Tanimizu2018,
   abstract = {Object recognition memory allows discrimination of familiar and novel objects. Previous studies have shown the importance of several brain regions for object recognition memories; however, the mechanisms underlying the consolidation of object recognition (OR) memory at the anatomic level remain unknown. Here, we analyzed the brain network for the generation of OR memory in mice by measuring the expression of the immediate-early gene c-fos. We found that c-fos expression was induced in the hippocampus (CA1 and CA3 regions), insular cortex (IC), perirhinal cortex (PRh), and medial prefrontal cortex (mPFC) when OR memory was generated, suggesting that gene expression in these brain regions contributes to the formation of OR memory. Consistently, inhibition of protein synthesis in the mPFC blocked the formation of long-term OR memory. Importantly, network analyses suggested that the hippocampus, IC, PRh and mPFC show increased connectivity with other brain regions when OR memory is formed. Thus, we suggest that a brain network composed of the hippocampus, IC, PRh, and mPFC is required for the generation of OR memory by connecting with other brain regions.},
   author = {Toshiyuki Tanimizu and Kyohei Kono and Satoshi Kida},
   doi = {10.1016/j.brainresbull.2017.05.017},
   issn = {18732747},
   journal = {Brain Research Bulletin},
   keywords = {Brain network,Consolidation,Object recognition memory},
   month = {7},
   pages = {27-34},
   pmid = {28587862},
   publisher = {Elsevier Inc.},
   title = {Brain networks activated to form object recognition memory},
   volume = {141},
   year = {2018},
}
@article{,
   abstract = {Many cognitive and behavioral tasks—such as interval timing, spatial navigation, motor control, and speech—require the execution of precisely-timed sequences of neural activation that cannot be fully explained by a succession of external stimuli. We show how repeatable and reliable patterns of spatiotemporal activity can be generated in chaotic and noisy spiking recurrent neural networks. We propose a general solution for networks to autonomously produce rich patterns of activity by providing a multi-periodic oscillatory signal as input. We show that the model accurately learns a variety of tasks, including speech generation, motor control, and spatial navigation. Further, the model performs temporal rescaling of natural spoken words and exhibits sequential neural activity commonly found in experimental data involving temporal processing. In the context of spatial navigation, the model learns and replays compressed sequences of place cells and captures features of neural activity such as the emergence of ripples and theta phase precession. Together, our findings suggest that combining oscillatory neuronal inputs with different frequencies provides a key mechanism to generate precisely timed sequences of activity in recurrent circuits of the brain.},
   author = {Philippe Vincent-Lamarre and Matias Calderini and Jean Philippe Thivierge},
   doi = {10.3389/fncom.2020.00078},
   issn = {16625188},
   journal = {Frontiers in Computational Neuroscience},
   keywords = {balanced networks,neural oscillations,recurrent neural networks,spiking neural networks,temporal processing},
   month = {9},
   publisher = {Frontiers Media S.A.},
   title = {Learning Long Temporal Sequences in Spiking Networks by Multiplexing Neural Oscillations},
   volume = {14},
   year = {2020},
}
@article{,
   abstract = {The contrast sensitivity function (CSF), how sensitivity varies with the frequency of the stimulus, is a fundamental assessment of visual performance. The CSF is generally assumed to be determined by low-level sensory processes. However, the spatial sensitivities of neurons in the early visual pathways, as measured in experiments with immobilized eyes, diverge from psychophysical CSF measurements in primates. Under natural viewing conditions, as in typical psychophysical measurements, humans continually move their eyes even when looking at a fixed point. Here, we show that the resulting transformation of the spatial scene into temporal modulations on the retina constitutes a processing stage that reconciles human CSF and the response characteristics of retinal ganglion cells under a broad range of conditions. Our findings suggest a fundamental integration between perception and action: eye movements work synergistically with the spatio-temporal sensitivities of retinal neurons to encode spatial information.},
   author = {Antonino Casile and Jonathan D Victor and Michele Rucci},
   doi = {10.7554/eLife.40924.001},
   title = {Contrast sensitivity reveals an oculomotor strategy for temporally encoding space},
   url = {https://doi.org/10.7554/eLife.40924.001},
}
@article{Ahissar2012,
   abstract = {During natural viewing, the eyes are never still. Even during fixation, miniature movementsof the eyes move the retinal image across tens of foveal photoreceptors. Most theories of vision implicitly assume that the visual system ignores these movements and somehow overcomes the resulting smearing. However, evidence has accumulated to indicate that fixational eye movements cannot be ignored by the visual system if fine spatial details are to be resolved. We argue that the only way the visual system can achieve its high resolution given its fixational movements is by seeing via these movements. Seeing via eye movements also eliminates the instability of the image, which would be induced by them otherwise. Here we present a hypothesis for vision, in which coarse details are spatially-encoded in gazerelated coordinates, and fine spatial details are temporally-encoded in relative retinal coordinates. The temporal encoding presented here achieves its highest resolution by encoding along the elongated axes of simple-cell receptive fields and not across these axes as suggested by spatial models of vision. According to our hypothesis, fine details of shape are encoded by inter-receptor temporal phases, texture by instantaneous intra-burst rates of individual receptors, and motion by inter-burst temporal frequencies. We further describe the ability of the visual system to readout the encoded information and recode it internally. We show how reading out of retinal signals can be facilitated by neuronal phase-locked loops (NPLLs), which lock to the retinal jitter; this locking enables recoding of motion information and temporal framing of shape and texture processing. A possible implementation of this locking-and-recoding process by specific thalamocortical loops is suggested. Overall it is suggested that high-acuity vision is based primarily on temporal mechanisms of the sort presented here and low-acuity vision is based primarily on spatial mechanisms. © 2012 Ahissar and Arieli.},
   author = {Ehud Ahissar and Amos Arieli},
   doi = {10.3389/fncom.2012.00089},
   issn = {16625188},
   issue = {OCTOBER 2012},
   journal = {Frontiers in Computational Neuroscience},
   keywords = {Active vision,Feedback,Fixational eye movements,Neural coding,Neuronal phase-locked loop,Simple cells,Temporal coding,Thalamocortical loop},
   month = {10},
   title = {Seeing via miniature eye movements: A dynamic hypothesis for vision},
   year = {2012},
}
@article{Callaway2004,
   abstract = {Visual cortical circuits are organized at multiple levels of complexity including cortical areas, layers and columns, and specific cell types within these modules. Making sense of the functions of these circuits from anatomical observations requires linking these circuits to function at each of these levels of complexity. Observations of these relationships have become increasingly sophisticated over the last several decades, beginning with correlations between the connectivities and functions of various visual cortical areas and progressing toward cell type-specificity. These studies have informed current views about the functional interactions between cortical areas and modules and the mechanisms by which fine scale microcircuits influence interactions at more coarse levels of organization. © 2004 Elsevier Ltd. All rights reserved.},
   author = {Edward M. Callaway},
   doi = {10.1016/j.neunet.2004.04.004},
   issn = {08936080},
   issue = {5-6},
   journal = {Neural Networks},
   keywords = {Driving,Excitation,Gating,Inhibition,Local circuit,Modulatory},
   pages = {625-632},
   pmid = {15288888},
   publisher = {Elsevier Ltd},
   title = {Feedforward, feedback and inhibitory connections in primate visual cortex},
   volume = {17},
   year = {2004},
}
@generic{Salinas2001,
   abstract = {For years we have known that cortical neurons collectively have synchronous or oscillatory patterns of activity, the frequencies and temporal dynamics of which are associated with distinct behavioural states. Although the function of these oscillations has remained obscure, recent experimental and theoretical results indicate that correlated fluctuations might be important for cortical processes, such as attention, that control the flow of information in the brain.},
   author = {Emilio Salinas and Terrence J. Sejnowski},
   doi = {10.1038/35086012},
   issn = {14710048},
   issue = {8},
   journal = {Nature Reviews Neuroscience},
   month = {8},
   pages = {539-550},
   pmid = {11483997},
   title = {Correlated neuronal activity and the flow of neural information},
   volume = {2},
   year = {2001},
}
@article{Zhou2020,
   abstract = {Memory is thought to be encoded by sparsely distributed neuronal ensembles in memory-related regions. However, it is unclear how memory-eligible neurons react during learning to encode trace fear memory and how they retrieve a memory. We implemented a fiber-optic confocal fluorescence endomicroscope to directly visualize calcium dynamics of hippocampal CA1 neurons in freely behaving mice subjected to trace fear conditioning. Here we report that the overall activity levels of CA1 neurons showed a right-skewed lognormal distribution, with a small portion of highly active neurons (termed Primed Neurons) filling the long-tail. Repetitive training induced Primed Neurons to shift from random activity to well-tuned synchronization. The emergence of activity synchronization coincided with the appearance of mouse freezing behaviors. In recall, a partial synchronization among the same subset of Primed Neurons was induced from random dynamics, which also coincided with mouse freezing behaviors. Additionally, training-induced synchronization facilitated robust calcium entry into Primed Neurons. In contrast, most CA1 neurons did not respond to tone and foot shock throughout the training and recall cycles. In conclusion, Primed Neurons are preferably recruited to encode trace fear memory and induction of activity synchronization among Primed Neurons out of random dynamics is critical for trace memory formation and retrieval.},
   author = {Yuxin Zhou and Liyan Qiu and Haiying Wang and Xuanmao Chen},
   doi = {10.1096/fj.201902274R},
   issn = {15306860},
   issue = {3},
   journal = {FASEB Journal},
   keywords = {GCaMP6,calcium imaging in freely behaving animals,declarative memory,memory consolidation,right-skewed lognormal distribution,synaptic plasticity},
   month = {3},
   pages = {3658-3676},
   pmid = {31944374},
   publisher = {John Wiley and Sons Inc.},
   title = {Induction of activity synchronization among primed hippocampal neurons out of random dynamics is key for trace memory formation and retrieval},
   volume = {34},
   url = {https://pubmed.ncbi.nlm.nih.gov/31944374/},
   year = {2020},
}
@article{Kaschube2010,
   abstract = {The brain's visual cortex processes information concerning form, pattern, and motion within functional maps that reflect the layout of neuronal circuits. We analyzed functional maps of orientation preference in the ferret, tree shrew, and galago - three species separated since the basal radiation of placental mammals more than 65 million years ago - and found a common organizing principle. A symmetry-based class of models for the self-organization of cortical networks predicts all essential features of the layout of these neuronal circuits, but only if suppressive long-range interactions dominate development. We show mathematically that orientation-selective long-range connectivity can mediate the required interactions. Our results suggest that self-organization has canalized the evolution of the neuronal circuitry underlying orientation preference maps into a single common design.},
   author = {Matthias Kaschube and Michael Schnabel and Siegrid Löwel and David M. Coppola and Leonard E. White and Fred Wolf},
   doi = {10.1126/science.1194869},
   issn = {00368075},
   issue = {6007},
   journal = {Science},
   month = {11},
   pages = {1113-1116},
   pmid = {21051599},
   title = {Universality in the evolution of orientation columns in the visual cortex},
   volume = {330},
   year = {2010},
}
@article{Braitenberg1979,
   abstract = {The optimal direction of lines in the visual field to which neurons in the visual cortex respond changes in a regular way when the recording electrode progresses tangentially through the cortex (Hubel and Wiesel, 1962). It is possible to reconstruct the field of orientations from long, sometimes multiple parallel penetrations (Hubel and Wiesel, 1974; Albus, 1975) by assuming that the orientations are arranged radially around centers. A method is developed which makes it possible to define uniquely the position of the centers in the vicinity of the electrode track. They turn out to be spaced at distances of about 0.5 mm and may be tentatively identified with the positions of the giant cells of Meynert. © 1979 Springer-Verlag.},
   author = {V. Braitenberg and C. Braitenberg},
   doi = {10.1007/BF00337296},
   issn = {03401200},
   issue = {3},
   journal = {Biological Cybernetics},
   month = {8},
   pages = {179-186},
   pmid = {497262},
   publisher = {Springer-Verlag},
   title = {Geometry of orientation columns in the visual cortex},
   volume = {33},
   url = {https://www.researchgate.net/publication/22645543_Geometry_of_orientation_columns_in_the_visual_cortex},
   year = {1979},
}
@report{Gray1989,
   abstract = {In areas 17and 18 of the cat visual cortex the firing probability of neurons, in response to the presentation of optimally aligned light bars within their receptive field, oscillates with a peak frequency near 40 Hz. The neuronal firing pattern is tightly correlated with the phase and amplitude of an oscillatory local field potential recorded through the same electrode. The amplitude of the local field-potential oscillations are maximal in response to stimuli that match the orientation and direction preference of the local cluster of neurons. Single and multiunit recordings from the dorsal lateral geniculate nucleus of the thalamus showed no evidence of oscillations of the neuronal firing probability in the range of 20-70 Hz. The results demonstrate that local neuronal populations in the visual cortex engage in stimulus-specific synchronous oscillations resulting from an intracortical mechanism. The oscilla-tory responses may provide a general mechanism by which activity patterns in spatially separate regions of the cortex are temporally coordinated. The mechanism by which populations of neurons in the cerebral cortex temporally coordinate their activity patterns in response to specific sensory stimuli constitutes a basic unresolved question in sensory physiology. In the vertebrate olfactory system the spatiotemporal coordination of neuronal activity is achieved by the synchronization of oscillatory responses having a frequency in the range of 40-80 Hz (1-5). Evidence suggesting that a similar mechanism for the synchronization of activity may operate in the neocortex has come from field potential recordings in awake animals. It has been demonstrated that oscillatory activity in the high beta-frequency range (20-50 Hz) occurs in sensory areas of the neocortex when the animals direct their attention to meaningful stimuli (6-11). Previously we have discovered, from recordings in area 17 of awake kittens, that neuronal responses recorded during periods of behavioral attention exhibit a rhythmic firing pattern that is tightly correlated with an oscillatory local field potential (LFP) having a frequency near 40 Hz (12). Thus, we sought to determine whether the oscillatory responses could also be recorded under varying conditions of anesthesia that would permit a more quantitative analysis of both their stimulus specificity as well as their temporal properties. Here, we extend our previous observations (13) and report that local groups of neurons, within functional columns ofthe visual cortex, engage in stimulus-specific oscillatory responses having a frequency near 40 Hz. This periodic neuronal activity is tightly correlated to the simultaneously recorded LFP, which in the majority of recordings has a similar orientation and direction preference as the local cluster of neurons. No comparable oscillations of firing probability were found for the thalamic input to visual cortex, indicating that the generation of oscillatory responses is a cortical phenomenon. The results suggest the hypothesis that the temporal pattern of the oscillatory response is used to synchronize the activity of neuronal populations in spatially separate regions of the cortex. MATERIALS AND METHODS Experiments were performed on a total of 15 adult cats and 12 kittens 4-6 weeks of age. The experimental procedures for preparation of the animals, the presentation of visual stimuli, and the recording of neuronal responses in the visual cortex have been described (14). For surgery anesthesia was induced by injection of a short-acting anesthetic (ketamine at 15 mg/kg or hexobarbital at 15 mg/kg). During recording, anesthesia was maintained with a mixture of 30% 02/70% N20, supplemented by 0.1-0.3% halothane or hexobarbital (5 mg/kg). Recordings usually began 3-5 hr after the initial anesthesia to insure that the effects of ketamine, when used, had worn off. The wound edges were infiltrated with lido-caine. Muscle paralysis was achieved by a continuous i.v. infusion of hexocarbacholinbromide (Imbretil). Multiunit activity (MUA) and LFPs were recorded from one to five 25-I&m diameter Teflon-coated platinum-iridium electrodes whose tips were etched to a point and coated with platinum black (15). The LFP and MUA were differentially amplified (5000-20,000 and 5000, respectively) and filtered (1-100 Hz and 1-3 kHz, respectively). The output of the MUA amplifier was fed through a threshold detector to isolate units, on the average, at twice the noise level, and the LFP and Schmitt trigger output were digitized on separate channels at a rate of 1 kHz each. The receptive field properties of the MUA recorded from each electrode were assessed with light stimuli projected on a tangent screen located 1.12 m in front of the cat's eye plane. All stimulus trials were 10 sec in duration, and trial sets were composed of 10 trials to each stimulus. On each trial the light stimulus was on and moving over the receptive field first in one direction (first through fourth sec) and then in the other direction (sixth through ninth sec). For histological verification of the electrode tip locations DC currents (5-10 1LA for 20 sec) were applied to the electrodes at the end of measurements , and the lesion locations were retrieved in Nissl-stained sections of the perfused fixed brains. A sample of the data, to be used for quantitative analysis, was taken from 30 recording sites in areas 17 and 18 of 14 animals. In four of the adult cats recordings of single and multiunit activity were also made in the dorsal lateral gen-iculate nucleus of the thalamus. In 25 recordings from area 17 a quantitative assay of orientation selectivity was performed at angular intervals of 22.50. Four separate sets of calculations were performed on the data to examine the temporal properties ofthe MUA and LFP Abbreviations: LFP, local field potential; MUA, multiunit activity; ACF, autocorrelation function. 1698 The publication costs of this article were defrayed in part by page charge payment. This article must therefore be hereby marked "advertisement" in accordance with 18 U.S.C. §1734 solely to indicate this fact.},
   author = {Charles M Gray and Wolf Singer},
   journal = {Proc. Nati. Acad. Sci. USA},
   pages = {1698-1702},
   title = {Stimulus-specific neuronal oscillations in orientation columns of cat visual cortex (thalamus/area 17)},
   volume = {86},
   year = {1989},
}
@article{thorpe1996,
   author = {Simon Thorpe and  Denis Fize and  Catherine Marlot},
   journal = {NATURE},
   month = {6},
   pages = {520-522},
   title = {Speed of processing in the human visual system},
   volume = {381},
   year = {1996},
}
@article{Jacobs2007,
   abstract = {A growing body of animal research suggests that neurons represent information not only in terms of their firing rates but also by varying the timing of spikes relative to neuronal oscillations. Although researchers have argued that this temporal coding is critical in human memory and perception, no supporting data from humans have been reported. This study provides the first analysis of the temporal relationship between brain oscillations and single-neuron activity in humans. Recording from 1924 neurons, we find that neuronal activity in various brain regions increases at specific phases of brain oscillations. Neurons in widespread brain regions were phase locked to oscillations in the theta- (4-8 Hz) andgamma-(30-90 Hz) frequency bands. In hippocampus, phase locking was prevalent in the delta-(1-4 Hz) and gamma-frequency bands. Individual neurons were phase locked to various phases of theta and delta oscillations, but they only were active at the trough of gamma oscillations. These findings provide support for the temporal-coding hypothesis in humans. Specifically, they indicate that theta and delta oscillations facilitate phase coding and that gamma oscillations help to decode combinations of simultaneously active neurons. Copyright © 2007 Society for Neuroscience.},
   author = {Joshua Jacobs and Michael J. Kahana and Arne D. Ekstrom and Itzhak Fried},
   doi = {10.1523/JNEUROSCI.4636-06.2007},
   issn = {02706474},
   issue = {14},
   journal = {Journal of Neuroscience},
   keywords = {Gamma,Intracranial EEG,Local field potential,Navigation,Phase locking,Theta},
   month = {4},
   pages = {3839-3844},
   pmid = {17409248},
   title = {Brain oscillations control timing of single-neuron activity in humans},
   volume = {27},
   year = {2007},
}
@report{,
   abstract = {The human brain spontaneously generates neural oscillations with a large variability in frequency, amplitude, duration, and recurrence. Little, however, is known about the long-term spa-tiotemporal structure of the complex patterns of ongoing activity. A central unresolved issue is whether fluctuations in oscil-latory activity reflect a memory of the dynamics of the system for more than a few seconds. We investigated the temporal correlations of network oscillations in the normal human brain at time scales ranging from a few seconds to several minutes. Ongoing activity during eyes-open and eyes-closed conditions was recorded with simultaneous magnetoencephalogra-phy and electroencephalography. Here we show that amplitude fluctuations of 10 and 20 Hz oscillations are correlated over thousands of oscillation cycles. Our analyses also indicated that these amplitude fluctuations obey power-law scaling behavior. The scaling exponents were highly invariant across subjects. We propose that the large variability, the long-range correlations, and the power-law scaling behavior of spontaneous oscillations find a unifying explanation within the theory of self-organized criticality, which offers a general mechanism for the emergence of correlations and complex dynamics in stochastic multiunit systems. The demonstrated scaling laws pose novel quantitative constraints on computational models of network oscillations. We argue that critical-state dynamics of spontaneous oscillations may lend neural networks capable of quick reorganization during processing demands.},
   author = {Klaus Linkenkaer-Hansen and Vadim V Nikouline and J Matias Palva and Risto J Ilmoniemi},
   keywords = {complexity,correlations,large-scale dynamics,scaling behavior,self-organized criticality,spontaneous oscillations,temporal properties},
   title = {Long-Range Temporal Correlations and Scaling Behavior in Human Brain Oscillations},
   url = {http://paos.colorado.edu/research/wavelets},
   year = {2001},
}
@generic{,
   abstract = {Oscillatory fluctuations of local field potentials (LFPs) in the theta (4-8. Hz) and gamma (25-140. Hz) band are held to play a mechanistic role in various aspects of memory including the representation and off-line maintenance of events and sequences of events, the assessment of novelty, the induction of plasticity during encoding, as well as the consolidation and the retrieval of stored memories. Recent findings indicate that theta and gamma related mechanisms identified in rodent studies have significant parallels in the neurophysiology of human and non-human primate memory. This correspondence between species opens new perspectives for a mechanistic investigation of human memory function. © 2010 Elsevier Ltd.},
   author = {Emrah Düzel and Will D. Penny and Neil Burgess},
   doi = {10.1016/j.conb.2010.01.004},
   issn = {09594388},
   issue = {2},
   journal = {Current Opinion in Neurobiology},
   pages = {143-149},
   pmid = {20181475},
   publisher = {Elsevier Ltd},
   title = {Brain oscillations and memory},
   volume = {20},
   url = {https://www.sciencedirect.com/science/article/pii/S095943881000005X},
   year = {2010},
}
@article{Basar2000,
   abstract = {Gamma oscillations, now widely regarded as functionally relevant signals of the brain, illustrate that the concept of event-related oscillations bridges the gap between single neurons and neural assemblies. Taking this concept further, we review experiments showing that oscillatory phenomena such as alpha, theta, or delta responses to events are strongly interwoven with sensory and cognitive functions. This review argues that selecti¨ely distributed delta, theta, alpha, and gamma oscillatory systems act as resonant communication networks through large populations of neurons. Thus, oscillatory processes might play a major role in relation with memory and integrati¨e functions. A new 'neurons-brain' doctrine is also proposed to extend the neuron doctrine of Sherrington.},
   author = {E Başar and C Başar-Eroğlu and S Karakaş and M Schürmann},
   doi = {10.1016/S0167-8760(99)00047-1},
   issn = {01678760},
   issue = {2-3},
   journal = {International Journal of Psychophysiology},
   keywords = {Brain oscillations delta, theta, alpha, gamma,Cognitive processing,Distributed networks,Event-related potentials,Evoked potentials,Memory,Sensory process-ing},
   month = {3},
   pages = {95-124},
   title = {Brain oscillations in perception and memory},
   volume = {35},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0167876099000471},
   year = {2000},
}

@article{Watts2005,
   abstract = {The cerebral cortex is pivotal in information processing and higher brain function and its laminar structure of six distinct layers, each in receipt of a different constellation of inputs, makes it important to identify connectivity patterns and distinctions between excitatory and inhibitory pathways. The 'feedforward' projections from layer 4-3 and from 3-5 target pyramidal cells and to lesser degrees interneurones. 'Feedback' projections from layer 5-3 and from 3-4, on the other hand, mainly target interneurones. Understanding the microcircuitry may give some insight into the computation and information processing performed in this brain region. © The Physiological Society 2004.},
   author = {Joanne Watts and Alex M. Thomson},
   doi = {10.1113/jphysiol.2004.076984},
   issn = {00223751},
   issue = {1},
   journal = {Journal of Physiology},
   month = {1},
   pages = {89-97},
   pmid = {15539397},
   title = {Excitatory and inhibitory connections show selectivity in the neocortex},
   volume = {562},
   url = {https://physoc.onlinelibrary.wiley.com/doi/full/10.1113/jphysiol.2004.076984},
   year = {2005},
}
@article{,
   abstract = {Microcircuits in different brain areas share similar architectural and biophysical properties with compact motor networks known as central pattern generators (CPGs). Consequently, CPGs have been suggested as valuable biological models for understanding of microcircuit dynamics and particularly, their synchronization. We use a well known compact motor network, the lobster pyloric CPG to study principles of intercircuit synchronization. We couple separate pyloric circuits obtained from two animals via artificial synapses and observe how their synchronization depends on the topology and kinetic parameters of the computer-generated synapses. Stable in-phase synchronization appears when electrically coupling the pacemaker groups of the two networks, but reciprocal inhibitory connections produce more robust and regular cooperative activity. Contralateral inhibitory connections offer effective synchronization and flexible setting of the burst phases of the interacting networks. We also show that a conductance-based mathematical model of the coupled circuits correctly reproduces the observed dynamics illustrating the generality of the phenomena. © 2009 Elsevier Inc. All rights reserved.},
   author = {Attila Szücs and Ramon Huerta and Mikhail I. Rabinovich and Allen I. Selverston},
   doi = {10.1016/j.neuron.2008.12.032},
   issn = {08966273},
   issue = {3},
   journal = {Neuron},
   keywords = {SYSNEURO},
   month = {2},
   pages = {439-453},
   pmid = {19217380},
   title = {Robust Microcircuit Synchronization by Inhibitory Connections},
   volume = {61},
   url = {https://www.sciencedirect.com/science/article/pii/S0896627309000385},
   year = {2009},
}
@report{Sohal2000,
   abstract = {Mice with an inactivated GABA A receptor 3 subunit gene have features of Angelman syndrome, including absence-like seizures. This suggests the occurrence of abnormal hypersyn-chrony in the thalamocortical system. Within the thalamus, the efficacy of inhibitory synapses between thalamic reticular (RE) neurons is selectively compromised, and thalamic oscillations in vitro are prolonged and lack spatial phase gradients (Hunts-man et al., 1999). Here we used computational models to examine how intra-RE inhibition regulates intrathalamic oscillations. A major effect is an abbreviation of network responses, which is caused by long-lasting intra-RE inhibition that shunts recurrent excitatory input. In addition, differential activation of RE cells desynchronizes network activity. Near the slice center, where many cells are initially activated, there is a resultant high level of intra-RE inhibition. This leads to RE cell burst truncation in the central region and a gradient in the timing of thalamo-cortical cell activity similar to that observed in vitro. Although RE cell burst durations were shortened by this mechanism, there was very little effect on the times at which RE cells began to burst. The above results depended on widespread stimuli that activated RE cells in regions larger than the diameter of intra-RE connections. By contrast, more focal stimuli could elicit oscillations that lasted several cycles and remained confined to a small region. These results suggest that intra-RE inhibition restricts intrathalamic activity to particular spatiotem-poral patterns to allow focal recurrent activity that may be relevant for normal thalamocortical function while preventing widespread synchronization as occurs in seizures. The thalamus participates in a wide range of thalamocortical oscillations, including 7-14 Hz sleep spindles (Steriade et al., 1993). Spindle activity occurs in the thalamus of decorticated cats (Morison and Basset, 1945) and in thalamic slices (von Krosigk et al., 1993; Huguenard and Prince, 1994a; Bal et al., 1995a,b; Kim et al., 1995), but not in cortex that has been disconnected from thalamus (Burns, 1950), suggesting that the thalamus plays an important role in generating the spindle rhythm. Several experimental observations suggest that reciprocal in-hibitory connections between thalamic reticular (RE) neurons regulate thalamic spindle oscillations and prevent hypersynchrony characteristic of some epilepsies. The anti-absence drug clonaz-epam may function by enhancing GABA A connections between RE cells, thereby reducing the output of RE cells to thalamocor-tical (TC) cells (Huguenard and Prince, 1994b). Additional evidence comes from mice lacking the 3 subunit of the GABA A receptor. Knockout of the 3 subunit reduces the strength and duration of GABA A synapses between RE cells without affecting those from RE to TC cells (Huntsman et al., 1999). This highly selective change has important consequences for intrathalamic oscillations elicited by stimulation of internal capsule in vitro (Hunstman et al., 1999). First, oscillations last much longer in thalamic slices from knockout (3 /) mice than in those from wild-type (3 /) animals. Second, phase differences between TC cell activity at different locations along knockout slices are negligible. In wild-type slices by contrast, phase differences between TC cell activity at different locations grow with distance. The phase lags observed in wild-type slices are at least an order of magnitude larger than those in knockout slices. These in vitro findings may help to explain why 3 / knockout mice have many features of Angelman syndrome, including seizures (Ho-manics et al., 1997; DeLorey et al., 1998). Here we used computational models to evaluate mechanisms by which intact intra-RE inhibition could produce these differences between oscillations in wild-type and knockout slices. We then studied how these mechanisms depend on the strength, kinetics, and spatial organization of intra-RE inhibition. Finally, we explore the functional consequences of these mechanisms for in-trathalamic activity. Our findings suggest that particular properties of intra-RE inhibition, such as its slow decay, enable it to restrict intrathalamic activity to particular spatiotemporal patterns and thus prevent epileptiform activity. MATERIALS AND METHODS Model neurons. We studied a network model that included 400 TC and 400 RE neurons. Models for both types of neurons were presented in an earlier study (Sohal and Huguenard, 1998). Each neuron was modeled as a single compartment. V T and V R , the membrane potentials of TC and RE cells, respectively, evolved according to: C m V ˙ T g L V T E L I T I h I K I Na I GABAATC , (1) C m V ˙ R g L V R E L I Ts I K I Na I AM PA I GABAARE , (2) where the specific capacitance of the membrane, C m , equals 1 F/cm 2 , g L is the leak conductance, E L is the reversal potential of the leak current, I T and I Ts are low-threshold calcium currents, I h is the hyperpolarization-activated cation current, I K and I Na are the potassium and sodium currents underlying action potentials, I GABA-A(TC) and I GABA-A(RE) are},
   author = {Vikaas S Sohal and Molly M Huntsman and John R Huguenard},
   issue = {5},
   journal = {The Journal of Neuroscience},
   keywords = {An-gelman syndrome,GABA A receptors,absence seizures,computational model,spindle rhythm,thalamus},
   pages = {1735-1745},
   title = {Reciprocal Inhibitory Connections Regulate the Spatiotemporal Properties of Intrathalamic Oscillations},
   volume = {20},
   url = {https://www.jneurosci.org/content/20/5/1735.short},
   year = {2000},
}
@article{Neftci2016,
   abstract = {Recent studies have shown that synaptic unreliability is a robust and sufficient mechanism for inducing the stochasticity observed in cortex. Here, we introduce Synaptic Sampling Machines (S2Ms), a class of neural network models that uses synaptic stochasticity as a means to Monte Carlo sampling and unsupervised learning. Similar to the original formulation of Boltzmann machines, these models can be viewed as a stochastic counterpart of Hopfield networks, but where stochasticity is induced by a random mask over the connections. Synaptic stochasticity plays the dual role of an efficient mechanism for sampling, and a regularizer during learning akin to DropConnect. A local synaptic plasticity rule implementing an event-driven form of contrastive divergence enables the learning of generative models in an on-line fashion. S2Ms perform equally well using discrete-timed artificial units (as in Hopfield networks) or continuous-timed leaky integrate and fire neurons. The learned representations are remarkably sparse and robust to reductions in bit precision and synapse pruning: removal of more than 75% of the weakest connections followed by cursory re-learning causes a negligible performance loss on benchmark classification tasks. The spiking neuron-based S2Ms outperform existing spike-based unsupervised learners, while potentially offering substantial advantages in terms of power and complexity, and are thus promising models for on-line learning in brain-inspired hardware.},
   author = {Emre O. Neftci and Bruno U. Pedroni and Siddharth Joshi and Maruan Al-Shedivat and Gert Cauwenberghs},
   doi = {10.3389/fnins.2016.00241},
   issn = {1662453X},
   issue = {JUN},
   journal = {Frontiers in Neuroscience},
   keywords = {Hopfield networks,Regularization,Spiking neural networks,Stochastic processes,Synaptic plasticty,Synaptic transmission,Unsupervised learning},
   month = {6},
   publisher = {Frontiers Research Foundation},
   title = {Stochastic synapses enable efficient brain-inspired learning machines},
   volume = {10},
   year = {2016},
}
@report{Buonomano1998,
   abstract = {It has been clear for almost two decades that cortical representations in adult animals are not fixed entities, but rather, are dynamic and are continuously modified by experience. The cortex can preferentially allocate area to represent the particular peripheral input sources that are proportionally most used. Alterations in cortical representations appear to underlie learning tasks dependent on the use of the behaviorally important peripheral inputs that they represent. The rules governing this cortical representational plasticity following manipulations of inputs, including learning, are increasingly well understood. In parallel with developments in the field of cortical map plasticity, studies of synaptic plasticity have characterized specific elementary forms of plasticity, including associative long-term potentiation and long-term depression of exci-tatory postsynaptic potentials. Investigators have made many important strides toward understanding the molecular underpinnings of these fundamental plasticity processes and toward defining the learning rules that govern their induction. The fields of cortical synaptic plasticity and cortical map plasticity have been implicitly linked by the hypothesis that synaptic plasticity underlies cortical map reorganization. Recent experimental and theoretical work has provided increasingly stronger support for this hypothesis. The goal of the current paper is to review the fields of both synaptic and cortical map plasticity with an emphasis on the work that attempts to unite both fields. A second objective is to highlight the gaps in our understanding of synaptic and cellular mechanisms underlying cortical representational plasticity. 149},
   author = {Dean V Buonomano and Michael M Merzenich},
   journal = {Annu. Rev. Neurosci},
   keywords = {Hebbian,LTD,LTP,cortex,topographic},
   pages = {149-86},
   title = {CORTICAL PLASTICITY: From Synapses to Maps},
   volume = {21},
   year = {1998},
}
@article{tobias2017,
   abstract = {A sophisticated analysis in mice of how inputs to neurons from other neurons are distributed across individual cells of the brain’s visual cortex provides information about how mammalian vision is processed},
   author = {Tobias Rose and  Mark Hübener},
   doi = {10.1038/nature23098},
   journal = {NATURE},
   title = {Synapses get together for vision},
   year = {2017},
}
@article{Abraham2019,
   abstract = {It has been 70 years since Donald Hebb published his formalized theory of synaptic adaptation during learning. Hebb’s seminal work foreshadowed some of the great neuroscientific discoveries of the following decades, including the discovery of long-term potentiation and other lasting forms of synaptic plasticity, and more recently the residence of memories in synaptically connected neuronal assemblies. Our understanding of the processes underlying learning and memory has been dominated by the view that synapses are the principal site of information storage in the brain. This view has received substantial support from research in several model systems, with the vast majority of studies on the topic corroborating a role for synapses in memory storage. Yet, despite the neuroscience community’s best efforts, we are still without conclusive proof that memories reside at synapses. Furthermore, an increasing number of non-synaptic mechanisms have emerged that are also capable of acting as memory substrates. In this review, we address the key findings from the synaptic plasticity literature that make these phenomena such attractive memory mechanisms. We then turn our attention to evidence that questions the reliance of memory exclusively on changes at the synapse and attempt to integrate these opposing views.},
   author = {Wickliffe C. Abraham and Owen D. Jones and David L. Glanzman},
   doi = {10.1038/s41539-019-0048-y},
   issn = {2056-7936},
   issue = {1},
   journal = {npj Science of Learning},
   month = {12},
   publisher = {Springer Science and Business Media LLC},
   title = {Is plasticity of synapses the mechanism of long-term memory storage?},
   volume = {4},
   year = {2019},
}
@article{Balduzzi2013,
   abstract = {Neurons deep in cortex interact with the environment extremely indirectly; the spikes they receive and produce are pre- and post-processed by millions of other neurons. This paper proposes two information-theoretic constraints guiding the production of spikes, that help ensure bursting activity deep in cortex relates meaningfully to events in the environment. First, neurons should emphasize selective responses with bursts. Second, neurons should propagate selective inputs by burst-firing in response to them. We show the constraints are necessary for bursts to dominate information-transfer within cortex, thereby providing a substrate allowing neurons to distribute credit amongst themselves. Finally, since synaptic plasticity degrades the ability of neurons to burst selectively, we argue that homeostatic regulation of synaptic weights is necessary, and that it is best performed offline during sleep. © 2012 Springer-Verlag.},
   author = {David Balduzzi and Giulio Tononi},
   doi = {10.1007/s12064-012-0165-0},
   issn = {16117530},
   issue = {1},
   journal = {Theory in Biosciences},
   keywords = {Credit assignment,Information theory,Selectivity,Synaptic plasticity},
   month = {3},
   pages = {27-39},
   pmid = {22956291},
   publisher = {Springer Verlag},
   title = {What can neurons do for their brain? Communicate selectivity with bursts},
   volume = {132},
   year = {2013},
}
@article{Sengupta2010,
   abstract = {The initiation and propagation of action potentials (APs) places high demands on the energetic resources of neural tissue. Each AP forces ATP-driven ion pumps to work harder to restore the ionic concentration gradients, thus consuming more energy. Here, we ask whether the ionic currents underlying the AP can be predicted theoretically from the principle of minimum energy consumption. A long-held supposition that APs are energetically wasteful, based on theoretical analysis of the squid giant axon AP, has recently been overturned by studies that measured the currents contributing to the AP in several mammalian neurons. In the single compartment models studied here, AP energy consumption varies greatly among vertebrate and invertebrate neurons, with several mammalian neuron models using close to the capacitive minimum of energy needed. Strikingly, energy consumption can increase by more than ten-fold simply by changing the overlap of the Na+ and K+ currents during the AP without changing the APs shape. As a consequence, the height and width of the AP are poor predictors of energy consumption. In the Hodgkin-Huxley model of the squid axon, optimizing the kinetics or number of Na+ and K+ channels can whittle down the number of ATP molecules needed for each AP by a factor of four. In contrast to the squid AP, the temporal profile of the currents underlying APs of some mammalian neurons are nearly perfectly matched to the optimized properties of ionic conductances so as to minimize the ATP cost. © 2010 Sengupta et al.},
   author = {Biswa Sengupta and Martin Stemmler and Simon B. Laughlin and Jeremy E. Niven},
   doi = {10.1371/journal.pcbi.1000840},
   issn = {1553734X},
   issue = {7},
   journal = {PLoS Computational Biology},
   month = {7},
   pages = {35},
   pmid = {20617202},
   title = {Action potential energy efficiency varies among neuron types in vertebrates and invertebrates},
   volume = {6},
   year = {2010},
}
@generic{Fellin2009,
   abstract = {Neuromodulation is a fundamental process in the brain that regulates synaptic transmission, neuronal network activity and behavior. Emerging evidence demonstrates that astrocytes, a major population of glial cells in the brain, play previously unrecognized functions in neuronal modulation. Astrocytes can detect the level of neuronal activity and release chemical transmitters to influence neuronal function. For example, recent findings show that astrocytes play crucial roles in the control of Hebbian plasticity, the regulation of neuronal excitability and the induction of homeostatic plasticity. This review discusses the importance of astrocyte-to-neuron signaling in different aspects of neuronal function from the activity of single synapses to that of neuronal networks. © 2008 The Author.},
   author = {Tommaso Fellin},
   doi = {10.1111/j.1471-4159.2008.05830.x},
   issn = {00223042},
   issue = {3},
   journal = {Journal of Neurochemistry},
   keywords = {Astrocytes,Ca2+ signaling,Glia,Gliotransmission,Synaptic plasticity,Synaptic transmission},
   month = {2},
   pages = {533-544},
   pmid = {19187090},
   title = {Communication between neurons and astrocytes: Relevance to the modulation of synaptic and network activity},
   volume = {108},
   year = {2009},
}
@generic{Disterhoft2006,
   abstract = {In vitro experiments indicate that intrinsic neuronal excitability, as evidenced by changes in the post-burst afterhyperpolarization (AHP) and spike-frequency accommodation, is altered during learning and normal aging in the brain. Here we review these studies, highlighting two consistent findings: (i) that AHP and accommodation are reduced in pyramidal neurons from animals that have learned a task; and (ii) that AHP and accommodation are enhanced in pyramidal neurons from aging subjects, a cellular change that might contribute to age-related learning impairments. Findings from in vivo single-neuron recording studies complement the in vitro data. From these consistently reproduced findings, we propose that the intrinsic AHP level might determine the degree of synaptic plasticity and learning. Furthermore, it seems that reductions in the AHP must occur before learning if young and aging subjects are to learn a task successfully. © 2006 Elsevier Ltd. All rights reserved.},
   author = {John F. Disterhoft and M. Matthew Oh},
   doi = {10.1016/j.tins.2006.08.005},
   issn = {01662236},
   issue = {10},
   journal = {Trends in Neurosciences},
   month = {10},
   pages = {587-599},
   pmid = {16942805},
   title = {Learning, aging and intrinsic neuronal plasticity},
   volume = {29},
   year = {2006},
}
@article{Long2005,
   abstract = {In the suprachiasmatic nucleus (SCN), the master circadian pacemaker, neurons show circadian variations in firing frequency. There is also considerable synchrony of spiking across SCN neurons on a scale of milliseconds, but the mechanisms are poorly understood. Using paired whole-cell recordings, we have found that many neurons in the rat SCN communicate via electrical synapses. Spontaneous spiking was often synchronized in pairs of electrically coupled neurons, and the degree of this synchrony could be predicted from the magnitude of coupling. In wild-type mice, as in rats, the SCN contained electrical synapses, but electrical synapses were absent in connexin36-knockout mice. The knockout mice also showed dampened circadian activity rhythms and a delayed onset of activity during transition to constant darkness. We suggest that electrical synapses in the SCN help to synchronize its spiking activity, and that such synchrony is necessary for normal circadian behavior.},
   author = {Michael A. Long and Michael J. Jutras and Barry W. Connors and Rebecca D. Burwell},
   doi = {10.1038/nn1361},
   issn = {10976256},
   issue = {1},
   journal = {Nature Neuroscience},
   month = {1},
   pages = {61-66},
   pmid = {15580271},
   title = {Electrical synapses coordinate activity in the suprachiasmatic nucleus},
   volume = {8},
   year = {2005},
}
@generic{Hormuzdi2004,
   abstract = {Gap junctions consist of intercellular channels dedicated to providing a direct pathway for ionic and biochemical communication between contacting cells. After an initial burst of publications describing electrical coupling in the brain, gap junctions progressively became less fashionable among neurobiologists, as the consensus was that this form of synaptic transmission would play a minimal role in shaping neuronal activity in higher vertebrates. Several new findings over the last decade (e.g. the implication of connexins in genetic diseases of the nervous system, in processing sensory information and in synchronizing the activity of neuronal networks) have brought gap junctions back into the spotlight. The appearance of gap junctional coupling in the nervous system is developmentally regulated, restricted to distinct cell types and persists after the establishment of chemical synapses, thus suggesting that this form of cell-cell signaling may be functionally interrelated with, rather than alternative to chemical transmission. This review focuses on gap junctions between neurons and summarizes the available data, derived from molecular, biological, electrophysiological, and genetic approaches, that are contributing to a new appreciation of their role in brain function. © 2004 Elsevier B.V. All rights reserved.},
   author = {Sheriar G. Hormuzdi and Mikhail A. Filippov and Georgia Mitropoulou and Hannah Monyer and Roberto Bruzzone},
   doi = {10.1016/j.bbamem.2003.10.023},
   issn = {00052736},
   issue = {1-2},
   journal = {Biochimica et Biophysica Acta - Biomembranes},
   keywords = {Connexin,Coupling,Gap junction,Neuron,Oscillation,Retina},
   month = {3},
   pages = {113-137},
   pmid = {15033583},
   title = {Electrical synapses: A dynamic signaling system that shapes the activity of neuronal networks},
   volume = {1662},
   year = {2004},
}
@report{Maclean2003,
   abstract = {strate that a neuron can sense and adapt to overexpres-sion of one ion channel type in the absence of significant Ithaca, New York 14853 changes in physiological activity. This is accomplished by increasing the amplitude of a second current with some opposing physiological effects, providing a bal-Summary ancing electrophysiological compensation for the induced current. The shal gene encodes the transient potassium cur-We set out to study the role of the shal gene, a member rent (I A) in neurons of the lobster stomatogastric gan-of the shaker family of voltage-dependent K channel glion. Overexpression of Shal by RNA injection into genes, in rhythmically active pyloric dilator (PD) neurons neurons produces a large increase in I A , but surpris-in the stomatogastric ganglion (STG) of the spiny lobster, ingly little change in the neuron's firing properties. Panulirus interruptus. These neurons are members of Accompanying the increase in I A is a dramatic and the pacemaker kernel of the pyloric network, a well un-linearly correlated increase in the hyperpolarization-derstood rhythmic motor pattern generator (Johnson activated inward current (I h). The enhanced I h electro-and Hooper, 1992; Ayali and Harris-Warrick, 1999). The physiologically compensates for the enhanced I A , PD neurons fire rhythmic bursts of action potentials su-since pharmacological blockade of I h uncovers the perimposed on slow membrane potential oscillations. physiological effects of the increased I A. Expression The relatively stereotyped oscillatory properties of these of a nonfunctional mutant Shal also induces a large neurons are determined by the complement of ion chan-increase in I h , demonstrating a novel activity-indepen-nels they express and their synaptic inputs (Getting, dent coupling between the Shal protein and I h en-1989; Selverston and Moulins, 1985). The fast transient hancement. Since I A and I h influence neuronal activity potassium current (I A) helps to shape PD neuron firing in opposite directions, our results suggest a selective properties, including the maximal spike frequency dur-coregulation of these channels as a mechanism for ing a burst and the rate of postinhibitory rebound (Tier-constraining cell activity within appropriate physiolog-ney and Harris-Warrick, 1992). Single-cell RT-PCR and ical parameters. immunocytochemistry have shown that the shal gene encodes I A in pyloric neurons within the STG (Baro et Introduction al., 1997, 2000, 2001; Baro and Harris-Warrick, 1998). To further study the role of I A , we artificially increased Many neurons, as functional units in networks generat-I A amplitude by microinjecting shal RNA into the PD ing complex behaviors, must develop and maintain a neurons. Despite large increases in I A amplitude, the stable physiological identity despite continuously firing properties of the neurons were essentially un-changing inputs both during development and in the changed. Surprisingly, the increased I A is compensated adult. Each neuron's intrinsic excitable properties and by a corresponding endogenous upregulation of the hy-preferred activity range are defined by the pattern of perpolarization-activated inward current, I h. Our findings specific ion channels, receptors, and enzymes that it suggest a novel level of regulation for specific ion chan-expresses. One important mechanism for developing nels that promotes stability in the activity of individual and maintaining stable neuronal function is activity-neurons as well as the entire network. dependent homeostasis; both vertebrate and invertebrate neurons can restore normal firing patterns after Results imposed changes in firing activity induced, for example, by loss of normal synaptic inputs or prolonged pharma-Significant Increase in I A 72 hr after Microinjection cological block of activity. Such activity-dependent re-of shal-GFP RNA sponses involve slow compensatory changes in the bal-To visualize protein expression after shal RNA injection, ance of ionic currents expressed in the neuron the sequence for green fluorescent protein (GFP) was (LeMasson et al., 1993; Turrigiano, 1999; Turrigiano et ligated near the C-terminal end of the shal clone. GFP al., 1994, 1995, 1998; Desai et al., 1999; Golowasch et ligation had no effect on the biophysical properties of al., 1999; Spitzer, 1999; Galante et al., 2001). However, the Shal current when expressed in Xenopus oocytes: since the trigger for the homeostatic response is a the amplitude of the current, voltage dependence of change in activity, the neuron must at least temporarily activation and inactivation, and rates of inactivation fire in an abnormal fashion. were all unaffected (n 6). To be sure that appending Here we report a novel activity-independent homeo-GFP had no effect, we carried out parallel measurements with both shal-GFP and shal RNA injections into the PDan anti-Shal antibody to visualize successful Shal ex-versity,},
   author = {Jason N Maclean and Ying Zhang and Bruce R Johnson and Ronald M Harris-Warrick},
   journal = {Neuron},
   title = {Activity-Independent Homeostasis in Rhythmically Active Neurons static mechanism that could function in concert with activity-dependent mechanisms to maintain neuronal firing patterns within their normal ranges. We demon},
   volume = {37},
   year = {2003},
}
@generic{Li2003,
   abstract = {Neurons in the brain touch and communicate with each other at specialized contacts called synapses. So, how do these tiny intercellular junctions form during development? The assembly of neuronal synapses proceeds through several stages, and occurs with surprising speed. Recent biochemical, genetic and imaging studies are beginning to disclose the molecular mechanisms that underlie synapse formation, growth and maturation.},
   author = {Zheng Li and Morgan Sheng},
   doi = {10.1038/nrm1242},
   issn = {14710072},
   issue = {11},
   journal = {Nature Reviews Molecular Cell Biology},
   month = {11},
   pages = {833-841},
   pmid = {14625534},
   title = {Some assembly required: The development of neuronal synapses},
   volume = {4},
   year = {2003},
}
@generic{Salinas2001,
   abstract = {For years we have known that cortical neurons collectively have synchronous or oscillatory patterns of activity, the frequencies and temporal dynamics of which are associated with distinct behavioural states. Although the function of these oscillations has remained obscure, recent experimental and theoretical results indicate that correlated fluctuations might be important for cortical processes, such as attention, that control the flow of information in the brain.},
   author = {Emilio Salinas and Terrence J. Sejnowski},
   doi = {10.1038/35086012},
   issn = {14710048},
   issue = {8},
   journal = {Nature Reviews Neuroscience},
   month = {8},
   pages = {539-550},
   pmid = {11483997},
   title = {Correlated neuronal activity and the flow of neural information},
   volume = {2},
   year = {2001},
}
@generic{Cohen1973,
   author = {L. B. Cohen},
   doi = {10.1152/physrev.1973.53.2.373},
   issn = {00319333},
   issue = {2},
   journal = {Physiological reviews},
   pages = {373-418},
   pmid = {4349816},
   title = {Changes in neuron structure during action potential propagation and synaptic transmission.},
   volume = {53},
   year = {1973},
}
@report{,
   abstract = {The brain acts as an integrated information processing system, which methods in cognitive neuroscience have so far depicted in a fragmented fashion. Here, we propose a simple and robust way to integrate functional MRI (fMRI) with single trial event-related potentials (ERP) to provide a more complete spatiotemporal characterization of evoked responses in the human brain. The idea behind the approach is to find brain regions whose fMRI responses can be predicted by paradigm-induced amplitude modulations of simultaneously acquired single trial ERPs. The method was used to study a variant of a two-stimulus auditory target detection (odd-ball) paradigm that manipulated predictability through alternations of stimulus sequences with random or regular target-to-target intervals. In addition to electrophysiologic and hemodynamic evoked responses to auditory targets per se, single-trial modulations were expressed during the latencies of the P2 (170-ms), N2 (200-ms), and P3 (320-ms) components and predicted spatially separated fMRI activation patterns. These spatiotemporal matches, i.e., the prediction of hemodynamic activation by time-variant information from single trial ERPs, permit inferences about regional responses using fMRI with the temporal resolution provided by electrophysiology. multimodal imaging P3 pattern learning target detection F unctional MRI (fMRI) of the blood oxygenation level-dependent (BOLD) response (BOLD-fMRI) measures local changes in brain hemodynamics associated with a cognitive process noninvasively with a high spatial resolution. However, an unsolved issue in fMRI research is the insufficient temporal resolution of the BOLD response. In contrast to the spatial resolution of BOLD-fMRI, event-related potentials (ERP) access the current induced by synaptic activity instantaneously, with an effective temporal resolution on the order of tens to hundreds of milliseconds in case of long-latency cortical responses. However, the location of underlying generators cannot be inferred with certainty. In combination, these two complementary noninvasive methods would allow for joint high-resolution spatial and temporal mapping of the mental process under investigation and add to a more complete understanding of the neural correlates of perception and cognition (1-3). In humans, this integrated spatial and temporal precision could so far be obtained only in direct intracranial recordings, usually performed in patients receiving brain surgery for treatment of epilepsy (4-7). There are basically three approaches to multimodal integration: (i) through fusion, usually referring to the use of a common forward or generative model that can explain both the electroencephalo-gram (EEG) and fMRI data (8, 9); (ii) through constraints, where spatial information from the fMRI is used for a (spatiotemporal) source reconstruction of the EEG (10-12); and (iii) through prediction, where the fMRI signal is modeled as some measure of the EEG convolved with a hemodynamic response function, a principle used in our study. Invasive recordings in animals have shown that the BOLD response is approximately linearly related to local changes in the underlying neuronal activity. The relationship appears to be stronger for the afferent pre-and postsynaptic processing, which produces the local field potential (LFP), than it is for the output from the neuron, i.e., spike rate or multiunit activity (13-16). The LFP is the basis for the scalp EEG and ERP when coherent at a more macroscopic scale (17), implying that spatiotemporal data integration can be achieved by investigating correlations between BOLD and scalp EEGERP. This can be done either continuously over time, as in the study of background rhythms (18-20) and epileptic discharges (21, 22) in the EEG, or in the context of inducing variation in a given cognitive operation (23-25). When a consistent relationship is detected, one can infer that the corresponding fMRI activation either directly represents the electric source or modulates remote generators (18-25). However, the temporal evolution of neuronal activation has not been addressed. To resolve this issue, we used the trial-to-trial variability of single-trial ERPs (26, 27) recorded simultaneously with the fMRI as predictors for hemody-namic responses to a variant of an auditory target detection (oddball) paradigm. In this design, infrequent targets were interspersed with frequent standard stimuli at random or regular intervals in an alternating way (see also Fig. 5, which is published as supporting information on the PNAS web site). Sequences of regularly spaced targets, i.e., patterns embedded in this design, affect the subjective predictabilityexpectancy (28, 29), and pilot experiments indicated that several components, at different laten-cies in the ERP, are modulated according to a sigmoid function of the number of times an interval is repeated, and learned. These amplitude modulations (AMs) develop across trials, on a timescale slow enough to be sampled with fMRI, and should be consistently correlated with the BOLD response in discrete brain regions across the observation time, assuming temporally and spatially independent neuronal generators (Fig. 1). fMRI responses that can be predicted by AMs in the ERP can be tied to the processing engaged at the time of the AMs. The approach thus allows inferences about This work was presented in part in poster form at the},
   author = {Tom Eichele and Karsten Specht and Matthias Moosmann and Marijtje L A Jongsma and Rodrigo Quian Quiroga and Helge Nordby and Kenneth Hugdahl},
   issue = {49},
   keywords = {AM, amplitude modulation,Abbreviations: ERP, event-related potential,BOLD, blood oxygen-ation level-dependent,EEG, electroencephalogram,TTI,,fMRI, functional MRI},
   title = {Assessing the spatiotemporal evolution of neuronal activation with single-trial event-related potentials and functional MRI},
   volume = {6},
   url = {www.pnas.orgcgidoi10.1073pnas.0505508102},
}
@report{,
   abstract = {The topic of multiple forms of memory is considered from a biological point of view. Fact-and-event (declarative, explicit) memory is contrasted with a collection of nonconscious (non-declarative, implicit) memory abilities including skills and habits , priming, and simple conditioning. Recent evidence is reviewed indicating that declarative and nondeclarative forms of memory have different operating characteristics and depend on separate brain systems. A brain-systems framework for understanding memory phenomena is developed in light of lesion studies involving rats, monkeys, and humans, as well as recent studies with normal humans using the divided visual field technique, event-related potentials, and positron emission to-mography (PET). m INTRODUCTION},
   author = {Larry R Squire},
   title = {Declarative and Nondeclarative Memory: Multiple Brain Systems Supporting Learning and Memory},
   url = {http://direct.mit.edu/jocn/article-pdf/4/3/232/1754979/jocn.1992.4.3.232.pdf},
}
@report{Gabrieli1998,
   abstract = {Current knowledge is summarized about long-term memory systems of the human brain, with memory systems defined as specific neural networks that support specific mnemonic processes. The summary integrates convergent evidence from neuropsychological studies of patients with brain lesions and from functional neuroimaging studies using positron emission tomography (PET) or functional magnetic resonance imaging (fMRI). Evidence is reviewed about the specific roles of hippocampal and parahippocampal regions , the amygdala, the basal ganglia, and various neocortical areas in declarative memory. Evidence is also reviewed about which brain regions mediate specific kinds of procedural memory, including sensorimotor, perceptual , and cognitive skill learning; perceptual and conceptual repetition priming ; and several forms of conditioning. Findings are discussed in terms of the functional neural architecture of normal memory, age-related changes in memory performance, and neurological conditions that affect memory such as amnesia, Alzheimer's disease, Parkinson's disease, and Huntington's disease. CONTENTS},
   author = {J D E Gabrieli},
   journal = {Annu. Rev. Psychol},
   keywords = {conditioning,declarative memory,functional brain imaging,repetition priming,skill learning},
   pages = {87-115},
   title = {COGNITIVE NEUROSCIENCE OF HUMAN MEMORY},
   volume = {49},
   year = {1998},
}
@report{Milner1998,
   author = {Brenda Milner and Larry R Squire and Eric R Kandel},
   journal = {Neuron},
   pages = {445-468},
   title = {Cognitive Neuroscience Review and the Study of Memory},
   volume = {20},
   year = {1998},
}
@generic{LaBar2006,
   abstract = {Emotional events often attain a privileged status in memory. Cognitive neuroscientists have begun to elucidate the psychological and neural mechanisms underlying emotional retention advantages in the human brain. The amygdala is a brain structure that directly mediates aspects of emotional learning and facilitates memory operations in other regions, including the hippocampus and prefrontal cortex. Emotion-memory interactions occur at various stages of information processing, from the initial encoding and consolidation of memory traces to their long-term retrieval. Recent advances are revealing new insights into the reactivation of latent emotional associations and the recollection of personal episodes from the remote past. © 2006 Nature Publishing Group.},
   author = {Kevin S. LaBar and Roberto Cabeza},
   doi = {10.1038/nrn1825},
   issn = {1471003X},
   issue = {1},
   journal = {Nature Reviews Neuroscience},
   month = {1},
   pages = {54-64},
   pmid = {16371950},
   title = {Cognitive neuroscience of emotional memory},
   volume = {7},
   year = {2006},
}
@article{Murray2020,
   abstract = {The learning of motor skills unfolds over multiple timescales, with rapid initial gains in performance followed by a longer period in which the behavior becomes more refined, habitual, and automatized. While recent lesion and inactivation experiments have provided hints about how various brain areas might contribute to such learning, their precise roles and the neural mechanisms underlying them are not well understood. In this work, we propose neural- and circuit-level mechanisms by which motor cortex, thalamus, and striatum support motor learning. In this model, the combination of fast cortical learning and slow subcortical learning gives rise to a covert learning process through which control of behavior is gradually transferred from cortical to subcortical circuits, while protecting learned behaviors that are practiced repeatedly against overwriting by future learning. Together, these results point to a new computational role for thalamus in motor learning and, more broadly, provide a framework for understanding the neural basis of habit formation and the automatization of behavior through practice.},
   author = {James M. Murray and G. Sean Escola},
   doi = {10.1038/s41467-020-19788-5},
   issn = {20411723},
   issue = {1},
   journal = {Nature Communications},
   month = {12},
   pmid = {33361766},
   publisher = {Nature Research},
   title = {Remembrance of things practiced with fast and slow learning in cortical and subcortical pathways},
   volume = {11},
   year = {2020},
}
@article{Morgan2020,
   abstract = {The brain utilizes distinct neural mechanisms that ease the transition through different stages of learning. Furthermore, evidence from category learning has shown that dissociable memory systems are engaged, depending on the structure of a task. This can even hold true for tasks that are very similar to each other, which complicates the process of classifying brain activity as relating to changes that are associated with learning or reflecting the engagement of a memory system suited for the task. The primary goals of these studies were to characterize the mechanisms that are associated with category learning and understand the extent to which different memory systems are recruited within a single task. Two studies providing spatial and temporal distinctions between learning-related changes in the brain and category-dependent memory systems are presented. The results from these experiments support the notion that exemplar memorization, rule-based, and perceptual similarity-based categorization are flexibly recruited in order to optimize performance during a single task. We conclude that these three methods, along with the memory systems they rely on, aid in the development of expertise, but their engagement might depend on the level of familiarity with a category.},
   author = {Kyle K. Morgan and Dagmar Zeithamova and Phan Luu and Don Tucker},
   doi = {10.3390/brainsci10040224},
   issn = {20763425},
   issue = {4},
   journal = {Brain Sciences},
   keywords = {Category learning,Eeg,Erp,Learning,Machine learning,Memory,Multiple memory systems,P300},
   month = {4},
   publisher = {MDPI AG},
   title = {Spatiotemporal dynamics of multiple memory systems during category learning},
   volume = {10},
   year = {2020},
}
@book_section{gerstnerchone2002,
   author = {Wulfram Gerstner and  Werner M. Kistler},
   doi = {10.1017/CBO9780511815706.002},
   journal = {Spiking Neuron Models},
   month = {8},
   pages = {1-28},
   publisher = {Cambridge University Press},
   title = {Introduction},
   url = {https://www.cambridge.org/core/product/identifier/CBO9780511815706A008/type/book_part},
   year = {2002},
}

@book{,
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-85148-3},
   editor = {G. Buzsáki and R. Llinás and W. Singer and A. Berthoz and Y. Christen},
   isbn = {978-3-642-85150-6},
   publisher = {Springer Berlin Heidelberg},
   title = {Temporal Coding in the Brain},
   url = {http://link.springer.com/10.1007/978-3-642-85148-3},
   year = {1994},
}
@report{Maass1997,
   abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology. 1. DEFINITIONS AND MOTIVATIONS If one classifies neural network models according to their computational units, one can distinguish three different generations. The first generation is based on McCulloch-Pitts neurons as computational units. These are also referred to as perceptrons or threshold gates. They give rise to a variety of neural network models such as multilayer perceptrons (also called threshold circuits), Hopfield nets, and Boltzmann machines. A characteristic feature of these models is that they can only give digital output. In fact they are universal for computations with digital input and output, and every boolean function can be computed by some multilayer perceptron with a single hidden layer. The second generation is based on computational units that apply an "activation function" with a continuous set of possible output values to a weighted sum (or polynomial) of the inputs. Common activation functions are the sigmoid function a(y) = 1/(1 + e-y) and the linear Acknowledgements: I would like to thank Eduardo Sontag and an anonymous referee for their helpful comments. Written under partial support by the Austrian Science Fund. Requests for reprints should be sent to W. Maass,},
   author = {Wolfgang Maass},
   issue = {9},
   keywords = {Computational complexity,Integrate-and-fire neutron,Lower bounds,Sigmoidal neural nets,Spiking neuron},
   pages = {1659-1671},
   title = {Networks of Spiking Neurons: The Third Generation of Neural Network Models},
   volume = {10},
   year = {1997},
}
@report{,
   abstract = {A frightening thought for a computer scientist is that there might be completely different ways of designing computing machinery, that we may miss by focusing on incremental improvements of current designs. In fact, we know that there exist much better design strategies, since the human brain has information processing capabilities that are in many aspects superior to our current computers. Furthermore the brain does not require expensive programming, debugging or replacement of failed parts, and it consumes just 10-20 Watts of energy. Unfortunately, most information processing strategies of the brain are still a mystery. In spite of many positive reports in the media, even the most basic questions concerning the organization of the computational units of the brain and the way in which the brain implements learning and memory, have not yet been answered. They are waiting to be unraveled by concerted efforts of scientists from many disciplines. Computer science is one of the disciplines from which substantial contributions are expected, and in fact other countries have established already hundreds of research facilities in the new hybrid discipline Computational Neuroscience 1 , which is dedicated to the investigation of computational principles in the brain. Computer scientists are contributing to these efforts through their experience in the evaluation of real and hypothetical systems that compute, as well as experience with robots and other machines that learn, move around, and explore their 1 http://home.earthlink.net/~perlewitz/},
   author = {Wolfgang Maass},
   title = {Computing with Spikes},
   url = {http://www.igi.TUGraz.at/maass},
}
@article{Esser2016,
   abstract = {Deep networks are now able to achieve human-level performance on a broad spectrum of recognition tasks. Independently, neuromorphic computing has now demonstrated unprecedented energy-efficiency through a new chip architecture based on spiking neurons, low precision synapses, and a scalable communication network. Here, we demonstrate that neuromorphic computing, despite its novel architectural primitives, can implement deep convolution networks that (/) approach state-of-the-art classification accuracy across eight standard datasets encompassing vision and speech, (ii) perform inference while preserving the hardware's underlying energy-efficiency and high throughput, running on the aforementioned datasets at between 1,200 and 2,600 frames/s and using between 25 and 275 mW (effectively >6,000 frames/s per Watt), and (iii') can be specified and trained using backpropagation with the same ease-of-use as contemporary deep learning. This approach allows the algorithmic power of deep learning to be merged with the efficiency of neuromorphic processors, bringing the promise of embedded, intelligent, brain-inspired computing one step closer.},
   author = {Steven K. Esser and Paul A. Merolla and John V. Arthur and Andrew S. Cassidy and Rathinakumar Appuswamy and Alexander Andreopoulos and David J. Berg and Jeffrey L. McKinstry and Timothy Melano and Davis R. Barch and Carmelo Di Nolfo and Pallab Datta and Arnon Amir and Brian Taba and Myron D. Flickner and Dharmendra S. Modha},
   doi = {10.1073/pnas.1604850113},
   issn = {10916490},
   issue = {41},
   journal = {Proceedings of the National Academy of Sciences of the United States of America},
   keywords = {Convolutional network,Neural network,Neuromorphic,Truenorth},
   month = {10},
   pages = {11441-11446},
   publisher = {National Academy of Sciences},
   title = {Convolutional networks for fast, energy-efficient neuromorphic computing},
   volume = {113},
   year = {2016},
}
@article{Zenke2018,
   abstract = {Avastmajority of computation in the brain is performed by spiking neural networks. Despite the ubiquity of such spiking, we currently lack an understanding of how biological spiking neural circuits learn and compute in vivo, as well as how we can instantiate such capabilities in artificial spiking circuits in silico. Here we revisit the problem of supervised learning in temporally coding multilayer spiking neural networks. First, by using a surrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based three-factor learning rule capable of training multilayer networks of deterministic integrate-and-fire neurons to perform nonlinear computations on spatiotemporal spike patterns. Second, inspired by recent results on feedback alignment, we compare the performance of our learning rule under different credit assignment strategies for propagating output errors to hidden units. Specifically, we test uniform, symmetric, and random feedback, finding that simpler tasks can be solved with any type of feedback, while more complex tasks require symmetric feedback. In summary, our results open the door to obtaining a better scientific understanding of learning and computation in spiking neural networks by advancing our ability to train them to solve nonlinear problems involving transformations between different spatiotemporal spike time patterns.},
   author = {Friedemann Zenke and Surya Ganguli},
   doi = {10.1162/neco_a_01086},
   issn = {1530888X},
   issue = {6},
   journal = {Neural Computation},
   month = {6},
   pages = {1514-1541},
   pmid = {29652587},
   publisher = {MIT Press Journals},
   title = {SuperSpike: Supervised learning in multilayer spiking neural networks},
   volume = {30},
   url = {https://arxiv.org/abs/1705.11146},
   year = {2018},
}
@article{Tavanaei2018,
   abstract = {In recent years, deep learning has been a revolution in the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained in a supervised manner using backpropagation. Huge amounts of labeled examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and arguably the only viable option if one wants to understand how the brain computes. SNNs are also more hardware friendly and energy-efficient than ANNs, and are thus appealing for technology, especially for portable devices. However, training deep SNNs remains a challenge. Spiking neurons' transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy, but also computational cost and hardware friendliness. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while the SNNs typically require much fewer operations.},
   author = {Amirhossein Tavanaei and Masoud Ghodrati and Saeed Reza Kheradpisheh and Timothee Masquelier and Anthony S. Maida},
   doi = {10.1016/j.neunet.2018.12.002},
   month = {4},
   title = {Deep Learning in Spiking Neural Networks},
   url = {http://arxiv.org/abs/1804.08150 http://dx.doi.org/10.1016/j.neunet.2018.12.002},
   year = {2018},
}
@article{Neftci2019,
   abstract = {Spiking neural networks are nature's versatile solution to fault-tolerant and energy efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking neural network processors attempt to emulate biological neural networks. These developments have created an imminent need for methods and tools to enable such systems to solve real-world signal processing problems. Like conventional neural networks, spiking neural networks can be trained on real, domain specific data. However, their training requires overcoming a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training spiking neural networks, and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. To that end, it gives an overview of existing approaches and provides an introduction to surrogate gradient methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.},
   author = {Emre O. Neftci and Hesham Mostafa and Friedemann Zenke},
   month = {1},
   title = {Surrogate Gradient Learning in Spiking Neural Networks},
   url = {http://arxiv.org/abs/1901.09948},
   year = {2019},
}
@article{Strubell2019,
   abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
   author = {Emma Strubell and Ananya Ganesh and Andrew McCallum},
   month = {6},
   title = {Energy and Policy Considerations for Deep Learning in NLP},
   url = {http://arxiv.org/abs/1906.02243},
   year = {2019},
}
@article{Pfeiffer2018,
   abstract = {Spiking neural networks (SNNs) are inspired by information processing in biology, where sparse and asynchronous binary signals are communicated and processed in a massively parallel fashion. SNNs on neuromorphic hardware exhibit favorable properties such as low power consumption, fast inference, and event-driven information processing. This makes them interesting candidates for the efficient implementation of deep neural networks, the method of choice for many machine learning tasks. In this review, we address the opportunities that deep spiking networks offer and investigate in detail the challenges associated with training SNNs in a way that makes them competitive with conventional deep learning, but simultaneously allows for efficient mapping to hardware. A wide range of training methods for SNNs is presented, ranging from the conversion of conventional deep networks into SNNs, constrained training before conversion, spiking variants of backpropagation, and biologically motivated variants of STDP. The goal of our review is to define a categorization of SNN training methods, and summarize their advantages and drawbacks. We further discuss relationships between SNNs and binary networks, which are becoming popular for efficient digital hardware implementation. Neuromorphic hardware platforms have great potential to enable deep spiking networks in real-world applications. We compare the suitability of various neuromorphic systems that have been developed over the past years, and investigate potential use cases. Neuromorphic approaches and conventional machine learning should not be considered simply two solutions to the same classes of problems, instead it is possible to identify and exploit their task-specific advantages. Deep SNNs offer great opportunities to work with new types of event-based sensors, exploit temporal codes and local on-chip learning, and we have so far just scratched the surface of realizing these advantages in practical applications.},
   author = {Michael Pfeiffer and Thomas Pfeil},
   doi = {10.3389/fnins.2018.00774},
   issn = {1662-4548},
   journal = {Frontiers in Neuroscience},
   month = {10},
   publisher = {Frontiers Media SA},
   title = {Deep Learning With Spiking Neurons: Opportunities and Challenges},
   volume = {12},
   year = {2018},
}
@report{Reinagel2000,
   abstract = {The amount of information a sensory neuron carries about a stimulus is directly related to response reliability. We recorded from individual neurons in the cat lateral geniculate nucleus (LGN) while presenting randomly modulated visual stimuli. The responses to repeated stimuli were reproducible, whereas the responses evoked by nonrepeated stimuli drawn from the same ensemble were variable. Stimulus-dependent information was quantified directly from the difference in entropy of these neural responses. We show that a single LGN cell can encode much more visual information than had been demonstrated previously, ranging from 15 to 102 bits/sec across our sample of cells. Information rate was correlated with the firing rate of the cell, for a consistent rate of 3.6 0.6 bits/spike (mean SD). This information can primarily be attributed to the high temporal precision with which firing probability is modulated; many individual spikes were timed with better than 1 msec precision. We introduce a way to estimate the amount of information encoded in temporal patterns of firing, as distinct from the information in the time varying firing rate at any temporal resolution. Using this method, we find that temporal patterns sometimes introduce redundancy but often encode visual information. The contribution of temporal patterns ranged from 3.4 to 25.5 bits/sec or from 9.4 to 24.9% of the total information content of the responses. Cells in the lateral geniculate nucleus of the thalamus (LGN) respond to spatial and temporal changes in light intensity within their receptive fields. The collective responses of many such cells constitute the input to visual cortex. All stimulus discrimination at the perceptual level must ultimately be supported by reliable differences in the neural response at the level of the LGN cell population. We are therefore interested in measuring the statistical discriminability of LGN responses elicited by different visual stimuli. It has been shown that the LGN can respond to visual stimuli with remarkable temporal precision (Reich et al., 1997). This implies that LGN neurons have the capability to signal information at high rates. Previous estimates of the information in LGN responses have used two general approaches. The first approach, stimulus reconstruction, relies on an explicit model of what the neuron is encoding, as well as an algorithm for decoding it (Bialek et al., 1991; Rieke et al., 1997). This method has been used to place lower bounds on the information encoded by single neurons (Rei-nagel et al., 1999) or pairs of neurons (Dan et al., 1998) in the LGN in response to dynamic visual stimuli. The second approach, the "direct" method, relies instead on statistical properties of the responses to different stimuli (the en-tropy of the responses). Because this involves only comparisons of spike trains, without reference to stimulus parameters, we need not know what features of the stimulus the cell encodes. Analysis of this type can be simplified by using a small set of stimuli and describing neural responses in terms of a few parameters, as has been done in previous studies of the LGN (Eckhorn and Pöpel, 1975; McClurkin et al., 1991). Recently, a version of the direct method has been developed that can be applied to the detailed firing patterns of neurons in response to arbitrarily complex stimuli (Strong et al., 1998). This method provides a direct measure of how much information is contained in a neural response, in the sense that the method is independent of any assumptions about what the neuron represents or how that information is represented. The information could be encoded at any temporal resolution and could involve any kind of temporal pattern. Here we apply this method to study the responses of individual LGN cells to a complex (high-entropy) temporal stimulus. Because the direct method does not constrain either the temporal resolution of the code or the role of temporal patterns, the result does not by itself tell us anything about how LGN cells encode stimuli. We therefore present two further analyses. First, we explore the temporal resolution of the neural code. Second, we introduce a measure of the contribution of temporal patterns. We distinguish three broad possibilities: (1) temporal patterns do not exist or are irrelevant to the neural code; (2) temporal patterns exist and make the neural code more redundant; or (3) temporal patterns exist and encode useful information. In our data, we find a range of results. Some cells encode information redundantly , whereas others use temporal patterns to encode visual information. In the latter case, to extract all the information from the spike trains, it would be necessary to consider temporal firing patterns; the time-varying instantaneous probability of firing would not be sufficient at any temporal resolution. MATERIALS AND METHODS Experimental Surger y and preparation. C ats were initially anesthetized with ketamine HC l (20 mg / kg, i.m.) followed by sodium thiopental (20 mg / kg, i.v., supplemented as needed and continued at 2-3 mg kg 1 hr 1 for the duration of the experiment). The animals were then ventilated through an endotracheal tube. Electrocardiograms, electroencephalograms, temperature , and expired C O 2 were monitored continuously. Animals were paralyzed with Norcuron (0.3 mg kg 1 hr 1 , i.v.). Eyes were refracted, fitted with appropriate contact lenses, and focused on a tangent screen. Electrodes were introduced through a 0.5 cm diameter craniotomy over the LGN. All surgical and experimental procedures were in accordance with National Institutes of Health and United States Department of Agriculture guidelines and were approved by the Harvard Medical Area Standing Committee on Animals. Electrical recording. Single LGN neurons in the A laminae of the LGN were recorded with plastic-coated tungsten electrodes (AM Systems, Ever-ett, WA). In some experiments, single units were recorded with electrodes of a multielectrode array (System Eckhorn Thomas Recording, Marburg, Germany). Recorded voltage signals were amplified, filtered, and passed to a personal computer running DataWave (L ongmont, C O) Discovery software , and spike times were determined to 0.1 msec resolution. Preliminary},
   author = {Pamela Reinagel and R Clay Reid},
   issue = {14},
   journal = {The Journal of Neuroscience},
   keywords = {LGN,entropy,information theory,neural coding,reliability,variability,white noise},
   pages = {5392-5400},
   title = {Temporal Coding of Visual Information in the Thalamus},
   volume = {20},
   year = {2000},
}
@report{Rucci2018,
   abstract = {Establishing a representation of space is a major goal of sensory systems. Spatial information, however, is not always explicit in the incoming sensory signals. In most modalities it needs to be actively extracted from cues embedded in the temporal flow of receptor activation. Vision, on the other hand, starts with a sophisticated optical imaging system that explicitly preserves spatial information on the retina. This may lead to the assumption that vision is predominantly a spatial process: all that is needed is to transmit the retinal image to the cortex, like uploading a digital photograph, to establish a spatial map of the world. However, this deceptively simple analogy is inconsistent with theoretical models and experiments that study visual processing in the context of normal motor behavior. We argue here that, as with other senses, vision relies heavily on temporal strategies and temporal neural codes to extract and represent spatial information.},
   author = {Michele Rucci and Ehud Ahissar and David Burr},
   doi = {10.1016/j.tics.2018.07.009},
   issn = {1879307X},
   issue = {10},
   journal = {Trends in Cognitive Sciences},
   keywords = {eye movements,microsaccade,ocular drift,retina,saccade,space perception,temporal processing,visual fixation,visual system},
   month = {10},
   pages = {883-895},
   pmid = {30266148},
   publisher = {Elsevier Ltd},
   title = {Temporal Coding of Visual Space},
   volume = {22},
   year = {2018},
}
@generic{Masquelier2011,
   abstract = {In this review, we describe our recent attempts to model the neural correlates of visual perception with biologically inspired networks of spiking neurons, emphasizing the dynamical aspects. Experimental evidence suggests distinct processing modes depending on the type of task the visual system is engaged in. A first mode, crucial for object recognition, deals with rapidly extracting the glimpse of a visual scene in the first 100 ms after its presentation. The promptness of this process points to mainly feedforward processing, which relies on latency coding, and may be shaped by spike timing-dependent plasticity (STDP). Our simulations confirm the plausibility and efficiency of such a scheme. A second mode can be engaged whenever one needs to perform finer perceptual discrimination through evidence accumulation on the order of 400 ms and above. Here, our simulations, together with theoretical considerations, show how predominantly local recurrent connections and long neural time-constants enable the integration and build-up of firing rates on this timescale. In particular, we review how a non-linear model with attractor states induced by strong recurrent connectivity provides straightforward explanations for several recent experimental observations. A third mode, involving additional top-down attentional signals, is relevant for more complex visual scene processing. In the model, as in the brain, these top-down attentional signals shape visual processing by biasing the competition between different pools of neurons. The winning pools may not only have a higher firing rate, but also more synchronous oscillatory activity. This fourth mode, oscillatory activity, leads to faster reaction times and enhanced information transfers in the model. This has indeed been observed experimentally. Moreover, oscillatory activity can format spike times and encode information in the spike phases with respect to the oscillatory cycle. This phenomenon is referred to as "phase-of-firing coding," and experimental evidence for it is accumulating in the visual system. Simulations show that this code can again be efficiently decoded by STDP. Future work should focus on continuous natural vision, bio-inspired hardware vision systems, and novel experimental paradigms to further distinguish current modeling approaches. © 2011 Masquelier, Albantakis and Deco.edu.},
   author = {Timothée Masquelier and Larissa Albantakis and Gustavo Deco},
   doi = {10.3389/fpsyg.2011.00151},
   issn = {16641078},
   issue = {JUN},
   journal = {Frontiers in Psychology},
   keywords = {Attention,Decision making,Neural coding,Neurodynamics,Oscillations,STDP,Spiking neurons,Vision},
   pmid = {21747774},
   title = {The timing of vision - how neural processing links to different temporal dynamics},
   volume = {2},
   year = {2011},
}
@report{decharms2000,
   abstract = {The principle function of the central nervous system is to represent and transform information and thereby mediate appropriate decisions and behaviors. The cerebral cortex is one of the primary seats of the internal representations maintained and used in perception, memory, decision making, motor control, and subjective experience, but the basic coding scheme by which this information is carried and transformed by neurons is not yet fully understood. This article defines and reviews how information is represented in the firing rates and temporal patterns of populations of cortical neurons, with a particular emphasis on how this information mediates behavior and experience.},
   author = {R Christopher Decharms and Anthony Zador},
   journal = {Annu. Rev. Neurosci},
   keywords = {coding,cortex,neural signal,temporal coding},
   pages = {613-647},
   title = {NEURAL REPRESENTATION AND THE CORTICAL CODE},
   volume = {23},
   year = {2000},
}
@book{,
   author = {Nikola K Kasabov},
   title = {Springer Series on Bio-and Neurosystems 7 Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence},
   url = {http://www.springer.com/series/15821},
}
@report{,
   abstract = {Biological neurons use short and sudden increases in voltage to send information. These signals are more commonly known as action potentials, spikes or pulses. Recent neurological research has shown that neurons encode information in the timing of single spikes, and not only just in their average firing frequency. This paper gives an introduction to spiking neural networks, some biological background, and will present two models of spiking neurons that employ pulse coding. Networks of spiking neurons are more powerful than their non-spiking predecessors as they can encode temporal information in their signals, but therefore do also need different and biologically more plausible rules for synaptic plasticity. You constantly receive sensory input from your environment. You process this information, recognizing food or danger , and take appropriate actions. Not only you; anything that interacts with its environment needs to do so. Mimicking such a seemingly simple mechanism in a robot proofs to be insanely difficult. Nature must laugh at our feeble attempts; animals perform this behaviour with apparent ease. The reason for this mind-boggling performance lies in their neural structure or 'brain'. Millions and millions of neurons are interconnected with each other and cooperate to efficiently process incoming signals and decide on actions. A typical neuron sends its signals out to over 10.000 other neu-rons, making it clear to even to inexpert reader that the signal flow is rather complicated. To put it mildly: we do not understand the brain that well yet. In fact, we do not even completely understand the functioning of a single neuron. The chemical activity of the synapse already proves to be infinitely more complex than firstly assumed. However, the rough concept of how neurons work is understood: neurons send out short pulses of electrical energy as signals, if they have received enough of these themselves. This basically simple mechanism has been moulded into a mathematical model for computer use. Artificial as these computerised neurons are, we refer to them as networks of artificial neurons, or artificial neural networks. We will sketch a short history of these now; the biological background of the real neuron will be drawn in the next chapter. Generations of artificial neurons Artificial neural networks are already becoming a fairly old technique within computer science; the first ideas and models are over fifty years old. The first generation of artificial neural networks consisted of McCulloch-Pitts threshold neu-rons [15], a conceptually very simple model: a neuron sends a binary 'high' signal if the sum of its weighted incoming signals rises above a threshold value. Even though these neurons can only give digital output, they have been successfully applied in powerful artificial neural networks like multi-layer perceptrons and Hopfield nets. For example, any function with Boolean output can be computed by a multi-layer perceptron with a single hidden layer; these networks are called universal for digital computations. Neurons of the second generation do not use a step-or threshold function to compute their output signals, but a continuous activation function, making them suitable for analog in-and output. Commonly used examples of activation functions are the sigmoid and hyperbolic tangent. Typical examples of neural networks consisting of neurons of these types are feed-forward and recurrent neural networks. These are more powerful than their first generation predecessors: when equipped with a threshold function at the output layer of the network they are universal for digital computations , and do so with fewer neurons than a network of the first generation [14]. In addition they can approximate any analog function arbitrarily well, making these networks universal for analog computations. Neuron models of the first two generations do not employ individual pulses, but their output signals typically lie between 0 and 1. These signals can be seen as normalized firing rates (frequencies) of the neuron within a certain period of time. This is a so-called rate coding, where a higher rate of firing correlates with a higher output signal. Rate coding implies an averaging mechanism, as real spikes work binary: spike, or no spike, there is no intermediate. Due to such an averaging window mechanism the output value of a neuron can be calculated in iteration. After such a cycle for each neu-ron the 'answer' of the network to the input values is known. Real neurons have a base firing-rate (an intermediate frequency of pulsing) and continuous activation functions can model these intermediate output frequencies. Hence, neu-rons of the second generation are more biologically realistic and powerful than neurons of the first generation [3].},
   author = {Jilles Vreeken},
   title = {Spiking neural networks, an introduction},
}
@article{Jang2019,
   abstract = {Spiking neural networks (SNNs) are distributed trainable systems whose computing elements, or neurons, are characterized by internal analog dynamics and by digital and sparse synaptic communications. The sparsity of the synaptic spiking inputs and the corresponding event-driven nature of neural processing can be leveraged by energy-efficient hardware implementations, which can offer significant energy reductions as compared to conventional artificial neural networks (ANNs). The design of training algorithms lags behind the hardware implementations. Most existing training algorithms for SNNs have been designed either for biological plausibility or through conversion from pretrained ANNs via rate encoding. This article provides an introduction to SNNs by focusing on a probabilistic signal processing methodology that enables the direct derivation of learning rules by leveraging the unique time-encoding capabilities of SNNs. We adopt discrete-time probabilistic models for networked spiking neurons and derive supervised and unsupervised learning rules from first principles via variational inference. Examples and open research problems are also provided.},
   author = {Hyeryung Jang and Osvaldo Simeone and Brian Gardner and André Grüning},
   doi = {10.1109/MSP.2019.2935234},
   month = {10},
   title = {An Introduction to Probabilistic Spiking Neural Networks: Probabilistic Models, Learning Rules, and Applications},
   url = {http://arxiv.org/abs/1910.01059 http://dx.doi.org/10.1109/MSP.2019.2935234},
   year = {2019},
}
@report{Meunier2002,
   abstract = {Hodgkin and Huxley (H–H) model for action potential generation has held firmfor half a century because this relatively simple and experimentally testablemodel embodies the major features of membrane nonlinearity: namely,voltage-dependent ionic currents that activate and inactivate in time. However,experimental and theoretical developments of the past 20 years force one tore-evaluate its usefulness. First, the H–H model is, in its original form, limited tothe two voltage-dependent currents found in the squid giant axon and it mustbe extended significantly if it is to deal with the excitable soma and dendrites ofneurons. Second, the macroscopic and deterministic H–H model does notcapture correctly the kinetics of the Na++channel and it cannot account for thestochastic response to current injection that arises from the discrete nature ofion channels. Third, much simpler integrate-and-fire-type models seem to bemore useful for exploring collective phenomena in neuronal networks. Is theH–H model threatened, or will it continue to set the fundamental framework forexploring neuronal excitability?},
   author = {Claude Meunuer and Claude Meunuer},
   journal = {TRENDS in Neurosciences},
   title = {Playing the Devil’s advocate: is theHodgkin–Huxley model useful?},
   url = {http://tins.trends.com},
   year = {2002},
}
@report{limit1993,
   abstract = {A standard membrane model, based on the continuous deterministic Hodgkin-Huxley equations, is compared to an alternative membrane model, based on discrete stochastic ion channel populations represented through Marlcov processes. Simulations explore the relationship between these two levels of description: the behavior predicted by the macroscopic membrane currents versus the behavior predicted by their microscopic ion channels. Discussion considers the extent to which these random events underlying neural signals mediate random events in neural computation.},
   author = {Adam F Strassberg and Louis J Defelice},
   title = {ARTICLE Limitations of the Hodgkin-Huxley Formalism: Effects of Single Channel Kinetics on Transmembrane Voltage Dynamics},
}
@report{NelsonM,
   author = {Mark Nelson and John Rinzel},
   title = {The Hodgkin-Huxley Model},
}
@article{Levi2018,
   abstract = {Neurological disorders affect millions of people which influence their cognitive and/or motor capabilities. The realization of a prosthesis must consider the biological activity of the cells and the connection between machine and biological cells. Biomimetic neural network is one solution in front of neurological diseases. The neuron replacement should be processed by reproducing the timing and the shape of the spike. Several mathematical equations which model neural activities exist. The most biologically plausible one is the Hodgkin–Huxley (HH) model. The connection between electrical devices and living cells require a tunable real-time system. The field programmable gate array (FPGA) is a nice component including flexibility, speed and stability. Here, we propose an implementation of HH neurons in FPGA serving as a presage for a modulating network opening a large scale of possibilities such as damage cells replacement and the study of the effect of the cells disease on the neural network.},
   author = {Timothée Levi and Farad Khoyratee and Sylvain Saïghi and Yoshiho Ikeuchi},
   doi = {10.1007/s10015-017-0397-7},
   issn = {16147456},
   issue = {1},
   journal = {Artificial Life and Robotics},
   keywords = {FPGA,Hodgkin–Huxley,Neurological diseases,Silicon neuron},
   month = {3},
   pages = {10-14},
   publisher = {Springer Tokyo},
   title = {Digital implementation of Hodgkin–Huxley neuron model for neurological diseases studies},
   volume = {23},
   year = {2018},
}

@article{Yedjour2017,
   abstract = {In this paper, we propose a spiking neural network model for edge detection in images. The proposed model is biologically inspired by the mechanisms employed by natural vision systems, more specifically by the biologically fulfilled function of simple cells of the human primary visual cortex that are selective for orientation. Several aspects are studied in this model according to three characteristics: feedforward spiking neural structure; conductance-based model of the Hodgkin–Huxley neuron and Gabor receptive fields structure. A visualized map is generated using the firing rate of neurons representing the orientation map of the visual cortex area. We have simulated the proposed model on different images. Successful computer simulation results are obtained. For comparison, we have chosen five methods for edge detection. We finally evaluate and compare the performances of our model toward contour detection using a public dataset of natural images with associated contour ground truths. Experimental results show the ability and high performance of the proposed network model.},
   author = {Hayat Yedjour and Boudjelal Meftah and Olivier Lézoray and Abdelkader Benyettou},
   doi = {10.1007/s10339-017-0803-z},
   issn = {16124790},
   issue = {3},
   journal = {Cognitive Processing},
   keywords = {Computational neuroscience,Edge detection,Gabor function,Hodgkin–Huxley model,Spiking neural networks,Visual cortex},
   month = {8},
   pages = {315-323},
   pmid = {28374125},
   publisher = {Springer Verlag},
   title = {Edge detection based on Hodgkin–Huxley neuron model simulation},
   volume = {18},
   year = {2017},
}
@report{reduction1997,
   abstract = {It is generally believed that a neuron is a threshold element that fires when some variable u reaches a threshold. Here we pursue the question of whether this picture can be justified and study the four-dimensional neuron model of Hodgkin and Huxley as a concrete example. The model is approximated by a response kernel expansion in terms of a single variable , the membrane voltage. The first-order term is linear in the input and its kernel has the typical form of an elementary postsynaptic potential. Higher-order kernels take care of nonlinear interactions between input spikes. In contrast to the standard Volterra expansion, the kernels depend on the firing time of the most recent output spike. In particular, a zero-order kernel that describes the shape of the spike and the typical after-potential is included. Our model neuron fires if the membrane voltage, given by the truncated response kernel expansion, crosses a threshold. The threshold model is tested on a spike train generated by the Hodgkin-Huxley model with a stochastic input current. We find that the threshold model predicts 90 percent of the spikes correctly. Our results show that, to good approximation, the description of a neuron as a threshold element can indeed be justified.},
   author = {Werner M Kistler and Wulfram Gerstner and J Leo Van Hemmen},
   title = {Reduction of the Hodgkin-Huxley Equations to a Single-Variable Threshold Model},
   url = {http://direct.mit.edu/neco/article-pdf/9/5/1015/813679/neco.1997.9.5.1015.pdf},
}
@article{Johnson2017,
   abstract = {Artificial neural networks, or ANNs, have grown a lot since their inception back in the 1940s. But no matter the changes, one of the most important components of neural networks is still the node, which represents the neuron. Within spiking neural networks, the node is especially important because it contains the functions and properties of neurons that are necessary for their network. One important aspect of neurons is the ionic flow which produces action potentials, or spikes. Forces of diffusion and electrostatic pressure work together with the physical properties of the cell to move ions around changing the cell membrane potential which ultimately produces the action potential. This tutorial reviews the Hodkgin-Huxley model and shows how it simulates the ionic flow of the giant squid axon via four differential equations. The model is implemented in Matlab using Euler's Method to approximate the differential equations. By using Euler's method, an extra parameter is created, the time step. This new parameter needs to be carefully considered or the results of the node may be impaired.},
   author = {Melissa G. Johnson and Sylvain Chartier},
   doi = {10.20982/tqmp.13.2.p105},
   issn = {2292-1354},
   issue = {2},
   journal = {The Quantitative Methods for Psychology},
   month = {5},
   pages = {105-119},
   publisher = {The Quantitative Methods for Psychology},
   title = {Spike neural models (part I): The Hodgkin-Huxley model},
   volume = {13},
   year = {2017},
}
@report{pattern2016,
   abstract = {Recent findings about using memristor devices to mimic biological synapses in neuromorphic systems open a new vision in neuroscience. Ultra-dense learning architectures can be implemented through the Spike-Timing-Dependent-Plasticity (STDP) mechanism by exploiting these nanoscale nonvolatile devices. In this paper, a Spiking Neural Network (SNN) that uses biologically plausible mechanisms is implemented. The proposed SNN relies on Hodgkin-Huxley neurons and memristor-based synapses to implement a bio-inspired neuromorphic platform. The behavior of the proposed SNN and its learning mechanism are discussed, and test results are provided to show the effectiveness of the proposed design for pattern classification applications.},
   author = {Amirali Amirsoleimani and Majid Ahmadi and Arash Ahmadi and Mounir Boukadoum},
   keywords = {Hudgkin Huxley,Memristor,Spike-Timing-Dependent-Plasticity (STDP),Spiking Neural Network (SNN).},
   title = {Brain-inspired Pattern Classification with Memristive Neural Network Using the Hodgkin-Huxley Neuron},
}

@article{squid,
   author = {by David Beeman},
   issn = {1861-1680},
   journal = {GENESIS Modeling Tutorial. Brains, Minds and Media},
   pages = {220},
   title = {Supplementary Material for bmm218: Introduction to Realistic Neural Modeling and bmm220: GENESIS Modeling Tutorial GENESIS Resources Installation Guidelines Datasheet GENESIS Resources},
   volume = {1},
   url = {http://www.dipp.nrw.de/lizenzen/dppl/dppl/DPPL_v2_en_06-2004.html.},
   year = {2005},
}

@article{Doutsi2021,
   abstract = {This paper introduces a novel coding/decoding mechanism that mimics one of the most important properties of the human visual system: its ability to enhance the visual perception quality in time. In other words, the brain takes advantage of time to process and clarify the details of the visual scene. This characteristic is yet to be considered by the state-of-the-art quantization mechanisms that process the visual information regardless the duration of time it appears in the visual scene. We propose a compression architecture built of neuroscience models; it first uses the leaky integrate-and-fire (LIF) model to transform the visual stimulus into a spike train and then it combines two different kinds of spike interpretation mechanisms (SIM), the time-SIM and the rate-SIM for the encoding of the spike train. The time-SIM allows a high quality interpretation of the neural code and the rate-SIM allows a simple decoding mechanism by counting the spikes. For that reason, the proposed mechanisms is called Dual-SIM quantizer (Dual-SIMQ). We show that (i) the time-dependency of Dual-SIMQ automatically controls the reconstruction accuracy of the visual stimulus, (ii) the numerical comparison of Dual-SIMQ to the state-of-the-art shows that the performance of the proposed algorithm is similar to the uniform quantization schema while it approximates the optimal behavior of the non-uniform quantization schema and (iii) from the perceptual point of view the reconstruction quality using the Dual-SIMQ is higher than the state-of-the-art.},
   author = {Effrosyni Doutsi and Lionel Fillatre and Marc Antonini and Panagiotis Tsakalides},
   doi = {10.1109/tip.2021.3070193},
   issn = {1057-7149},
   journal = {IEEE Transactions on Image Processing},
   month = {4},
   pages = {4305-4315},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {Dynamic Image Quantization Using Leaky Integrate-and-Fire Neurons},
   volume = {30},
   year = {2021},
}
@article{Liu2019,
   abstract = {Recent studies have shown that the electronic hardware devices can be compromised by the faults and fault tolerance is a crucial capability. This paper addresses the challenge of fault detection in the CMOS circuits, using two bio-inspired structures based on the HP lab's memristor and the BSIM3v3.2.2 transistor models. The first fault detection circuit (FDC) includes the memristor-based synapses and a modified leaky integrate-and-fire (LIF)-based neuron. The memristor-based synapse circuits can be further optimized which is the proposed second fault detection method (O-FDC), and it has a lower hardware overhead and power consumption compared to the former. Experimental results demonstrate that the proposed structures can detect the circuit faults under the inputs of direct current (DC), alternating current (AC) voltage sources, and pulse signals. Under the input of DC, the fault detection times for the two proposed structures are about 0.16 ms and 1.2 ms, respectively; when the input source is AC, the corresponding fault detection times are about 0.206 ms and 0.758 ms; and it takes only 6.47us for fault detection under the input of pulse signals. This work provides an alternative solution to enhance the fault-tolerant capability of the hardware systems.},
   author = {Junxiu Liu and Yongchuang Huang and Yuling Luo and Jim Harkin and Liam McDaid},
   doi = {10.1016/j.neucom.2018.11.078},
   issn = {18728286},
   journal = {Neurocomputing},
   keywords = {Fault detection,Memristive synapses,Memristor,Spiking neuron model},
   month = {2},
   pages = {473-482},
   publisher = {Elsevier B.V.},
   title = {Bio-inspired fault detection circuits based on synapse and spiking neuron models},
   volume = {331},
   year = {2019},
}
@article{Chu2015,
   abstract = {This paper presents a neuromorphic system for visual pattern recognition realized in hardware. A new learning rule based on modified spike-timing-dependent plasticity is also presented and implemented with passive synaptic devices. The system includes an artificial photoreceptor, a Pr0.7Ca0.3MnO3-based memristor array, and CMOS neurons. The artificial photoreceptor consisting of a CMOS image sensor and a field-programmable gate array converts an image into spike signals, and the memristor array is used to adjust the synaptic weights between the input and output neurons according to the learning rule. A leaky integrate-and-fire model is used for the output neuron that is built together with the image sensor on a single chip. The system has 30 input neurons that are interconnected to 10 output neurons through 300 memristors. Each input neuron corresponding to a pixel in a 5 × 6 pixel image generates voltage pulses according to the pixel value. The voltage pulses are then weighted and integrated by the memristors and the output neurons, respectively, to be compared with a certain threshold voltage above which an output neuron fires. The system has been successfully demonstrated by training and recognizing number images from 0 to 9.},
   author = {Myonglae Chu and Byoungho Kim and Sangsu Park and Hyunsang Hwang and Moongu Jeon and Byoung Hun Lee and Byung Geun Lee},
   doi = {10.1109/TIE.2014.2356439},
   issn = {02780046},
   issue = {4},
   journal = {IEEE Transactions on Industrial Electronics},
   keywords = {Complimentary metal-oxide-semiconductor (CMOS) image sensor,leaky integrate-and-fire (I-F) neurons,memristor,neural network,neuromorphic,pattern recognition,spike-timing-dependent plasticity (STDP)},
   pages = {2410-2419},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Neuromorphic Hardware System for Visual Pattern Recognition with Memristor Array and CMOS Neuron},
   volume = {62},
   year = {2015},
}
@article{Yang2020,
   abstract = {Artificial neuron is an important part of constructing neuromorphic network in which information can be computed with high parallelism and efficiency like in the human brain. However, owing to the poor biological plausibility, artificial neurons based on traditional complementary metal-oxide-semiconductor (CMOS) platform fail to reveal the rich ion dynamics of the biological counterparts. Organic–inorganic halide perovskites (OHPs) are prospective for imitating the ion dynamics on the membranes of biological neurons because of its intrinsic ion migration. Herein, a diffusive CH3NH3PbI3(MAPbI3)-based memristor with superior amplitude-frequency characteristics and highly linear conductivity modulation for more than 1000 states have been fabricated for the construction of a leaky integrate-and-fire (LIF) bio-inspired neuron. The as-designed LIF model can successfully emulate the leakage, spatiotemporal integration and firing functions in a biological neuron. Moreover, by connecting LIF neurons with a 2*2 non-volatile Al2O3-based synaptic array, a simple spiking neural network (SNN) which is called the 3rd generation of neural network has been implemented at the hardware level to study the cognitive performance of the network. The SNN exhibits outstanding selective sensitivity to particular input sequence, indicating the excellent adaptability and versatility of the network for future applications of neuromorphic computing by utilizing novel ionotropic device.},
   author = {Jia Qin Yang and Ruopeng Wang and Zhan Peng Wang and Qin Yuan Ma and Jing Yu Mao and Yi Ren and Xiaoyang Yang and Ye Zhou and Su Ting Han},
   doi = {10.1016/j.nanoen.2020.104828},
   issn = {22112855},
   journal = {Nano Energy},
   keywords = {Leaky integrate-and-fire neurons,Memristor,Neuromorphic computing,Organic–inorganic halide perovskite,Solution process},
   month = {8},
   publisher = {Elsevier Ltd},
   title = {Leaky integrate-and-fire neurons based on perovskite memristor for spiking neural networks},
   volume = {74},
   year = {2020},
}
@article{Nahmias2013,
   abstract = {We propose an original design for a neuron-inspired photonic computational primitive for a large-scale, ultrafast cognitive computing platform. The laser exhibits excitability and behaves analogously to a leaky integrate-and-fire (LIF) neuron. This model is both fast and scalable, operating up to a billion times faster than a biological equivalent and is realizable in a compact, vertical-cavity surface-emitting laser (VCSEL). We show that - under a certain set of conditions - the rate equations governing a laser with an embedded saturable absorber reduces to the behavior of LIF neurons. We simulate the laser using realistic rate equations governing a VCSEL cavity, and show behavior representative of cortical spiking algorithms simulated in small circuits of excitable lasers. Pairing this technology with ultrafast, neural learning algorithms would open up a new domain of processing. © 2012 IEEE.},
   author = {Mitchell A. Nahmias and Bhavin J. Shastri and Alexander N. Tait and Paul R. Prucnal},
   doi = {10.1109/JSTQE.2013.2257700},
   issn = {1077260X},
   issue = {5},
   journal = {IEEE Journal on Selected Topics in Quantum Electronics},
   keywords = {Cognitive computing,excitability,leaky integrate-and-fire (LIF) neuron,mixed-signal,neural networks,neuromorphic,optoelectronics,photonic neuron,semiconductor lasers,spike processing,ultrafast,vertical-cavity surface-emitting lasers (VCSELs)},
   title = {A leaky integrate-and-fire laser neuron for ultrafast cognitive computing},
   volume = {19},
   year = {2013},
}
@report{sound2003,
   abstract = {We present a neurally inspired technique for detecting onsets and amplitude modulation in sound. This starts with a cochlea-like filter. The outputs from this filter are spike coded, in a way similar to the auditory nerve. These AN-like spikes are presented to leaky integrate-and-fire (LIF) neurons through a depressing synapse. The spike outputs from these are then processed by another layer of LIF neurons. Onsets are detected with essentially zero latency. Amplitude modulation is detected in a way similar to that of onset chopper cells. We present results from some of the TIMIT database.},
   author = {Leslie S Smith and Dagmar S Fraser},
   title = {Sound feature detection using leaky integrate-and-fire neurons},
}
@article{Chatterjee2019,
   abstract = {The fundamental building block of an artificial spiking neural network (SNN) is an element which can effectively mimic a biological neuron. There are several electronic and spintronic devices which have been demonstrated as a neuron. But the main concern here is the energy consumption and large area of those artificial neurons. In this letter, we propose and demonstrate a highly scalable and CMOS compatible bulk FinFET with an n+ buried layer for ultra low energy artificial neuron using well calibrated TCAD simulations. The proposed device shows the signature spiking frequency versus input voltage curve of a biological neuron. The energy per spike of the integrate block of the proposed leaky integrate and fire (LIF) neuron is 6.3fJ/spike which is the minimum reported till date.},
   author = {Dibyendu Chatterjee and Anil Kottantharayil},
   doi = {10.1109/LED.2019.2924259},
   issn = {15580563},
   issue = {8},
   journal = {IEEE Electron Device Letters},
   keywords = {Bulk FinFET,impact ionization,leaky integrate and fire neuron,spiking neural network (SNN)},
   month = {8},
   pages = {1301-1304},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A CMOS Compatible Bulk FinFET-Based Ultra Low Energy Leaky Integrate and Fire Neuron for Spiking Neural Networks},
   volume = {40},
   year = {2019},
}
@book{pattern2007,
   abstract = {Formerly known as ICEEE.},
   author = {Institute of Electrical and Electronics Engineers.},
   isbn = {9781424473144},
   publisher = {[IEEE]},
   title = {Electrical Engineering Computing Science and Automatic Control (CCE), 2010 7th International Conference on : date, 8-10 Sept. 2010.},
   year = {2010},
}
@article{Rozenberg2019,
   abstract = {We introduce an ultra-compact electronic circuit that realizes the leaky-integrate-and-fire model of artificial neurons. Our circuit has only three active devices, two transistors and a silicon controlled rectifier (SCR). We demonstrate the implementation of biologically realistic features, such as spike-frequency adaptation, a refractory period and voltage modulation of spiking rate. All characteristic times can be controlled by the resistive parameters of the circuit. We built the circuit with out-of-the-shelf components and demonstrate that our ultra-compact neuron is a modular block that can be associated to build multi-layer deep neural networks. We also argue that our circuit has low power requirements, as it is normally off except during spike generation. Finally, we discuss the ultimate ultra-compact limit, which may be achieved by further replacing the SCR circuit with Mott materials.},
   author = {M. J. Rozenberg and O. Schneegans and P. Stoliar},
   doi = {10.1038/s41598-019-47348-5},
   issn = {20452322},
   issue = {1},
   journal = {Scientific reports},
   month = {7},
   pages = {11123},
   pmid = {31366958},
   publisher = {NLM (Medline)},
   title = {An ultra-compact leaky-integrate-and-fire model for building spiking neural networks},
   volume = {9},
   year = {2019},
}
@article{Mullowney2008,
   abstract = {The Ornstein-Uhlenbeck process has been proposed as a model for the spontaneous activity of a neuron. In this model, the firing of the neuron corresponds to the first passage of the process to a constant boundary, or threshold. While the Laplace transform of the first-passage time distribution is available, the probability distribution function has not been obtained in any tractable form. We address the problem of estimating the parameters of the process when the only available data from a neuron are the interspike intervals, or the times between firings. In particular, we give an algorithm for computing maximum likelihood estimates and their corresponding confidence regions for the three identifiable (of the five model) parameters by numerically inverting the Laplace transform. A comparison of the two-parameter algorithm (where the time constant τ is known a priori) to the three-parameter algorithm shows that significantly more data is required in the latter case to achieve comparable parameter resolution as measured by 95% confidence intervals widths. The computational methods described here are a efficient alternative to other well known estimation techniques for leaky integrate-and-fire models. Moreover, it could serve as a template for performing parameter inference on more complex integrate-and-fire neuronal models. © Springer Science+Business Media, LLC 2007.},
   author = {Paul Mullowney and Satish Iyengar},
   doi = {10.1007/s10827-007-0047-5},
   issn = {09295313},
   issue = {2},
   journal = {Journal of Computational Neuroscience},
   keywords = {Inverse Laplace transform,Ornstein-Uhlenbeck process,Parameter inference},
   month = {4},
   pages = {179-194},
   pmid = {17661164},
   title = {Parameter estimation for a leaky integrate-and-fire neuronal model from ISI data},
   volume = {24},
   year = {2008},
}
@article{Brunel2007,
   author = {Nicolas Brunel and Mark C.W. Van Rossum},
   doi = {10.1007/s00422-007-0189-6},
   issn = {03401200},
   issue = {5-6},
   journal = {Biological Cybernetics},
   month = {12},
   pages = {341-349},
   pmid = {18046573},
   title = {Quantitative investigations of electrical nerve excitation treated as polarization},
   volume = {97},
   year = {2007},
}
@report{Chaturvedi2012,
   abstract = {It is well known that biological neurons use pulses or spikes to encode information. Neurological research shows that the biological neurons store information in the timing of spikes. Spiking neural networks belong to the third generation of neural networks in which computation is performed in the temporal (time related) domain and relies on the timings between spikes. The leaky integrate-and-fire neuron is probably the best-known example of a formal spiking neuron model. In this work, we have presented the simulation of LIF model of SNN for performing the image segmentation using clustering techniques. Image segmentation is a classic inverse problem which consists of achieving a compact region based description of the image scene by decomposing it into meaningful or spatially coherent regions sharing similar attributes. Because of its simplicity and efficiency, clustering approach is used for the segmentation of (textured) natural images. After the selection and the extraction of the image features , the feature samples, handled as vectors, are grouped together in compact but well-separated clusters corresponding to each class of the image and therefore the results demonstrate how SNN can be applied with efficacy in image segmentation with better accuracy.},
   author = {Soni A Chaturvedi and Soni Chaturvedi and Meftah Boudjelal and A A Khurshid},
   issue = {1},
   journal = {International Journal of Wisdom Based Computing},
   keywords = {Clustering,K-Means Algorithm,Spike; Integrate and fire,Spiking Neural Network(SNN),time to first spike Segmentation},
   title = {Image Segmentation using Leaky Integrate and Fire Model of Spiking Neural Network},
   volume = {2},
   url = {https://www.researchgate.net/publication/316284219},
   year = {2012},
}
@report{SVM2014,
   abstract = {Classification is recursively an impulse decision making tasks of human quickness. In today's world it is the gaining interest of active research with the areas of neural network. Classification and recognition of objects plays a vital role in computer vision research. In Neural Network , Spiking Neural networks are found to be exquisitely beneficial in impregnable classification of data. In this paper we will classify and recognise the object using Support Vector Machine (SVM) and one of the most popular model of SNN which is the Leaky-Integrate and Fire (LIF) model.SVM are supervised learning models that are used for classification of images. SNNs incorporate the concept of time into their operating model. LIF model is easy to implement have good computational efficiency. This paper depicts comparison of SVM and SNN model LIF for object classification and recognition.},
   author = {Mrs S Chaturvedi and A A Khurshid},
   keywords = {Electronics RCOEM,India Email Id:aakhurshid@gmailcom 2 Keywords: Support Vector Machine (SVM),Leaky Integrate and Fire Neuron Model (LIF),Nagpur,Professor and head,Spiking Neural Network(SNN)},
   title = {Comparison of Support Vector Machine and Leaky-Integrated & Fire SNN model for Object Recognition},
   url = {https://www.researchgate.net/publication/316283981},
}
@report{subthreshold2005,
   abstract = {Many spiking neuron models have been proposed over the last decades with varying computational complexity and abstraction from biological neurons. Among the few studies that have compared spiking models, little emphasis has been given to the formal description of calibration methods in tuning model parameters. We give an example of calibrating a leaky integrate-and-fire neuron with the first-spike time of a Hodgkin-Huxley neuron. We further demonstrate how model parameters can be tuned to minimize subthreshold differences in membrane potential. This example emphasizes the dependencies of calibration methods on other experimental parameters, complicating detailed comparisons of spiking models.},
   author = {Dominic I Standage and Thomas P Trappenberg},
   keywords = {Keywords},
   title = {Differences in the subthreshold dynamics of leaky integrate-and-fire and Hodgkin-Huxley neuron mo els},
}
@report{,
   abstract = {The leaky integrate-and-fire (LIF) model of neuronal spiking (Stein 1967) provides an analytically tractable formalism of neuronal firing rate in terms of a neuron's membrane time constant, threshold, and refractory period. LIF neurons have mainly been used to model physiologically realistic spike trains, but little application of the LIF model appears to have been made in explicitly computational contexts. In this article, we show that the transfer function of a LIF neuron provides, over a wide-parameter range, a compressive nonlinearity sufficiently close to that of the logarithm so that LIF neurons can be used to multiply neural signals by mere addition of their outputs yielding the logarithm of the product. A simulation of the LIF multiplier shows that under a wide choice of parameters , a LIF neuron can log-multiply its inputs to within a 5% relative error.},
   author = {Doron Tal and Eric L Schwartz},
   title = {Computing with the Leaky Integrate-and-Fire Neuron: Logarithmic Computation and Multiplication},
   url = {http://direct.mit.edu/neco/article-pdf/9/2/305/813551/neco.1997.9.2.305.pdf},
}

@article{Haghiri2018,
   abstract = {Fast speed and a high accuracy implementation of biological plausible neural networks are vital key objectives to achieve new solutions to model, simulate and cure the brain diseases. Efficient hardware implementation of spiking neural networks is a significant approach in biological neural networks. This paper presents a multiplierless noisy Izhikevich neuron (MNIN) model, which is used for the digital implementation of biological neural networks in large scale. Simulation results show that the MNIN model reproduces the same operations of the original noisy Izhikevich neuron. The proposed model has a low-cost hardware implementation property compared with the original neuron model. The field-programmable gate array realization results demonstrated that the MNIN model follows the different spiking patterns appropriately.},
   author = {Saeed Haghiri and Abdulhamid Zahedi and Ali Naderi and Arash Ahmadi},
   doi = {10.1109/TBCAS.2018.2868746},
   issn = {19324545},
   issue = {6},
   journal = {IEEE Transactions on Biomedical Circuits and Systems},
   keywords = {FPGA,Neuron,coupling behaviors,noisy Izhikevich model},
   month = {12},
   pages = {1422-1430},
   pmid = {30188839},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Multiplierless Implementation of Noisy Izhikevich Neuron with Low-Cost Digital Design},
   volume = {12},
   year = {2018},
}
@article{Valadez-Godinez2020,
   abstract = {Since more than a decade ago, three statements about spiking neuron (SN) implementations have been widely accepted: 1) Hodgkin and Huxley (HH) model is computationally prohibitive, 2) Izhikevich (IZH) artificial neuron is as efficient as Leaky Integrate-and-Fire (LIF) model, and 3) IZH model is more efficient than HH model (Izhikevich, 2004). As suggested by Hodgkin and Huxley (1952), their model operates in two modes: by using the α’s and β’s rate functions directly (HH model) and by storing them into tables (HHT model) for computational cost reduction. Recently, it has been stated that: 1) HHT model (HH using tables) is not prohibitive, 2) IZH model is not efficient, and 3) both HHT and IZH models are comparable in computational cost (Skocik & Long, 2014). That controversy shows that there is no consensus concerning SN simulation capacities. Hence, in this work, we introduce a refined approach, based on the multiobjective optimization theory, describing the SN simulation capacities and ultimately choosing optimal simulation parameters. We have used normalized metrics to define the capacity levels of accuracy, computational cost, and efficiency. Normalized metrics allowed comparisons between SNs at the same level or scale. We conducted tests for balanced, lower, and upper boundary conditions under a regular spiking mode with constant and random current stimuli. We found optimal simulation parameters leading to a balance between computational cost and accuracy. Importantly, and, in general, we found that 1) HH model (without using tables) is the most accurate, computationally inexpensive, and efficient, 2) IZH model is the most expensive and inefficient, 3) both LIF and HHT models are the most inaccurate, 4) HHT model is more expensive and inaccurate than HH model due to α’s and β’s table discretization, and 5) HHT model is not comparable in computational cost to IZH model. These results refute the theory formulated over a decade ago (Izhikevich, 2004) and go more in-depth in the statements formulated by Skocik and Long (2014). Our statements imply that the number of dimensions or FLOPS in the SNs are theoretical but not practical indicators of the true computational cost. The metric we propose for the computational cost is more precise than FLOPS and was found to be invariant to computer architecture. Moreover, we found that the firing frequency used in previous works is a necessary but an insufficient metric to evaluate the simulation accuracy. We also show that our results are consistent with the theory of numerical methods and the theory of SN discontinuity. Discontinuous SNs, such LIF and IZH models, introduce a considerable error every time a spike is generated. In addition, compared to the constant input current, the random input current increases the computational cost and inaccuracy. Besides, we found that the search for optimal simulation parameters is problem-specific. That is important because most of the previous works have intended to find a general and unique optimal simulation. Here, we show that this solution could not exist because it is a multiobjective optimization problem that depends on several factors. This work sets up a renewed thesis concerning the SN simulation that is useful to several related research areas, including the emergent Deep Spiking Neural Networks.},
   author = {Sergio Valadez-Godínez and Humberto Sossa and Raúl Santiago-Montero},
   doi = {10.1016/j.neunet.2019.09.026},
   issn = {18792782},
   journal = {Neural Networks},
   keywords = {Accuracy,Computational cost,Numerical method,Simulation,Spiking neuron,Time step},
   month = {2},
   pages = {196-217},
   pmid = {31689679},
   publisher = {Elsevier Ltd},
   title = {On the accuracy and computational cost of spiking neuron implementation},
   volume = {122},
   year = {2020},
}
@inproceedings{,
   abstract = {Some spiking neuron models have proved to solve different linear and non-linear pattern recognition problems. Indeed, only one spiking neuron can generate comparable results as classical artificial neural network. However, depending on the classification problem, one spiking model could be better or less efficient than other. In this paper we propose a methodology to create spiking neuron models using Gene Expression Programming. The new models created are applied in eight pattern recognition problems. The results obtained are compared with previous results generated adopting the Izhikevich spiking neuron model. This first effort will help us to generate spiking neuron models which will be adaptable to a specific pattern recognition problem. © 2013 IEEE.},
   author = {Josafath I. Espinosa-Ramos and Nareli Cruz-Cortes and Roberto A. Vazquez},
   doi = {10.1109/IJCNN.2013.6706795},
   isbn = {9781467361293},
   journal = {Proceedings of the International Joint Conference on Neural Networks},
   title = {Creation of spiking neuron models applied in pattern recognition problems},
   year = {2013},
}
@book{Elnabawy2018,
   abstract = {"IEEE Catalog Number: CFP18NEW-USB"--PDF copyright page.},
   author = {IEEE Circuits and Systems Society and  Institute of Electrical and Electronics Engineers},
   isbn = {9781538648599},
   title = {2018 16th IEEE International New Circuits and Systems Conference (NEWCAS) : 24-27 June 2018.},
}
@article{Godinez2017,
   abstract = {It is known that, depending on the numerical method, the simulation accuracy of a spiking neuron increases monotonically and that the computational cost increases in a power-law complexity as the time step reduces. Moreover, the mechanism responsible for generating the action potentials also affects the accuracy and computational cost. However, little attention has been paid to how the time span and firing rate influence the simulation. This study describes how the time span and firing rate variables affect the accuracy, computational cost, and efficiency. It was found that the simulation is importantly affected by these two variables.},
   author = {Sergio Valadez-Godínez and Humberto Sossa and Raúl Santiago-Montero},
   doi = {10.13053/CyS-21-4-2787},
   issn = {20079737},
   issue = {4},
   journal = {Computacion y Sistemas},
   keywords = {Accuracy,Computational cost,Firing rate,Hodgkin-Huxley,Izhikevich,Leaky integrate-and-fire,Numerical method,Simulation,Spiking neuron,Time span,Time step},
   pages = {841-861},
   publisher = {Instituto Politecnico Nacional},
   title = {How the Accuracy and Computational Cost of Spiking Neuron Simulation are Affected by the Time Span and Firing Rate},
   volume = {21},
   year = {2017},
}
@report{,
   abstract = {This paper depicts pattern classification of uppercase and lower case English character, using Leaky integrated and fire neuron model and Izhikevich neuron model of spiking neural network. Spiking neural network is one of the best artificial neural networks, which are widely used in the field of neuron science. In this paper we focused on spiking mechanism of both models and compare them in terms of accuracy and simulation time. Leaky integrate and fire neuron model which is one of the simple and efficient model of spiking neural network that analyze and simulate efficiently. On the other hand Izhikevich model is one of the powerful models which can simulate thousands of neurons in real time. Using these two models simulation results are obtained for recognition of uppercase and lower case English characters. Finally we report on simulation results of both models and discuss their performance, in terms of recognition rate and speed. Keywords-Graphical user interface (GUI), Izhikevich neuron model, leaky integrated and fire neuron model (LIF), spiking neural network (SNN).},
   author = {Soni A Chaturvedi},
   issue = {06},
   journal = {CiiT International Journal of Digital Image Processing},
   title = {Comparison of LIF and Izhikevich Spiking Neural Models for Recognition of Uppercase and Lowercase English Characters},
   volume = {6},
   url = {https://www.researchgate.net/publication/316283886},
}
@article{Skocik2014,
   abstract = {We review the Hodgkin-Huxley, Izhikevich, and leaky integrate-and-fire neuron models in regular spiking modes solved with the forward Euler, fourth-order Runge-Kutta, and exponential Euler methods and determine the necessary time steps and corresponding computational costs required to make the solutions accurate. We conclude that the leaky integrate-and-fire needs the least number of computations, and that the Hodgkin-Huxley and Izhikevich models are comparable in computational cost. © 2012 IEEE.},
   author = {Michael J. Skocik and Lyle N. Long},
   doi = {10.1109/TNNLS.2013.2294016},
   issn = {21622388},
   issue = {8},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Accuracy,Hodgkin - Huxley,Izhikevich,computational costs,leaky integrate-and-fire},
   pages = {1474-1483},
   pmid = {25050945},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {On the capabilities and computational costs of neuron models},
   volume = {25},
   year = {2014},
}
@inproceedings{Chaturvedi2014,
   abstract = {Pattern recognition is related with automatic detection or classification of an object. It find application in various fields such as automated analysis of medical image of blood contents, identification of humans from figure prints, hand writing recognition etc. In this paper we will analyse Izhikevich neuron model used for recognizing pattern and we also compare Izhikevich neuron model and Leaky integrate and fire neuron model which belongs to category of spiking neural network. © 2014 IEEE.},
   author = {Soni Chaturvedi and Neha R. Sondhiya and Rutika N. Titre},
   doi = {10.1109/ICESC.2014.65},
   isbn = {9781479921027},
   journal = {Proceedings - International Conference on Electronic Systems, Signal Processing, and Computing Technologies, ICESC 2014},
   keywords = {Izhikevich neuron model,leaky integrate and fire model,spiking neural network},
   pages = {346-349},
   title = {Izhikevich model based pattern classifier for hand written character recognition-a review analysis},
   year = {2014},
}
@generic{Izhikevich2003,
   abstract = {A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin-Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop PC.},
   author = {Eugene M. Izhikevich},
   doi = {10.1109/TNN.2003.820440},
   issn = {10459227},
   issue = {6},
   journal = {IEEE Transactions on Neural Networks},
   keywords = {Bursting,Cortex,Hodgkin-Huxley,PCNN,Quadratic integrate-and-fire,Spiking,Thalamus},
   month = {11},
   pages = {1569-1572},
   pmid = {18244602},
   title = {Simple model of spiking neurons},
   volume = {14},
   year = {2003},
}
@report{Antonio2010,
   abstract = {In this paper is shown how an Izhikevich neuron can be applied to solve different linear and non-linear pattern recognition problems. Given a set of input patterns belonging to K classes, each input pattern is transformed into an input signal, then the Izhikevich neuron is stimulated during T ms and finally the firing rate is computed. After adjusting the synaptic weights of the neural model, input patterns belonging to the same class will generate almost the same firing rate and input patterns belonging to different classes will generate firing rates different enough to discriminate among the different classes. At last, a comparison between a feed-forward neural network and the Izhikevich neural model is presented when they are applied to solve non-linear and real object recognition problems.},
   author = {Roberto Antonio and Vázquez Universidad and La Salle México and Roberto A Vázquez},
   title = {Izhikevich Neuron Model and its Application in Pattern Recognition Spiking neurons and their applications to pattern recognition View project Izhikevich Neuron Model and its Application in Pattern Recognition},
   url = {https://www.researchgate.net/publication/285718078},
   year = {2010},
}
@inproceedings{Rice2009,
   abstract = {There has been a strong push recently to examine biological scale simulations of neuromorphic algorithms to achieve stronger inference capabilities than current computing algorithms. The recent Izhikevich spiking neuron model is ideally suited for such large scale cortical simulations due to its efficiency and biological accuracy. In this paper we explore the feasibility of using FPGAs for large scale simulations of the Izhikevich model. We developed a modularized processing element to evaluate a large number of Izhikevich spiking neurons in a pipelined manner. This approach allows for easy scalability of the model to larger FPGAs. We utilized a character recognition algorithm based on the Izhikevich model for this study and scaled up the algorithm to use over 9000 neurons. The FPGA implementation of the algorithm on a Xilinx Virtex 4 provided a speedup of approximately 8.5 times an equivalent software implementation on a 2.2 GHz AMD Opteron core. Our results indicate that FPGAs are suitable for large scale cortical simulations utilizing the Izhikevich spiking neuron model. © 2009 IEEE.},
   author = {Kenneth L. Rice and Mohammad A. Bhuiyan and Tarek M. Taha and Christopher N. Vutsinas and Melissa C. Smith},
   doi = {10.1109/ReConFig.2009.77},
   isbn = {9780769539171},
   journal = {ReConFig'09 - 2009 International Conference on ReConFigurable Computing and FPGAs},
   keywords = {FPGA,Spiking neural networks},
   pages = {451-456},
   title = {FPGA implementation of Izhikevich spiking neural networks for character recognition},
   year = {2009},
}
@article{luna-a2019,
   author = {Antonio Luna-Álvarez and Dante Mújica-Vargas and Manuel Mejía-Lavalle},
   doi = {10.13053/rcs-148-10-6},
   issn = {1870-4069},
   issue = {10},
   journal = {Research in Computing Science},
   month = {12},
   pages = {65-80},
   publisher = {Instituto Politecnico Nacional/Centro de Investigacion en Computacion},
   title = {Convolutional Model with Classification through Izhikevich Neuron},
   volume = {148},
   year = {2019},
}
@inproceedings{Rangan2010,
   abstract = {We present a circuit architecture for compact analog VLSI implementation of the Izhikevich neuron model, which efficiently describes a wide variety of neuron spiking and bursting dynamics using two state variables and four adjustable parameters. Log-domain circuit design utilizing MOS transistors in subthreshold results in high energy efficiency, with less than 1pJ of energy consumed per spike. We also discuss the effects of parameter variations on the dynamics of the equations, and present simulation results that replicate several types of neural dynamics. The low power operation and compact analog VLSI realization make the architecture suitable for human-machine interface applications in neural prostheses and implantable bioelectronics, as well as large-scale neural emulation tools for computational neuroscience. © 2010 IEEE.},
   author = {Venkat Rangan and Abhishek Ghosh and Vladimir Aparin and Gert Cauwenberghs},
   doi = {10.1109/IEMBS.2010.5627392},
   isbn = {9781424441235},
   journal = {2010 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC'10},
   pages = {4164-4167},
   pmid = {21096884},
   title = {A subthreshold a VLSI implementation of the Izhikevich simple neuron model},
   year = {2010},
}
@book{,
   abstract = {Scholarly & Professional},
   isbn = {9781612841373},
   publisher = {IEEE},
   title = {2011 IEEE 9th International New Circuits and Systems Conference.},
   year = {2011},
}
@article{Izhikevich2004,
   abstract = {We discuss the biological plausibility and computational efficiency of some of the most useful models of spiking and bursting neurons. We compare their applicability to large-scale simulations of cortical neural networks. © 2004 IEEE.},
   author = {Eugene M. Izhikevich},
   doi = {10.1109/TNN.2004.832719},
   issn = {10459227},
   issue = {5},
   journal = {IEEE Transactions on Neural Networks},
   month = {9},
   pages = {1063-1070},
   pmid = {15484883},
   title = {Which model to use for cortical spiking neurons?},
   volume = {15},
   year = {2004},
}
@report{Bohte2002,
   abstract = {For a network of spiking neurons that encodes information in the timing of individual spike times, we derive a supervised learning rule, SpikeProp, akin to traditional error-backpropagation. With this algorithm, we demonstrate how networks of spiking neurons with biologically reasonable action potentials can perform complex non-linear classiÿcation in fast temporal coding just as well as rate-coded networks. We perform experiments for the classical XOR problem, when posed in a temporal setting, as well as for a number of other benchmark datasets. Comparing the (implicit) number of spiking neurons required for the encoding of the interpolated XOR problem, the trained networks demonstrate that temporal coding is a viable code for fast neural information processing, and as such requires less neurons than instantaneous rate-coding. Furthermore, we ÿnd that reliable temporal computation in the spiking networks was only accomplished when using spike response functions with a time constant longer than the coding interval, as has been predicted by theoretical considerations.},
   author = {Sander M Bohte and Joost N Kok and Han La Poutrã},
   journal = {Neurocomputing},
   keywords = {Error-backpropagation,Spiking neurons,Temporal coding},
   pages = {17-37},
   title = {Error-backpropagation in temporally encoded networks of spiking neurons},
   volume = {48},
   url = {www.elsevier.com/locate/neucom},
   year = {2002},
}
@article{ruf1998,
   author = {Thomas Natschläger and Berthold Ruf},
   doi = {10.1088/0954-898X_9_3_003},
   issn = {0954-898X},
   issue = {3},
   journal = {Network: Computation in Neural Systems},
   month = {1},
   title = {Spatial and temporal pattern analysis via spiking neurons},
   volume = {9},
   year = {1998},
}
@report{Bohte2012,
   abstract = {Neural adaptation underlies the ability of neurons to maximize encoded information over a wide dynamic range of input stimuli. While adaptation is an intrinsic feature of neuronal models like the Hodgkin-Huxley model, the challenge is to integrate adaptation in models of neural computation. Recent computational models like the Adaptive Spike Response Model implement adaptation as spike-based addition of fixed-size fast spike-triggered threshold dynamics and slow spike-triggered currents. Such adaptation has been shown to accurately model neural spiking behavior over a limited dynamic range. Taking a cue from kinetic models of adaptation, we propose a multiplicative Adaptive Spike Response Model where the spike-triggered adaptation dynamics are scaled multiplicatively by the adaptation state at the time of spiking. We show that unlike the additive adaptation model, the firing rate in the multiplicative adaptation model saturates to a maximum spike-rate. When simulating variance switching experiments, the model also quantitatively fits the experimental data over a wide dynamic range. Furthermore, dynamic threshold models of adaptation suggest a straightforward interpretation of neural activity in terms of dynamic signal encoding with shifted and weighted exponential kernels. We show that when thus encoding rectified filtered stimulus signals, the multiplicative Adaptive Spike Response Model achieves a high coding efficiency and maintains this efficiency over changes in the dynamic signal range of several orders of magnitude, without changing model parameters.},
   author = {Sander M Bohte},
   title = {Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model},
   year = {2012},
}
@report{Jolivet2003,
   abstract = {We propose a simple method to map a generic threshold model, namely the Spike Response Model, to artificial data of neuronal activity using a minimal amount of a priori information. Here, data are generated by a detailed mathematical model of neuronal activity. The model neuron is driven with in-vivo-like current injected, and we test to which extent it is possible to predict the spike train of the detailed neuron model from that of the Spike Response Model. In particular, we look at the number of spikes correctly predicted within a biologically relevant time window. We find that the Spike Response Model achieves prediction of up to 80% of the spikes with correct timing (±2ms). Other characteristics of activity, such as mean rate and coefficient of variation of spike trains, are predicted in the correct range as well.},
   author = {Renaud Jolivet and Timothy J Lewis and Wulfram Gerstner},
   title = {The Spike Response Model: A Framework to Predict Neuronal Spike Trains},
   url = {http://diwww.epfl.ch/mantrahttp://www.cns.nyu.edu/˜tlewis},
}
@report{Gerstner1993,
   abstract = {Hebbian learning allows a network of spiking neurons to store and retrieve spatio-temporal patterns with a time resolution of 1 ms, despite the long post-synaptic and dendritic integration times. To show this, we introduce and analyze a model of spiking neurons, the spike response model, with a realistic distribution of axonal delays and with realistic postsynaptic potentials. Learning is performed by a local Hebbian rule which is based on the synchronism of presynaptic neuro-transmitter release and some short-acting postsynaptic process. The time window of this synchronism determines the temporal resolution of pattern retrieval, which can be initiated by applying a short external stimulus pattern. Furthermore, a rate quantization is found in dependence upon the threshold value of the neurons, i.e., in a given time a pattern runs n times as often as learned, where n is a positive integer (n/> 0). We show that all information about the spike pattern is lost if only mean firing rates (temporal average) or ensemble activities (spatial average) are considered. An average over several retrieval runs in order to generate a post-stimulus time histogram may also deteriorate the signal. The full information on a pattern is contained in the spike raster of a single run. Our results stress the importance, and advantage , of coding by spatio-temporal spike patterns instead of firing rates and average ensemble activity. The implications regarding modelling and experimental data analysis are discussed.},
   author = {Wulfram Gerstner and Raphael Ritz and J Leo Van Hemmen},
   journal = {Biol. Cybern},
   pages = {503-515},
   publisher = {Springer},
   title = {Why spikes? Hebbian learning and retrieval of time-resolved excitation patterns},
   volume = {69},
   year = {1993},
}
@article{Clayton2011,
   abstract = {This paper introduces a novel probabilistic spike-response model through the combination of avalanche diode-generated Poisson distributed noise, and a standard exponential decay-based spike-response curve. The noise source, which is derived from a 0.35-µm single-photon avalanche diode (kept in the dark), was tested experimentally to verify its characteristics, before being combined with a field-programmable gate-array implementation of a spike-response model. This simple model was then analyzed, and shown to reproduce seven of eight behaviors recorded during an extensive study of the ventral medial hypothalamic (VMH) region of the brain. It is thought that many of the cell types found within the VMH are fed from a tonic noise synaptic input, where the patterns generated are a product of their spike response and not their interconnection. This paper shows how this tonic noise source can be modelled, and due to the independent nature of the noise sources, provides an avenue for the exploration of networks of noise-fueled neurons, which play a significant role in pattern generation within the brain. © 2011, IEEE.},
   author = {Thomas Clayton and Katherine Cameron and Bruce R. Rae and Robert K. Henderson and Alan Murray and Nancy Sabatier and Gareth Leng and Edoardo Charbon},
   doi = {10.1109/TBCAS.2010.2100392},
   issn = {19409990},
   issue = {3},
   journal = {IEEE Transactions on Biomedical Circuits and Systems},
   keywords = {Modelling,single-photon avalanche diode (SPAD),spike-response model,spiking neuron},
   pages = {231-243},
   title = {An Implementation of a Spike-Response Model With Escape Noise Using an Avalanche Diode},
   volume = {5},
   year = {2011},
}
@report{Ourdighi2016,
   abstract = {This study investigates the efficiency of the one-layered Spiking Neural Network (SNN) on the enhancing of the breast cancer diagnostic results. The proposed network is based on Spike Response Model (SRM) with multiple delays per connection. Beside its simplicity, this model allows to modeling the production of a biologically realistic response to incoming synaptic events. By using a supervised learning, the training process was founded around of an error-back propagation algorithm depending only on the time of the first spike emitted. In experimentation, our approach was exclusively tested on Wisconsin Breast Cancer Database (WBCD). The results were evaluated in accuracy classification and the area under Receiver Operating Characteristics (ROC) Area Under ROC Curve (AUC). In summary, we achieved 99.26% of accuracy classification with an AUC equal to 0.992.},
   author = {Asmaa Ourdighi and Abdelkader Benyettou},
   issue = {3},
   journal = {The International Arab Journal of Information Technology},
   keywords = {SNN,SRM,WBCD,a gradient descent rule},
   title = {An Efficient Spiking Neural Network Approach based on Spike Response Model for Breast Cancer Diagnostic},
   volume = {13},
   year = {2016},
}
@article{,
   abstract = {In this work, we propose a variation of a direct reinforcement learning algorithm, suitable for usage with spiking neurons based on the spike response model (SRM). The SRM is a biologically inspired, flexible model of spiking neuron based on kernel functions that describe the effect of spike reception and emission on the membrane potential of the neuron. In our experiments, the spikes emitted by a SRM neuron are used as input signals in a simple control task. The reinforcement signal obtained from the environment is used by the direct reinforcement learning algorithm, that modifies the synaptic weights of the neuron, adjusting the spiking firing times in order to obtain a better performance at the given problem. The obtained results are comparable to those from classic methods based on value function approximation and temporal difference, for simple control tasks. © 2006 Elsevier B.V. All rights reserved.},
   author = {Murilo Saraiva de Queiroz and Roberto Coelho de Berrêdo and Antônio de Pádua Braga},
   doi = {10.1016/j.neucom.2006.07.002},
   issn = {09252312},
   issue = {1-3},
   journal = {Neurocomputing},
   keywords = {Reinforcement learning,Spike response model,Spiking neuron},
   month = {12},
   pages = {14-20},
   title = {Reinforcement learning of a simple control task using the spike response model},
   volume = {70},
   year = {2006},
}
@inproceedings{Josafath2013,
   abstract = {Some spiking neuron models have proved to solve different linear and non-linear pattern recognition problems. Indeed, only one spiking neuron can generate comparable results as classical artificial neural network. However, depending on the classification problem, one spiking model could be better or less efficient than other. In this paper we propose a methodology to create spiking neuron models using Gene Expression Programming. The new models created are applied in eight pattern recognition problems. The results obtained are compared with previous results generated adopting the Izhikevich spiking neuron model. This first effort will help us to generate spiking neuron models which will be adaptable to a specific pattern recognition problem. © 2013 IEEE.},
   author = {Josafath I. Espinosa-Ramos and Nareli Cruz-Cortes and Roberto A. Vazquez},
   doi = {10.1109/IJCNN.2013.6706795},
   isbn = {9781467361293},
   journal = {Proceedings of the International Joint Conference on Neural Networks},
   title = {Creation of spiking neuron models applied in pattern recognition problems},
   year = {2013},
}
@article{Masquelier2008,
   author = {Timothée Masquelier and Rudy Guyonneau and Simon J. Thorpe},
   doi = {10.1371/journal.pone.0001377},
   issn = {1932-6203},
   issue = {1},
   journal = {PLoS ONE},
   month = {1},
   title = {Spike Timing Dependent Plasticity Finds the Start of Repeating Patterns in Continuous Spike Trains},
   volume = {3},
   year = {2008},
}
@article{stdp2010,
   author = {Jesper Sjöström and Wulfram Gerstner},
   doi = {10.4249/scholarpedia.1362},
   issn = {1941-6016},
   issue = {2},
   journal = {Scholarpedia},
   title = {Spike-timing dependent plasticity},
   volume = {5},
   year = {2010},
}
@article{Markram2012,
   author = {H. Markram and W. Gerstner and P. J. Sjöström},
   doi = {10.3389/fnsyn.2012.00002},
   issn = {1663-3563},
   journal = {Frontiers in Synaptic Neuroscience},
   title = {Spike-Timing-Dependent Plasticity: A Comprehensive Overview},
   volume = {4},
   year = {2012},
}
@article{Shatz1992,
   author = {Carla J. Shatz},
   doi = {10.1038/scientificamerican0992-60},
   issn = {0036-8733},
   issue = {3},
   journal = {Scientific American},
   month = {9},
   title = {The Developing Brain},
   volume = {267},
   year = {1992},
}
@article{Markram1995,
   author = {H Markram and P J Helm and B Sakmann},
   doi = {10.1113/jphysiol.1995.sp020708},
   issn = {00223751},
   issue = {1},
   journal = {The Journal of Physiology},
   month = {5},
   title = {Dendritic calcium transients evoked by single back-propagating action potentials in rat neocortical pyramidal neurons.},
   volume = {485},
   year = {1995},
}
@article{Markram1997,
   author = {H. Markram},
   doi = {10.1126/science.275.5297.213},
   issn = {00368075},
   issue = {5297},
   journal = {Science},
   month = {1},
   title = {Regulation of Synaptic Efficacy by Coincidence of Postsynaptic APs and EPSPs},
   volume = {275},
   year = {1997},
}
@article{Gerstner1996,
   author = {Wulfram Gerstner and Richard Kempter and J. Leo van Hemmen and Hermann Wagner},
   doi = {10.1038/383076a0},
   issn = {0028-0836},
   issue = {6595},
   journal = {Nature},
   month = {9},
   title = {A neuronal learning rule for sub-millisecond temporal coding},
   volume = {383},
   year = {1996},
}
@article{Fusi2000,
   abstract = {<p>We present a model for spike-driven dynamics of a plastic synapse, suited for a VLSI implementation. The synaptic device behaves as a capacitor on short timescales and preserves the memory of two stable states (efficacies) on long timescales. The transitions (LTP/LTD) are stochastic because both the number and the distribution of neural spikes in any finite (stimulation) interval fluctuate, even at fixed pre- and postsynaptic spike rates. The dynamics of the single synapse is studied analytically by extending the solution to a classic problem in queuing theory (Takàcs process). The model of the synapse is implemented in a VLSI and consists of only 18 transistors. It is also directly simulated. The simulations indicate that LTP/LTD probabilities versus rates are robust to fluctuations of the electronic parameters in a wide range of rates. The solutions for these probabilities are in very good agreement with both the simulations and measurements. Moreover, the probabilities are readily manipulable by variations of the chip's parameters, even in ranges where they are very small. The tests of the electronic device cover the range from spontaneous activity (3–4 Hz) to stimulus-driven rates (50 Hz). Low transition probabilities can be maintained in all ranges, even though the intrinsic time constants of the device are short (∼ 100 ms).</p>},
   author = {Stefano Fusi and Mario Annunziato and Davide Badoni and Andrea Salamon and Daniel J. Amit},
   doi = {10.1162/089976600300014917},
   issn = {0899-7667},
   issue = {10},
   journal = {Neural Computation},
   month = {10},
   title = {Spike-Driven Synaptic Plasticity: Theory, Simulation, VLSI Implementation},
   volume = {12},
   year = {2000},
}
@article{Fellous2004,
   author = {J.-M. Fellous},
   doi = {10.1523/JNEUROSCI.4649-03.2004},
   issn = {0270-6474},
   issue = {12},
   journal = {Journal of Neuroscience},
   month = {3},
   title = {Discovering Spike Patterns in Neuronal Responses},
   volume = {24},
   year = {2004},
}
@article{rossum2000,
   author = {M. C. W. van Rossum and G. Q. Bi and G. G. Turrigiano},
   doi = {10.1523/JNEUROSCI.20-23-08812.2000},
   issn = {0270-6474},
   issue = {23},
   journal = {The Journal of Neuroscience},
   month = {12},
   title = {Stable Hebbian Learning from Spike Timing-Dependent Plasticity},
   volume = {20},
   year = {2000},
}
@article{Guyonneau2005,
   abstract = {<p>Spike timing-dependent plasticity (STDP) is a learning rule that modifies the strength of a neuron's synapses as a function of the precise temporal relations between input and output spikes. In many brains areas, temporal aspects of spike trains have been found to be highly reproducible. How will STDP affect a neuron's behavior when it is repeatedly presented with the same input spike pattern? We show in this theoretical study that repeated inputs systematically lead to a shaping of the neuron's selectivity, emphasizing its very first input spikes, while steadily decreasing the postsynaptic response latency. This was obtained under various conditions of background noise, and even under conditions where spiking latencies and firing rates, or synchrony, provided conflicting informations. The key role of first spikes demonstrated here provides further support for models using a single wave of spikes to implement rapid neural processing.</p>},
   author = {Rudy Guyonneau and Rufin VanRullen and Simon J. Thorpe},
   doi = {10.1162/0899766053429390},
   issn = {0899-7667},
   issue = {4},
   journal = {Neural Computation},
   month = {4},
   title = {Neurons Tune to the Earliest Spikes Through STDP},
   volume = {17},
   year = {2005},
}
@article{Masquelier2007,
   author = {Timothée Masquelier and Simon J Thorpe},
   doi = {10.1371/journal.pcbi.0030031},
   issn = {1553-7358},
   issue = {2},
   journal = {PLoS Computational Biology},
   month = {2},
   title = {Unsupervised Learning of Visual Features through Spike Timing Dependent Plasticity},
   volume = {3},
   year = {2007},
}
@article{thorpe2001,
   abstract = {It is often supposed that the messages sent to the visual cortex by the retinal ganglion cells are encoded by the mean firing rates observed on spike trains generated with a Poisson process. Using an information transmission approach, we evaluate the performances of two such codes, one based on the spike count and the other on the mean interspike interval, and compare the results with a rank order code, where the first ganglion cells to emit a spike are given a maximal weight. Our results show that the rate codes are far from optimal for fast information transmission and that the temporal structure of the spike train can be efficiently used to maximize the information transfer rate under conditions where each cell needs to fire only one spike.},
   author = {Rufin Van Rullen and Simon J. Thorpe},
   doi = {10.1162/08997660152002852},
   issn = {08997667},
   issue = {6},
   journal = {Neural Computation},
   month = {6},
   pages = {1255-1283},
   pmid = {11387046},
   title = {Rate coding versus temporal order coding: What the retinal ganglion cells tell the visual cortex},
   volume = {13},
   year = {2001},
}
@article{Painkras2013,
   author = {Eustace Painkras and Luis A. Plana and Jim Garside and Steve Temple and Francesco Galluppi and Cameron Patterson and David R. Lester and Andrew D. Brown and Steve B. Furber},
   doi = {10.1109/JSSC.2013.2259038},
   issn = {0018-9200},
   issue = {8},
   journal = {IEEE Journal of Solid-State Circuits},
   month = {8},
   title = {SpiNNaker: A 1-W 18-Core System-on-Chip for Massively-Parallel Neural Network Simulation},
   volume = {48},
   year = {2013},
}
@article{Plana2011,
   abstract = {<p>The design and implementation of globally asynchronous locally synchronous systems-on-chip is a challenging activity. The large size and complexity of the systems require the use of computer-aided design (CAD) tools but, unfortunately, most tools do not work adequately with asynchronous circuits. This article describes the successful design and implementation of SpiNNaker, a GALS multicore system-on-chip. The process was completed using commercial CAD tools from synthesis to layout. A hierarchical methodology was devised to deal with the asynchronous sections of the system, encapsulating and validating timing assumptions at each level. The crossbar topology combined with a pipelined asynchronous fabric implementation allows the on-chip network to meet the stringent requirements of the system. The implementation methodology constrains the design in a way that allows the tools to complete their tasks successfully. A first test chip, with reduced resources and complexity was taped-out using the proposed methodology. Test chips were received in December 2009 and were fully functional. The methodology had to be modified to cope with the increased complexity of the SpiNNaker SoC. SpiNNaker chips were delivered in May 2011 and were also fully operational, and the interconnect requirements were met.</p>},
   author = {Luis A. Plana and David Clark and Simon Davidson and Steve Furber and Jim Garside and Eustace Painkras and Jeffrey Pepper and Steve Temple and John Bainbridge},
   doi = {10.1145/2043643.2043647},
   issn = {1550-4832},
   issue = {4},
   journal = {ACM Journal on Emerging Technologies in Computing Systems},
   month = {12},
   title = {SpiNNaker},
   volume = {7},
   year = {2011},
}
@article{Furber2014,
   author = {Steve B. Furber and Francesco Galluppi and Steve Temple and Luis A. Plana},
   doi = {10.1109/JPROC.2014.2304638},
   issn = {0018-9219},
   issue = {5},
   journal = {Proceedings of the IEEE},
   month = {5},
   title = {The SpiNNaker Project},
   volume = {102},
   year = {2014},
}
@inproceedings{Cassidy2013,
   author = {Andrew S. Cassidy and Paul Merolla and John V. Arthur and Steve K. Esser and Bryan Jackson and Rodrigo Alvarez-Icaza and Pallab Datta and Jun Sawada and Theodore M. Wong and Vitaly Feldman and Arnon Amir and Daniel Ben-Dayan Rubin and Filipp Akopyan and Emmett McQuinn and William P. Risk and Dharmendra S. Modha},
   doi = {10.1109/IJCNN.2013.6707077},
   isbn = {978-1-4673-6129-3},
   journal = {The 2013 International Joint Conference on Neural Networks (IJCNN)},
   month = {8},
   publisher = {IEEE},
   title = {Cognitive computing building block: A versatile and efficient digital neuron model for neurosynaptic cores},
   year = {2013},
}
@book_section{Shastri2018,
   author = {Bhavin J. Shastri and Alexander N. Tait and Thomas Ferreira de Lima and Mitchell A. Nahmias and Hsuan-Tung Peng and Paul R. Prucnal},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-27737-5_702-1},
   journal = {Encyclopedia of Complexity and Systems Science},
   publisher = {Springer Berlin Heidelberg},
   title = {Neuromorphic Photonics, Principles of Neuromorphic Photonics},
   year = {2018},
}
@article{Furber2016,
   author = {Steve Furber},
   doi = {10.1088/1741-2560/13/5/051001},
   issn = {1741-2560},
   issue = {5},
   journal = {Journal of Neural Engineering},
   month = {10},
   title = {Large-scale neuromorphic computing systems},
   volume = {13},
   year = {2016},
}
@report{Lansky1995,
   abstract = {The temporal patterns of action potentials fired by a two-point stochastic neuron model were investigated. In this model the membrane potential of the dendritic compartment follows the Orstein-Uhlenbeck process and is not affected by the spiking activity. The axonal compartment, corresponding to the spike initiation site, is described by a simplified RC circuit. Es-timators of the mean and variance of the input, based on a sampling of the axonal membrane potential, were derived and applied to simulated data. The dependencies of the mean firing frequency and of the coefficient of variation and serial correlation of interspike intervals on the mean and variance of the input were also studied by computer simulation in both 1-and 2-point models. The main property distinguishing the 2-point model from the classical 1-point model is its ability to produce clusters of short (or long) intervals between spikes under conditions of constant stimulation, as often observed in real neu-rons. It is shown that the nearly linear frequency response of the neuron, starting with subthreshold values of the input, is accounted for by the variability of the input (noise), which indicates that noise can play a positive role in nervous systems. The linear response frequency with respect to noise of the models suggests that the neuron can function as a noise encoder.},
   author = {Petr Lansky and Jean Pierre Rospars},
   journal = {Biol. Cybern},
   pages = {397-406},
   title = {Ornstein-Uhlenbeck model neuron revisited},
   volume = {72},
   year = {1995},
}
@report{Tuckwell1998,
   abstract = {An analytical approach is presented for determining the response of a neuron or of the activity in a network of connected neurons, represented by systems of nonlinear ordinary stochastic differential equations-the Fitzhugh-Nagumo system with Gaussian white noise current. For a single neuron, five equations hold for the first-and second-order central moments of the voltage and recovery variables. From this system we obtain, under certain assumptions, five differential equations for the means, variances, and covariance of the two components. One may use these quantities to estimate the probability that a neuron is emitting an action potential at any given time. The differential equations are solved by numerical methods. We also perform simulations on the stochastic Fitzugh-Nagumo system and compare the results with those obtained from the differential equations for both sustained and intermittent deterministic current inputs with superimposed noise. For intermittent currents, which mimic synaptic input, the agreement between the analytical and simulation results for the moments is excellent. For sustained input, the analytical approximations perform well for small noise as there is excellent agreement for the moments. In addition, the probability that a neuron is spiking as obtained from the empirical distribution of the potential in the simulations gives a result almost identical to that obtained using the analytical approach. However, when there is sustained large-amplitude noise, the analytical method is only accurate for short time intervals. Using the simulation method, we study the distribution of the interspike interval directly from simulated sample paths. We confirm that noise extends the range of input currents over which (nonperiodic) spike trains may exist and investigate the dependence of such firing on the magnitude of the mean input current and the noise amplitude. For networks we find the differential equations for the means, variances, and covariances of the voltage and recovery variables and show how solving them leads to an expression for the probability that a given neuron, or given set of neurons, is firing at time t. Using such expressions one may implement dynamical rules for changing synaptic strengths directly without sampling. The present analytical method applies equally well to temporally nonhomogeneous input currents and is expected to be useful for computational studies of information processing in various nervous system centers.},
   author = {Henry C Tuckwell and Roger Rodriguez},
   journal = {Journal of Computational Neuroscience},
   keywords = {Fitzhugh-Nagumo model,interspike intervals,noise-induced activity,stochastic neural network,stochastic neuron model},
   pages = {91-113},
   title = {P1: AAL/UKD Analytical and Simulation Results for Stochastic Fitzhugh-Nagumo Neurons and Neural Networks},
   volume = {5},
   year = {1998},
}
@article{Ditlevsen2006,
   abstract = {The stochastic Feller neuronal model is studied, and estimators of the model input parameters, depending on the firing regime of the process, are derived. Closed expressions for the first two moments of functionals of the first-passage time (FTP) through a constant boundary in the suprathreshold regime are derived, which are used to calculate moment estimators. In the subthreshold regime, the exponentiality of the FTP is utilized to characterize the input parameters. The methods are illustrated on simulated data. Finally, approximations of the first-passage-time moments are suggested, and biological interpretations and comparisons of the parameters in the Feller and the Ornstein-Uhlenbeck models are discussed. © 2006 The American Physical Society.},
   author = {Susanne Ditlevsen and Petr Lansky},
   doi = {10.1103/PhysRevE.73.061910},
   issn = {15393755},
   issue = {6},
   journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
   title = {Estimation of the input parameters in the Feller neuronal model},
   volume = {73},
   year = {2006},
}
@report{Gammaitoni1998,
   abstract = {Over the last two decades, stochastic resonance has continuously attracted considerable attention. The term is given to a phenomenon that is manifest in nonlinear systems whereby generally feeble input information (such as a weak signal) can be be amplified and optimized by the assistance of noise. The effect requires three basic ingredients: (i) an energetic activation barrier or, more generally, a form of threshold; (ii) a weak coherent input (such as a periodic signal); (iii) a source of noise that is inherent in the system, or that adds to the coherent input. Given these features, the response of the system undergoes resonance-like behavior as a function of the noise level; hence the name stochastic resonance. The underlying mechanism is fairly simple and robust. As a consequence, stochastic resonance has been observed in a large variety of systems, including bistable ring lasers, semiconductor devices, chemical reactions, and mechanoreceptor cells in the tail fan of a crayfish. In this paper, the authors report, interpret, and extend much of the current understanding of the theory and physics of stochastic resonance. They introduce the readers to the basic features of stochastic resonance and its recent history. Definitions of the characteristic quantities that are important to quantify stochastic resonance, together with the most important tools necessary to actually compute those quantities, are presented. The essence of classical stochastic resonance theory is presented, and important applications of stochastic resonance in nonlinear optics, solid state devices, and neurophysiology are described and put into context with stochastic resonance theory. More elaborate and recent developments of stochastic resonance theory are discussed, ranging from fundamental quantum properties-being important at low temperatures-over spatiotemporal aspects in spatially distributed systems, to realizations in chaotic maps. In conclusion the authors summarize the achievements and attempt to indicate the most promising areas for future research in theory and experiment. [S0034-6861(98)00101-9] CONTENTS},
   author = {Luca Gammaitoni and Peter Jung and Fabio Marchesoni},
   title = {Stochastic resonance},
   year = {1998},
}
@report{Honggi2002,
   abstract = {Noise is usually thought of as the enemy of order rather than as a constructive influence. In nonlinear systems that possess some sort of threshold, random noise plays a beneficial role in enhancing the detection of weak information-carrying signals. This phenomenon, termed stochastic resonance, does find useful applications in physical, biological, and biomedical contexts. Certain biological systems may even use this effect for optimizing function and behavior.},
   author = {Peter H‰nggi},
   journal = {BIOPHYSICS SPECIAL CHEMPHYSCHEM},
   keywords = {biophysics ¥},
   publisher = {WILEY},
   title = {Stochastic Resonance in Biology How Noise Can Enhance Detection of Weak Signals and Help Improve Biological Information Processing},
   volume = {3},
   year = {2002},
}
@article{Lansky2006,
   abstract = {Five parameters of one of the most common neuronal models, the diffusion leaky integrate-and-fire model, also known as the Ornstein-Uhlenbeck neuronal model, were estimated on the basis of intracellular recording. These parameters can be classified into two categories. Three of them (the membrane time constant, the resting potential and the firing threshold) characterize the neuron itself. The remaining two characterize the neuronal input. The intracellular data were collected during spontaneous firing, which in this case is characterized by a Poisson process of interspike intervals. Two methods for the estimation were applied, the regression method and the maximum-likelihood method. Both methods permit to estimate the input parameters and the membrane time constant in a short time window (a single interspike interval). We found that, at least in our example, the regression method gave more consistent results than the maximum-likelihood method. The estimates of the input parameters show the asymptotical normality, which can be further used for statistical testing, under the condition that the data are collected in different experimental situations. The model neuron, as deduced from the determined parameters, works in a subthreshold regimen. This result was confirmed by both applied methods. The subthreshold regimen for this model is characterized by the Poissonian firing. This is in a complete agreement with the observed interspike interval data. © Springer Science + Business Media, LLC 2006.},
   author = {Petr Lansky and Pavel Sanda and Jufang He},
   doi = {10.1007/s10827-006-8527-6},
   issn = {09295313},
   issue = {2},
   journal = {Journal of Computational Neuroscience},
   keywords = {Leaky integrate-and-fire model,Ornstein-Uhlenbeck neuronal model,Parameters estimation,Spontaneous firing},
   month = {10},
   pages = {211-223},
   pmid = {16871351},
   title = {The parameters of the stochastic leaky integrate-and-fire neuronal model},
   volume = {21},
   year = {2006},
}
@generic{Lansky2008,
   abstract = {Parameters in diffusion neuronal models are divided into two groups; intrinsic and input parameters. Intrinsic parameters are related to the properties of the neuronal membrane and are assumed to be known throughout the paper. Input parameters characterize processes generated outside the neuron and methods for their estimation are reviewed here. Two examples of the diffusion neuronal model, which are based on the integrate-and-fire concept, are investigated-the Ornstein-Uhlenbeck model as the most common one and the Feller model as an illustration of state-dependent behavior in modeling the neuronal input. Two types of experimental data are assumed-intracellular describing the membrane trajectories and extracellular resulting in knowledge of the interspike intervals. The literature on estimation from the trajectories of the diffusion process is extensive and thus the stress in this review is set on the inference made from the interspike intervals. © 2008 Springer-Verlag.},
   author = {Petr Lansky and Susanne Ditlevsen},
   doi = {10.1007/s00422-008-0237-x},
   issn = {03401200},
   issue = {4-5},
   journal = {Biological Cybernetics},
   keywords = {Feller process,First-passage times,Fortets integral equation,Interspike intervals,Laplace transform,Maximum likelihood,Moment method,Ornstein-Uhlenbeck,Statistical inference},
   month = {11},
   pages = {253-262},
   pmid = {18496710},
   title = {A review of the methods for signal estimation in stochastic diffusion leaky integrate-and-fire neuronal models},
   volume = {99},
   year = {2008},
}
@article{Sinyavskiy2010,
   abstract = {A generalized model of spiking neuron is proposed as a non-stationary stochastic spike sequences processing unit. The generalized spiking neuron model is described with a conditional spike generation probability distribution and with a state evolution operator. An information theory language is used for the convenient neuron's learning tasks description. The problem of spiking neuron learning with the teacher is solved using information entropy minimization algorithm. A particular implementation of generalized model using stochastic Spike Response Model with alpha-functions set is provided. A task of time delay maintenance between input and output spikes and a task of detecting of a spiking pattern in a noisy stream of pulse signals are considered using extended SRM neuron. It is shown that after the using of the proposed learning method spiking neuron became capable to detect a spatial-temporal pulse pattern and to serve as an adaptive delay unit. © 2010 Allerton Press, Inc.},
   author = {O. Y. Sinyavskiy and A. I. Kobrin},
   doi = {10.3103/S1060992X10040077},
   issn = {1060992X},
   issue = {4},
   journal = {Optical Memory and Neural Networks (Information Optics)},
   keywords = {Information entropy,Information theory,Neuron models,Spiking neuron,Spiking neuron learning},
   pages = {300-309},
   title = {Generalized stochastic spiking neuron model and extended spike response model in spatial-temporal pulse pattern detection task},
   volume = {19},
   year = {2010},
}
@article{Clayton2011,
   abstract = {This paper introduces a novel probabilistic spike-response model through the combination of avalanche diode-generated Poisson distributed noise, and a standard exponential decay-based spike-response curve. The noise source, which is derived from a 0.35-µm single-photon avalanche diode (kept in the dark), was tested experimentally to verify its characteristics, before being combined with a field-programmable gate-array implementation of a spike-response model. This simple model was then analyzed, and shown to reproduce seven of eight behaviors recorded during an extensive study of the ventral medial hypothalamic (VMH) region of the brain. It is thought that many of the cell types found within the VMH are fed from a tonic noise synaptic input, where the patterns generated are a product of their spike response and not their interconnection. This paper shows how this tonic noise source can be modelled, and due to the independent nature of the noise sources, provides an avenue for the exploration of networks of noise-fueled neurons, which play a significant role in pattern generation within the brain. © 2011, IEEE.},
   author = {Thomas Clayton and Katherine Cameron and Bruce R. Rae and Robert K. Henderson and Alan Murray and Nancy Sabatier and Gareth Leng and Edoardo Charbon},
   doi = {10.1109/TBCAS.2010.2100392},
   issn = {19409990},
   issue = {3},
   journal = {IEEE Transactions on Biomedical Circuits and Systems},
   keywords = {Modelling,single-photon avalanche diode (SPAD),spike-response model,spiking neuron},
   pages = {231-243},
   title = {An Implementation of a Spike-Response Model With Escape Noise Using an Avalanche Diode},
   volume = {5},
   year = {2011},
}
@report{,
   title = {Viewpoint Learning in Spiking Neural Networks by Reinforcement of Stochastic Synaptic Transmission},
}
@article{Fox1997,
   abstract = {A Hodgkin-Huxley model algorithm for the numerical simulation of noise in neurons is contracted from a master equation description (cellular automoton) into a Langevin description. This reduction reduces the time required for a simulation by about two orders of magnitude. Earlier work is summarized, condensed, and made explicit to make the algorithm transparent and facilitate applications. Two approximate treatments are reported. An extension of this approach is presented that includes spatial dependence and the propagation of a noisy action potential along an axon.},
   author = {Ronald F. Fox},
   doi = {10.1016/S0006-3495(97)78850-7},
   issn = {00063495},
   issue = {5},
   journal = {Biophysical Journal},
   pages = {2068-2074},
   pmid = {9129809},
   publisher = {Biophysical Society},
   title = {Stochastic versions of the Hodgkin-Huxley equations},
   volume = {72},
   year = {1997},
}
@report{Pillow2004,
   abstract = {Recent work has examined the estimation of models of stimulus-driven neural activity in which some linear filtering process is followed by a nonlinear, probabilistic spiking stage. We analyze the estimation of one such model for which this nonlinear step is implemented by a noisy, leaky, integrate-and-fire mechanism with a spike-dependent after-current. This model is a biophysically plausible alternative to models with Poisson (memory-less) spiking, and has been shown to effectively reproduce various spiking statistics of neurons in vivo. However, the problem of estimating the model from extracellular spike train data has not been examined in depth. We formulate the problem in terms of maximum likelihood estimation, and show that the computational problem of maximizing the likelihood is tractable. Our main contribution is an algorithm and a proof that this algorithm is guaranteed to find the global optimum with reasonable speed. We demonstrate the effectiveness of our estimator with numerical simulations. A central issue in computational neuroscience is the characterization of the functional relationship between sensory stimuli and neural spike trains. A common model for this relationship consists of linear filtering of the stimulus, followed by a nonlinear, probabilistic spike generation process. The linear filter is typically interpreted as the neuron's "receptive field," while the spiking mechanism accounts for simple nonlinearities like rectification and response saturation. Given a set of stimuli and (extracellularly) recorded spike times, the characterization problem consists of estimating both the linear filter and the parameters governing the spiking mechanism. One widely used model of this type is the Linear-Nonlinear-Poisson (LNP) cascade model, in which spikes are generated according to an inhomogeneous Poisson process, with rate determined by an instantaneous ("memoryless") nonlinear function of the filtered input. This model has a number of desirable features, including conceptual simplicity and computational tractability. Additionally, reverse correlation analysis provides a simple unbi-ased estimator for the linear filter [5], and the properties of estimators (for both the linear filter and static nonlinearity) have been thoroughly analyzed, even for the case of highly non-symmetric or "naturalistic" stimuli [12]. One important drawback of the LNP model, * JWP and LP contributed equally to this work. We thank E.J. Chichilnisky for helpful discussions.},
   author = {Jonathan W Pillow and Liam Paninski and Eero P Simoncelli},
   pages = {1311-1318},
   publisher = {MIT Press},
   title = {Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model *},
   volume = {16},
   year = {2004},
}
@article{Sinyavskiy2010,
   abstract = {A generalized model of spiking neuron is proposed as a non-stationary stochastic spike sequences processing unit. The generalized spiking neuron model is described with a conditional spike generation probability distribution and with a state evolution operator. An information theory language is used for the convenient neuron's learning tasks description. The problem of spiking neuron learning with the teacher is solved using information entropy minimization algorithm. A particular implementation of generalized model using stochastic Spike Response Model with alpha-functions set is provided. A task of time delay maintenance between input and output spikes and a task of detecting of a spiking pattern in a noisy stream of pulse signals are considered using extended SRM neuron. It is shown that after the using of the proposed learning method spiking neuron became capable to detect a spatial-temporal pulse pattern and to serve as an adaptive delay unit. © 2010 Allerton Press, Inc.},
   author = {O. Y. Sinyavskiy and A. I. Kobrin},
   doi = {10.3103/S1060992X10040077},
   issn = {1060992X},
   issue = {4},
   journal = {Optical Memory and Neural Networks (Information Optics)},
   keywords = {Information entropy,Information theory,Neuron models,Spiking neuron,Spiking neuron learning},
   pages = {300-309},
   title = {Generalized stochastic spiking neuron model and extended spike response model in spatial-temporal pulse pattern detection task},
   volume = {19},
   year = {2010},
}
@article{Galves2013,
   abstract = {We consider a new class of non Markovian processes with a countable number of interacting components. At each time unit, each component can take two values, indicating if it has a spike or not at this precise moment. The system evolves as follows. For each component, the probability of having a spike at the next time unit depends on the entire time evolution of the system after the last spike time of the component. This class of systems extends in a non trivial way both the interacting particle systems, which are Markovian (Spitzer in Adv. Math. 5:246-290, 1970) and the stochastic chains with memory of variable length which have finite state space (Rissanen in IEEE Trans. Inf. Theory 29(5):656-664, 1983). These features make it suitable to describe the time evolution of biological neural systems. We construct a stationary version of the process by using a probabilistic tool which is a Kalikow-type decomposition either in random environment or in space-time. This construction implies uniqueness of the stationary process. Finally we consider the case where the interactions between components are given by a critical directed Erdös-Rényi-type random graph with a large but finite number of components. In this framework we obtain an explicit upper-bound for the correlation between successive inter-spike intervals which is compatible with previous empirical findings. © 2013 Springer Science+Business Media New York.},
   author = {A. Galves and E. Löcherbach},
   doi = {10.1007/s10955-013-0733-9},
   issn = {00224715},
   issue = {5},
   journal = {Journal of Statistical Physics},
   keywords = {Biological neural nets,Chains of infinite memory,Chains of variable length memory,Hawkes process,Interacting particle systems,Kalikow-decomposition},
   month = {6},
   pages = {896-921},
   publisher = {Springer Science and Business Media, LLC},
   title = {Infinite Systems of Interacting Chains with Memory of Variable Length-A Stochastic Model for Biological Neural Nets},
   volume = {151},
   year = {2013},
}
@article{Kasabov2010,
   abstract = {Spiking neural networks (SNN) are promising artificial neural network (ANN) models as they utilise information representation as trains of spikes, that adds new dimensions of time, frequency and phase to the structure and the functionality of ANN. The current SNN models though are deterministic, that restricts their applications for large scale engineering and cognitive modelling of stochastic processes. This paper proposes a novel probabilistic spiking neuron model (pSNM) and suggests ways of building pSNN for a wide range of applications including classification, string pattern recognition and associative memory. It also extends previously published computational neurogenetic models. © 2009 Elsevier Ltd. All rights reserved.},
   author = {Nikola Kasabov},
   doi = {10.1016/j.neunet.2009.08.010},
   issn = {08936080},
   issue = {1},
   journal = {Neural Networks},
   keywords = {Classification,Computational neurogenetic models,Probabilistic modelling,Quantum inspired evolutionary algorithm,Spiking neural networks},
   month = {1},
   pages = {16-19},
   pmid = {19783402},
   title = {To spike or not to spike: A probabilistic spiking neuron model},
   volume = {23},
   year = {2010},
}
@article{Clayton2011,
   abstract = {This paper introduces a novel probabilistic spike-response model through the combination of avalanche diode-generated Poisson distributed noise, and a standard exponential decay-based spike-response curve. The noise source, which is derived from a 0.35-µm single-photon avalanche diode (kept in the dark), was tested experimentally to verify its characteristics, before being combined with a field-programmable gate-array implementation of a spike-response model. This simple model was then analyzed, and shown to reproduce seven of eight behaviors recorded during an extensive study of the ventral medial hypothalamic (VMH) region of the brain. It is thought that many of the cell types found within the VMH are fed from a tonic noise synaptic input, where the patterns generated are a product of their spike response and not their interconnection. This paper shows how this tonic noise source can be modelled, and due to the independent nature of the noise sources, provides an avenue for the exploration of networks of noise-fueled neurons, which play a significant role in pattern generation within the brain. © 2011, IEEE.},
   author = {Thomas Clayton and Katherine Cameron and Bruce R. Rae and Robert K. Henderson and Alan Murray and Nancy Sabatier and Gareth Leng and Edoardo Charbon},
   doi = {10.1109/TBCAS.2010.2100392},
   issn = {19409990},
   issue = {3},
   journal = {IEEE Transactions on Biomedical Circuits and Systems},
   keywords = {Modelling,single-photon avalanche diode (SPAD),spike-response model,spiking neuron},
   pages = {231-243},
   title = {An Implementation of a Spike-Response Model With Escape Noise Using an Avalanche Diode},
   volume = {5},
   year = {2011},
}
@inproceedings{Yang2015,
   abstract = {The Izhikevich spiking neuron model is a relatively new mathematical framework which is able to represent many observed spiking neuron behaviors, excitatory or inhibitory, by simply adjusting a set of four model parameters. This model is deterministic in nature and has achieved wide applications in analytical and numerical analysis of biological neurons due largely to its biological plausibility and computational efficiency. In this work we present a stochastic version of the Izhikevich neuron, and measure its performance in transmitting information in a range of biological frequencies. The work reveals that the deterministic Izhikevich model has a wide information transmission range and is generally better in transmitting information than its stochastic counterpart.},
   author = {Zhijun Yang and Vaibhav Gandhi and Mehmet Karamanoglu and Bruce Graham},
   doi = {10.1109/IJCNN.2015.7280534},
   isbn = {9781479919604},
   journal = {Proceedings of the International Joint Conference on Neural Networks},
   keywords = {Correlation,Information content,Izhikevich neuron,Mutual information,Probability},
   month = {9},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Characterising information correlation in a stochastic Izhikevich neuron},
   volume = {2015-September},
   year = {2015},
}
@article{Maass2015,
   abstract = {Both the brain and digital computers process information, but they do this in completely different ways. Neurons in the brain transmit information not through bits, but through spikes. Spikes are short voltage increases that are generated near the cell body of a neuron, with average spike rates below 10 Hz. These spikes are transmitted via fine axonal fibers and synapses to about 10 000 other neurons. Neurons also differ in another fundamental aspect from processors in a digital computer: they produce spikes according to stochastic rather than deterministic rules. This article discusses recent progress in understanding how complex computations can be carried out with such stochastically spiking neurons. Other recent developments suggest that spike-based neural networks can be emulated by neuromorphic hardware at a fraction of the energy consumed by current digital computing hardware. Can both developments be merged to provide a blueprint for substantially more energy-efficient computing devices? Explores these issues and examines the viability of such a merger.},
   author = {Wolfgang Maass},
   doi = {10.1109/JPROC.2015.2496679},
   issn = {15582256},
   issue = {12},
   journal = {Proceedings of the IEEE},
   keywords = {Biological neural networks,Brain models,Digital computers,Energy efficiency,Neural networks,Neurons,Optical fiber devices,Optical fiber networks,Program processors,Voltage measurement},
   month = {12},
   pages = {2219-2224},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {To Spike or Not to Spike: That Is the Question},
   volume = {103},
   year = {2015},
}
@inproceedings{Wu2012,
   abstract = {Traditional spiking neural networks (SNNs) uses simulated spiking neuron models for computation units. Action potentials (APs or spikes) are generated when the integrated sensory or synaptic inputs to a neuron reach a threshold value. However, spiking generation is not a deterministic process, making current models limited for their potentials and applications. Here we consider the effects of adding probabilistic parameters to the spiking neuron model, which controls the synapses established during spiking generation and transmitting. The Hebbian learning rule is employed for controlling the probabilistic parameters self-adaptation and connection weights associated with the synapses are established using Thorpe's rule during the network learning procedure. The proposed framework combines the essence of stochastic characteristics of the cortical neurons in vivo, the biologically plausibility of Hodgkin-Huxley type neuron dynamics, as well as the computational efficiency of integrate-and-fire (I&F) type neurons. A simple simulation acquired following aforementioned instructions (based on Izhivich's SNN model) exhibits more explicit behavior and robust performance than the original model and deterministic network organizations. © 2012 IEEE.},
   author = {Ting Wu and Siyao Fu and Long Cheng and Rui Zheng and Xiuqing Wang and Xinkai Kuai and Guosheng Yang},
   doi = {10.1109/IJCNN.2012.6252438},
   isbn = {9781467314909},
   journal = {Proceedings of the International Joint Conference on Neural Networks},
   title = {A simple probabilistic spiking neuron model with Hebbian learning rules},
   year = {2012},
}
@article{Yamakou2019,
   abstract = {In this paper, we provide a complete mathematical construction for a stochastic leaky-integrate-and-fire model (LIF) mimicking the interspike interval (ISI) statistics of a stochastic FitzHugh–Nagumo neuron model (FHN) in the excitable regime, where the unique fixed point is stable. Under specific types of noises, we prove that there exists a global random attractor for the stochastic FHN system. The linearization method is then applied to estimate the firing time and to derive the associated radial equation representing a LIF equation. This result confirms the previous prediction in Ditlevsen and Greenwood (J Math Biol 67(2):239–259, 2013) for the Morris-Lecar neuron model in the bistability regime consisting of a stable fixed point and a stable limit cycle.},
   author = {Marius E. Yamakou and Tat Dat Tran and Luu Hoang Duc and Jürgen Jost},
   doi = {10.1007/s00285-019-01366-z},
   issn = {14321416},
   issue = {2},
   journal = {Journal of Mathematical Biology},
   keywords = {Excitable regime,FitzHugh–Nagumo model,Leaky integrate-and-fire model,Random attractor,Stationary distribution},
   month = {7},
   pages = {509-532},
   pmid = {31049662},
   publisher = {Springer Verlag},
   title = {The stochastic Fitzhugh–Nagumo neuron model in the excitable regime embeds a leaky integrate-and-fire model},
   volume = {79},
   year = {2019},
}
@inproceedings{Bagheri2018,
   abstract = {Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at harnessing the energy efficiency of spike-domain processing by building on computing elements that operate on, and exchange, spikes. In this paper, the problem of training a two-layer SNN is studied for the purpose of classification, under a Generalized Linear Model (GLM) probabilistic neural model that was previously considered within the computational neuroscience literature. Conventional classification rules for SNNs operate offline based on the number of output spikes at each output neuron. In contrast, a novel training method is proposed here for a first-to-spike decoding rule, whereby the SNN can perform an early classification decision once spike firing is detected at an output neuron. Numerical results bring insights into the optimal parameter selection for the GLM neuron and on the accuracy-complexity trade-off performance of conventional and first-to-spike decoding.},
   author = {Alireza Bagheri and Osvaldo Simeone and Bipin Rajendran},
   doi = {10.1109/ICASSP.2018.8462410},
   isbn = {9781538646588},
   issn = {15206149},
   journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
   keywords = {First-to-spike decoding,Generalized Linear Model (GLM),Neuromorphic computing,Spiking Neural Network (SNN)},
   month = {9},
   pages = {2986-2990},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Training probabilistic spiking neural networks with first- to-spike decoding},
   volume = {2018-April},
   year = {2018},
}
@article{Maruan2015,
   abstract = {Recent theoretical studies have shown that probabilistic spiking can be interpreted as learning and inference in cortical microcircuits. This interpretation creates new opportunities for building neuromorphic systems driven by probabilistic learning algorithms. However, such systems must have two crucial features: 1) the neurons should follow a specific behavioral model, and 2) stochastic spiking should be implemented efficiently for it to be scalable. This paper proposes a memristor-based stochastically spiking neuron that fulfills these requirements. First, the analytical model of the memristor is enhanced so it can capture the behavioral stochasticity consistent with experimentally observed phenomena. The switching behavior of the memristor model is demonstrated to be akin to the firing of the stochastic spike response neuron model, the primary building block for probabilistic algorithms in spiking neural networks. Furthermore, the paper proposes a neural soma circuit that utilizes the intrinsic nondeterminism of memristive switching for efficient spike generation. The simulations and analysis of the behavior of a single stochastic neuron and a winner-take-all network built of such neurons and trained on handwritten digits confirm that the circuit can be used for building probabilistic sampling and pattern adaptation machinery in spiking networks. The findings constitute an important step towards scalable and efficient probabilistic neuromorphic platforms.},
   author = {Maruan Al-Shedivat and Rawan Naous and Gert Cauwenberghs and Khaled Nabil Salama},
   doi = {10.1109/JETCAS.2015.2435512},
   issn = {21563357},
   issue = {2},
   journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
   keywords = {Neuromorphic systems,probabilistic inference,probabilistic learning,spiking neurons,stochastic computing,stochastic memristors,winner-take-all},
   month = {6},
   pages = {242-253},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Memristors empower spiking neurons with stochasticity},
   volume = {5},
   year = {2015},
}
@article{Josep2012,
   abstract = {Spiking Neural Networks, the last generation of Artificial Neural Networks, are characterized by its bio-inspired nature and by a higher computational capacity with respect to other neural models. In real biological neurons, stochastic processes represent an important mechanism of neural behavior and are responsible of its special arithmetic capabilities. In this work we present a simple hardware implementation of spiking neurons that considers this probabilistic nature. The advantage of the proposed implementation is that it is fully digital and therefore can be massively implemented in Field Programmable Gate Arrays. The high computational capabilities of the proposed model are demonstrated by the study of both feed-forward and recurrent networks that are able to implement high-speed signal filtering and to solve complex systems of linear equations. © 2012 World Scientific Publishing Company.},
   author = {Josep L. RossellÓ and Vincent Canals and Antoni Morro and Antoni Oliver},
   doi = {10.1142/S0129065712500141},
   issn = {01290657},
   issue = {4},
   journal = {International Journal of Neural Systems},
   keywords = {Gabor filters,Stochastic spiking neural networks,hardware implementation,neural networks,signal processing},
   month = {8},
   pmid = {22830964},
   title = {Hardware implementation of stochastic spiking neural networks},
   volume = {22},
   year = {2012},
}
@book{IEEE2011,
   abstract = {Scholarly & Professional},
   isbn = {9781424496372},
   publisher = {IEEE},
   title = {2011 International Joint Conference on Neural Networks.},
   year = {2011},
}
@article{Jang2019,
   abstract = {Spiking neural networks (SNNs) are distributed trainable systems whose computing elements, or neurons, are characterized by internal analog dynamics and by digital and sparse synaptic communications. The sparsity of the synaptic spiking inputs and the corresponding event-driven nature of neural processing can be leveraged by energy-efficient hardware implementations, which can offer significant energy reductions as compared to conventional artificial neural networks (ANNs). The design of training algorithms for SNNs, however, lags behind hardware implementations: most existing training algorithms for SNNs have been designed either for biological plausibility or through conversion from pretrained ANNs via rate encoding.},
   author = {Hyeryung Jang and Osvaldo Simeone and Brian Gardner and Andre Gruning},
   doi = {10.1109/MSP.2019.2935234},
   issn = {15580792},
   issue = {6},
   journal = {IEEE Signal Processing Magazine},
   month = {11},
   pages = {64-77},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {An Introduction to Probabilistic Spiking Neural Networks: Probabilistic Models, Learning Rules, and Applications},
   volume = {36},
   year = {2019},
}
@report{Dhoble2011,
   abstract = {This paper proposes a novel architecture for continuous spatio-temporal data modeling and pattern recognition utilizing evolving probabilistic spiking neural network 'reservoirs' (epSNNr). The paper demonstrates on a simple experimental data for moving object recognition that: (1) The epSNNr approach is more accurate and flexible than using standard SNN; (2) The use of probabilistic neuronal models is superior in several aspects when compared with the traditional deterministic SNN models, including a better performance on noisy data.},
   author = {Nikola Kasabov and Kshitij Dhoble and Nuttapod Nuntalid and Ammar Mohemmed},
   keywords = {Liquid State Machine,Reser-voir Computing,Spatio-Temporal Patterns,Spiking Neural Network},
   title = {LNCS 7064 - Evolving Probabilistic Spiking Neural Networks for Spatio-temporal Pattern Recognition: A Preliminary Study on Moving Object Recognition},
   url = {http://www.kedri.info},
}
@article{Gaba2013,
   abstract = {Nanoscale resistive switching devices (memristive devices or memristors) have been studied for a number of applications ranging from non-volatile memory, logic to neuromorphic systems. However a major challenge is to address the potentially large variations in space and time in these nanoscale devices. Here we show that in metal-filament based memristive devices the switching can be fully stochastic. While individual switching events are random, the distribution and probability of switching can be well predicted and controlled. Rather than trying to force high switching probabilities using excess voltage or time, the inherent stochastic nature of resistive switching allows these binary devices to be used as building blocks for novel error-tolerant computing schemes such as stochastic computing and provides the needed "analog" feature for neuromorphic applications. To verify such potential, we demonstrated memristor-based stochastic bitstreams in both time and space domains, and show that an array of binary memristors can act as a multi-level "analog" device for neuromorphic applications. © 2013 The Royal Society of Chemistry.},
   author = {Siddharth Gaba and Patrick Sheridan and Jiantao Zhou and Shinhyun Choi and Wei Lu},
   doi = {10.1039/c3nr01176c},
   issn = {20403364},
   issue = {13},
   journal = {Nanoscale},
   month = {7},
   pages = {5872-5878},
   title = {Stochastic memristive devices for computing and neuromorphic applications},
   volume = {5},
   year = {2013},
}
@article{Hsieh2013,
   abstract = {This paper proposes a probabilistic spiking neural network (PSNN) with unimodal weight distribution, possessing long- and short-term plasticity. The proposed algorithm is derived by both the arithmetic gradient decent calculation and bioinspired algorithms. The algorithm is benchmarked by the Iris and Wisconsin breast cancer (WBC) data sets. The network features fast convergence speed and high accuracy. In the experiment, the PSNN took not more than 40 epochs for convergence. The average testing accuracy for Iris and WBC data is 96.7% and 97.2%, respectively. To test the usefulness of the PSNN to real world application, the PSNN was also tested with the odor data, which was collected by our self-developed electronic nose (e-nose). Compared with the algorithm (K-nearest neighbor) that has the highest classification accuracy in the e-nose for the same odor data, the classification accuracy of the PSNN is only 1.3% less but the memory requirement can be reduced at least 40%. All the experiments suggest that the PSNN is hardware friendly. First, it requires only nine-bits weight resolution for training and testing. Second, the PSNN can learn complex data sets with a little number of neurons that in turn reduce the cost of VLSI implementation. In addition, the algorithm is insensitive to synaptic noise and the parameter variation induced by the VLSI fabrication. Therefore, the algorithm can be implemented by either software or hardware, making it suitable for wider application. © 2012 IEEE.},
   author = {Hung Yi Hsieh and Kea Tiong Tang},
   doi = {10.1109/TNNLS.2013.2271644},
   issn = {2162237X},
   issue = {12},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Gradient descent learning,Hebbian learning,hardware compatible,probabilistic spiking neural network,resolution requirement reduction,short-term plasticity},
   pages = {2063-2074},
   pmid = {24805223},
   title = {Hardware friendly probabilistic spiking neural network with long-term and short-term plasticity},
   volume = {24},
   year = {2013},
}
@inproceedings{Nuntalid2011,
   abstract = {This study investigates the feasibility of Bens Spike Algorithm (BSA) to encode continuous EEG spatio-temporal data into input spike streams for a classification in a spiking neural network classifier. A novel evolving probabilistic spiking neural network reservoir (epSNNr) architecture is used for the purpose of learning and classifying the EEG signals after the BSA transformation. Experiments are conducted with EEG data measuring a cognitive state of a single individual under 4 different stimuli. A comparison is drawn between using traditional machine learning algorithms and using BSA plus epSNNr, when different probabilistic models of neurons are utilised. The comparison demonstrates that: (1) The BSA is a suitable transformation for EEG data into spike trains; (2) The performance of the epSNNr improves when a probabilistic model of a neuron is used, compared to the use of a deterministic LIF model of a neuron; (3) The classification accuracy of the EEG data in an epSNNr depends on the type of the probabilistic neuronal model used. The results suggest that an epSNNr can be optimised in terms of neuronal models used and parameters that would better match the noise and the dynamics of EEG data. Potential applications of the proposed method for BCI and medical studies are briefly discussed. © 2011 Springer-Verlag.},
   author = {Nuttapod Nuntalid and Kshitij Dhoble and Nikola Kasabov},
   doi = {10.1007/978-3-642-24955-6_54},
   isbn = {9783642249549},
   issn = {03029743},
   issue = {PART 1},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Electroencephalograms (EEG),Spatio-Temporal Patterns,Stochastic neuron models,evolving probabilistic spiking neural networks},
   pages = {451-460},
   title = {EEG classification with BSA spike encoding algorithm and evolving probabilistic spiking neural network},
   volume = {7062 LNCS},
   year = {2011},
}
@article{Sengupta2016,
   abstract = {Deep spiking neural networks are becoming increasingly powerful tools for cognitive computing platforms. However, most of the existing studies on such computing models are developed with limited insights on the underlying hardware implementation, resulting in area and power expensive designs. Although several neuromimetic devices emulating neural operations have been proposed recently, their functionality has been limited to very simple neural models that may prove to be inefficient at complex recognition tasks. In this paper, we venture into the relatively unexplored area of utilizing the inherent device stochasticity of such neuromimetic devices to model complex neural functionalities in a probabilistic framework in the time domain. We consider the implementation of a deep spiking neural network capable of performing high-accuracy and low-latency classification tasks, where the neural computing unit is enabled by the stochastic switching behavior of a magnetic tunnel junction. The simulation studies indicate an energy improvement of 20 × over a baseline CMOS design in 45-nm technology.},
   author = {Abhronil Sengupta and Maryam Parsa and Bing Han and Kaushik Roy},
   doi = {10.1109/TED.2016.2568762},
   issn = {00189383},
   issue = {7},
   journal = {IEEE Transactions on Electron Devices},
   keywords = {Magnetic tunnel junction (MTJ),neuromorphic computing,spiking neural networks},
   month = {7},
   pages = {2963-2970},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Probabilistic Deep Spiking Neural Systems Enabled by Magnetic Tunnel Junction},
   volume = {63},
   year = {2016},
}
@article{Fonseca2017,
   abstract = {Constraint satisfaction problems (CSP) are at the core of numerous scientific and technological applications. However, CSPs belong to the NP-complete complexity class, for which the existence (or not) of efficient algorithms remains a major unsolved question in computational complexity theory. In the face of this fundamental difficulty heuristics and approximation methods are used to approach instances of NP (e.g., decision and hard optimization problems). The human brain efficiently handles CSPs both in perception and behavior using spiking neural networks (SNNs), and recent studies have demonstrated that the noise embedded within an SNN can be used as a computational resource to solve CSPs. Here, we provide a software framework for the implementation of such noisy neural solvers on the SpiNNaker massively parallel neuromorphic hardware, further demonstrating their potential to implement a stochastic search that solves instances of P and NP problems expressed as CSPs. This facilitates the exploration of new optimization strategies and the understanding of the computational abilities of SNNs. We demonstrate the basic principles of the framework by solving difficult instances of the Sudoku puzzle and of the map color problem, and explore its application to spin glasses. The solver works as a stochastic dynamical system, which is attracted by the configuration that solves the CSP. The noise allows an optimal exploration of the space of configurations, looking for the satisfiability of all the constraints; if applied discontinuously, it can also force the system to leap to a new random configuration effectively causing a restart.},
   author = {Gabriel A. Fonseca Guerra and Steve B. Furber},
   doi = {10.3389/fnins.2017.00714},
   issn = {1662453X},
   issue = {DEC},
   journal = {Frontiers in Neuroscience},
   keywords = {Constraint satisfaction,SpiNNaker,Spiking neural networks,Spiking neurons,Stochastic search},
   month = {12},
   publisher = {Frontiers Media S.A.},
   title = {Using stochastic spiking neural networks on SpiNNaker to solve constraint satisfaction problems},
   volume = {11},
   year = {2017},
}
@article{Lansky2006,
   abstract = {Five parameters of one of the most common neuronal models, the diffusion leaky integrate-and-fire model, also known as the Ornstein-Uhlenbeck neuronal model, were estimated on the basis of intracellular recording. These parameters can be classified into two categories. Three of them (the membrane time constant, the resting potential and the firing threshold) characterize the neuron itself. The remaining two characterize the neuronal input. The intracellular data were collected during spontaneous firing, which in this case is characterized by a Poisson process of interspike intervals. Two methods for the estimation were applied, the regression method and the maximum-likelihood method. Both methods permit to estimate the input parameters and the membrane time constant in a short time window (a single interspike interval). We found that, at least in our example, the regression method gave more consistent results than the maximum-likelihood method. The estimates of the input parameters show the asymptotical normality, which can be further used for statistical testing, under the condition that the data are collected in different experimental situations. The model neuron, as deduced from the determined parameters, works in a subthreshold regimen. This result was confirmed by both applied methods. The subthreshold regimen for this model is characterized by the Poissonian firing. This is in a complete agreement with the observed interspike interval data. © Springer Science + Business Media, LLC 2006.},
   author = {Petr Lansky and Pavel Sanda and Jufang He},
   doi = {10.1007/s10827-006-8527-6},
   issn = {09295313},
   issue = {2},
   journal = {Journal of Computational Neuroscience},
   keywords = {Leaky integrate-and-fire model,Ornstein-Uhlenbeck neuronal model,Parameters estimation,Spontaneous firing},
   month = {10},
   pages = {211-223},
   pmid = {16871351},
   title = {The parameters of the stochastic leaky integrate-and-fire neuronal model},
   volume = {21},
   year = {2006},
}

@article{Gallego2019,
   abstract = {Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of microseconds), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.},
   author = {Guillermo Gallego and Tobi Delbruck and Garrick Orchard and Chiara Bartolozzi and Brian Taba and Andrea Censi and Stefan Leutenegger and Andrew Davison and Joerg Conradt and Kostas Daniilidis and Davide Scaramuzza},
   doi = {10.1109/TPAMI.2020.3008413},
   month = {4},
   title = {Event-based Vision: A Survey},
   url = {http://arxiv.org/abs/1904.08405 http://dx.doi.org/10.1109/TPAMI.2020.3008413},
   year = {2019},
}
@inproceedings{davis,
   author = {M. Litzenberger and B. Kohn and A.N. Belbachir and N. Donath and G. Gritsch and H. Garn and C. Posch and S. Schraml},
   doi = {10.1109/ITSC.2006.1706816},
   isbn = {1-4244-0093-7},
   journal = {2006 IEEE Intelligent Transportation Systems Conference},
   publisher = {IEEE},
   title = {Estimation of Vehicle Speed Based on Asynchronous Data from a Silicon Retina Optical Sensor},
   year = {2006},
}
@article{boahen2004,
   author = {K.A. Boahen},
   doi = {10.1109/TCSI.2004.830703},
   issn = {1057-7122},
   issue = {7},
   journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
   month = {7},
   title = {A Burst-Mode Word-Serial Address-Event Link—I: Transmitter Design},
   volume = {51},
   year = {2004},
}
@book{liu2015,
   author = {Shih-Chii Liu and Tobi Delbruck and Giacomo Indiveri and Adrian Whatley and Rodney Douglas},
   city = {Chichester, UK},
   doi = {10.1002/9781118927601},
   isbn = {9781118927601},
   month = {1},
   publisher = {John Wiley & Sons, Ltd},
   title = {Event-Based Neuromorphic Systems},
   year = {2015},
}
@article{posch2014,
   author = {Christoph Posch and Teresa Serrano-Gotarredona and Bernabe Linares-Barranco and Tobi Delbruck},
   doi = {10.1109/JPROC.2014.2346153},
   issn = {0018-9219},
   issue = {10},
   journal = {Proceedings of the IEEE},
   month = {10},
   title = {Retinomorphic Event-Based Vision Sensors: Bioinspired Cameras With Spiking Output},
   volume = {102},
   year = {2014},
}
@article{indiveri2015,
   author = {Giacomo Indiveri and Shih-Chii Liu},
   doi = {10.1109/JPROC.2015.2444094},
   issn = {0018-9219},
   issue = {8},
   journal = {Proceedings of the IEEE},
   month = {8},
   title = {Memory and Information Processing in Neuromorphic Systems},
   volume = {103},
   year = {2015},
}
@inproceedings{delbruck2010,
   author = {Tobi Delbruck and Bernabe Linares-Barranco and Eugenio Culurciello and Christoph Posch},
   doi = {10.1109/ISCAS.2010.5537149},
   isbn = {978-1-4244-5308-5},
   journal = {Proceedings of 2010 IEEE International Symposium on Circuits and Systems},
   month = {5},
   publisher = {IEEE},
   title = {Activity-driven, event-based vision sensors},
   year = {2010},
}
@article{Lichtsteiner2008,
   author = {Patrick Lichtsteiner and Christoph Posch and Tobi Delbruck},
   doi = {10.1109/JSSC.2007.914337},
   issn = {0018-9200},
   issue = {2},
   journal = {IEEE Journal of Solid-State Circuits},
   title = {A 128×128 120 dB 15 μs latency asynchronous temporal contrast vision sensor},
   volume = {43},
   year = {2008},
}
@article{davis,
   author = {Christian Brandli and Raphael Berner and Minhao Yang and Shih-Chii Liu and Tobi Delbruck},
   doi = {10.1109/JSSC.2014.2342715},
   issn = {0018-9200},
   issue = {10},
   journal = {IEEE Journal of Solid-State Circuits},
   month = {10},
   title = {A 240 × 180 130 dB 3 µs Latency Global Shutter Spatiotemporal Vision Sensor},
   volume = {49},
   year = {2014},
}
@article{fossum1997,
   author = {E.R. Fossum},
   doi = {10.1109/16.628824},
   issn = {00189383},
   issue = {10},
   journal = {IEEE Transactions on Electron Devices},
   title = {CMOS image sensors: electronic camera-on-a-chip},
   volume = {44},
   year = {1997},
}
@article{nozaki2017,
   author = {Yuji Nozaki and Tobi Delbruck},
   doi = {10.1109/TED.2017.2717848},
   issn = {0018-9383},
   issue = {8},
   journal = {IEEE Transactions on Electron Devices},
   month = {8},
   title = {Temperature and Parasitic Photocurrent Effects in Dynamic Vision Sensors},
   volume = {64},
   year = {2017},
}
@article{Gallego2020,
   author = {Guillermo Gallego and Tobi Delbruck and Garrick Michael Orchard and Chiara Bartolozzi and Brian Taba and Andrea Censi and Stefan Leutenegger and Andrew Davison and Jorg Conradt and Kostas Daniilidis and Davide Scaramuzza},
   doi = {10.1109/TPAMI.2020.3008413},
   issn = {0162-8828},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   title = {Event-based Vision: A Survey},
   year = {2020},
}
@article{ahad2012,
   author = {Md. Atiqur Rahman Ahad and J. K. Tan and H. Kim and S. Ishikawa},
   doi = {10.1007/s00138-010-0298-4},
   issn = {0932-8092},
   issue = {2},
   journal = {Machine Vision and Applications},
   month = {3},
   title = {Motion history image: its variants and applications},
   volume = {23},
   year = {2012},
}
@article{lagorge2017,
   author = {Xavier Lagorce and Garrick Orchard and Francesco Galluppi and Bertram E. Shi and Ryad B. Benosman},
   doi = {10.1109/TPAMI.2016.2574707},
   issn = {0162-8828},
   issue = {7},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   month = {7},
   title = {HOTS: A Hierarchy of Event-Based Time-Surfaces for Pattern Recognition},
   volume = {39},
   year = {2017},
}
@article{gallego2019,
   abstract = {Event cameras are novel vision sensors that output pixel-level brightness changes ("events") instead of traditional video frames. These asynchronous sensors offer several advantages over traditional cameras, such as, high temporal resolution, very high dynamic range, and no motion blur. To unlock the potential of such sensors, motion compensation methods have been recently proposed. We present a collection and taxonomy of twenty two objective functions to analyze event alignment in motion compensation approaches (Fig. 1). We call them Focus Loss Functions since they have strong connections with functions used in traditional shape-from-focus applications. The proposed loss functions allow bringing mature computer vision tools to the realm of event cameras. We compare the accuracy and runtime performance of all loss functions on a publicly available dataset, and conclude that the variance, the gradient and the Laplacian magnitudes are among the best loss functions. The applicability of the loss functions is shown on multiple tasks: rotational motion, depth and optical flow estimation. The proposed focus loss functions allow to unlock the outstanding properties of event cameras.},
   author = {Guillermo Gallego and Mathias Gehrig and Davide Scaramuzza},
   month = {4},
   title = {Focus Is All You Need: Loss Functions For Event-based Vision},
   url = {http://arxiv.org/abs/1904.07235},
   year = {2019},
}
@report{rebecq2019,
   abstract = {Event cameras are novel sensors that report brightness changes in the form of a stream of asynchronous "events" instead of intensity frames. They offer significant advantages with respect to conventional cameras: high temporal resolution, high dynamic range, and no motion blur. While the stream of events encodes in principle the complete visual signal, the reconstruction of an intensity image from a stream of events is an ill-posed problem in practice. Existing reconstruction approaches are based on hand-crafted priors and strong assumptions about the imaging process as well as the statistics of natural images. In this work we propose to learn to reconstruct intensity images from event streams directly from data instead of relying on any hand-crafted priors. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. During training we propose to use a perceptual loss to encourage reconstructions to follow natural image statistics. We further extend our approach to synthesize color images from color event streams. Our quantitative experiments show that our network surpasses state-of-the-art reconstruction methods by a large margin in terms of image quality (> 20%), while comfortably running in real-time. We show that the network is able to synthesize high framerate videos (> 5,000 frames per second) of high-speed phenomena (e.g. a bullet hitting an object) and is able to provide high dynamic range reconstructions in challenging lighting conditions. As an additional contribution, we demonstrate the effectiveness of our reconstructions as an intermediate representation for event data. We show that off-the-shelf computer vision algorithms can be applied to our reconstructions for tasks such as object classification and visual-inertial odometry and that this strategy consistently outperforms algorithms that were specifically designed for event data. We release the reconstruction code, a pre-trained model and the datasets to enable further research.},
   author = {Henri Rebecq and René Ren´ and René Ranftl and Vladlen Koltun and Davide Scaramuzza},
   journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
   keywords = {Dynamic Vision Sensor,High Dynamic Range,High Speed,Index Terms-Event-based vision,Video Reconstruction},
   pages = {1},
   title = {High Speed and High Dynamic Range Video with an Event Camera MULTIMEDIA MATERIAL},
   volume = {XX},
   url = {http://rpg.ifi.uzh.ch/e2vid.},
   year = {2019},
}
@inproceedings{bi2019graph,
title={Graph-based Object Classification for Neuromorphic Vision Sensing},
author={Bi, Y and Chadha, A and Abbas, A and and Bourtsoulatze, E and Andreopoulos, Y},
booktitle={2019 IEEE International Conference on Computer Vision (ICCV)},
year={2019},
organization={IEEE}
}
@inproceedings{ochard2018,
 author = {Shrestha, Sumit Bam and Orchard, Garrick},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {SLAYER: Spike Layer Error Reassignment in Time},
 url = {https://proceedings.neurips.cc/paper/2018/file/82f2b308c3b01637c607ce05f52a2fed-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{Guo2021,
   abstract = {Various hypotheses of information representation in brain, referred to as neural codes, have been proposed to explain the information transmission between neurons. Neural coding plays an essential role in enabling the brain-inspired spiking neural networks (SNNs) to perform different tasks. To search for the best coding scheme, we performed an extensive comparative study on the impact and performance of four important neural coding schemes, namely, rate coding, time-to-first spike (TTFS) coding, phase coding, and burst coding. The comparative study was carried out using a biological 2-layer SNN trained with an unsupervised spike-timing-dependent plasticity (STDP) algorithm. Various aspects of network performance were considered, including classification accuracy, processing latency, synaptic operations (SOPs), hardware implementation, network compression efficacy, input and synaptic noise resilience, and synaptic fault tolerance. The classification tasks on Modified National Institute of Standards and Technology (MNIST) and Fashion-MNIST datasets were applied in our study. For hardware implementation, area and power consumption were estimated for these coding schemes, and the network compression efficacy was analyzed using pruning and quantization techniques. Different types of input noise and noise variations in the datasets were considered and applied. Furthermore, the robustness of each coding scheme to the non-ideality-induced synaptic noise and fault in analog neuromorphic systems was studied and compared. Our results show that TTFS coding is the best choice in achieving the highest computational performance with very low hardware implementation overhead. TTFS coding requires 4x/7.5x lower processing latency and 3.5x/6.5x fewer SOPs than rate coding during the training/inference process. Phase coding is the most resilient scheme to input noise. Burst coding offers the highest network compression efficacy and the best overall robustness to hardware non-idealities for both training and inference processes. The study presented in this paper reveals the design space created by the choice of each coding scheme, allowing designers to frame each scheme in terms of its strength and weakness given a designs’ constraints and considerations in neuromorphic systems.},
   author = {Wenzhe Guo and Mohammed E. Fouda and Ahmed M. Eltawil and Khaled Nabil Salama},
   doi = {10.3389/fnins.2021.638474},
   issn = {1662453X},
   journal = {Frontiers in Neuroscience},
   keywords = {burst coding,neural codes,neuromorphic computing,phase coding,rate coding,spiking neural networks,time to first spike coding,unsupervised learning},
   month = {3},
   publisher = {Frontiers Media S.A.},
   title = {Neural Coding in Spiking Neural Networks: A Comparative Study for Robust Neuromorphic Systems},
   volume = {15},
   year = {2021},
}
@report{Richmond1987,
   abstract = {AND CONCLUSIONS 1. The purpose of this study was to describe how the responses of neurons in inferior temporal (IT) cortex represent visual stimuli. In the preceding paper we described the responses of IT neurons to a large set of two-dimensional black and white patterns. The responses to different stimuli showed temporal modulation of the spike trains. This paper develops a method for quantifying temporal modulation and shows that the stimulus determines the distribution over time, as well as the number, of spikes in a response. 2. The responses were quantified using an orthogonal set of temporal waveforms called principal components. The principal components related to each neuron were extracted from all the responses of that neuron to all of the stimuli, regardless of which stimulus elicited which response. Each response was then projected onto the set of principal components to obtain a set of coefficients that quantified its temporal modulation.},
   author = {Barry J Richmond and Lance M Optican},
   issue = {1},
   journal = {JOURNALOF NEUROPHYSIOLOGY},
   title = {Temporal Encoding of Two-Dimensional Patterns by Single Units in Primate Inferior Temporal Cortex. II. Quantification of Response Waveform},
   volume = {57},
   url = {www.physiology.org/journal/jn},
   year = {1987},
}
@article{Lobov2020,
   abstract = {One of the modern trends in the design of human–machine interfaces (HMI) is to involve the so called spiking neuron networks (SNNs) in signal processing. The SNNs can be trained by simple and efficient biologically inspired algorithms. In particular, we have shown that sensory neurons in the input layer of SNNs can simultaneously encode the input signal based both on the spiking frequency rate and on varying the latency in generating spikes. In the case of such mixed temporal-rate coding, the SNN should implement learning working properly for both types of coding. Based on this, we investigate how a single neuron can be trained with pure rate and temporal patterns, and then build a universal SNN that is trained using mixed coding. In particular, we study Hebbian and competitive learning in SNN in the context of temporal and rate coding problems. We show that the use of Hebbian learning through pair-based and triplet-based spike timing-dependent plasticity (STDP) rule is accomplishable for temporal coding, but not for rate coding. Synaptic competition inducing depression of poorly used synapses is required to ensure a neural selectivity in the rate coding. This kind of competition can be implemented by the so-called forgetting function that is dependent on neuron activity. We show that coherent use of the triplet-based STDP and synaptic competition with the forgetting function is sufficient for the rate coding. Next, we propose a SNN capable of classifying electromyographical (EMG) patterns using an unsupervised learning procedure. The neuron competition achieved via lateral inhibition ensures the “winner takes all” principle among classifier neurons. The SNN also provides gradual output response dependent on muscular contraction strength. Furthermore, we modify the SNN to implement a supervised learning method based on stimulation of the target classifier neuron synchronously with the network input. In a problem of discrimination of three EMG patterns, the SNN with supervised learning shows median accuracy 99.5% that is close to the result demonstrated by multi-layer perceptron learned by back propagation of an error algorithm.},
   author = {Sergey A. Lobov and Andrey V. Chernyshov and Nadia P. Krilova and Maxim O. Shamshin and Victor B. Kazantsev},
   doi = {10.3390/s20020500},
   issn = {14248220},
   issue = {2},
   journal = {Sensors (Switzerland)},
   keywords = {EMG interface,Lateral inhibition,Neural competition,Pair-based STDP,Rate coding,STDP,Synaptic competition,Temporal coding,Triplet-based STDP},
   month = {1},
   pmid = {31963143},
   publisher = {MDPI AG},
   title = {Competitive learning in a spiking neural network: Towards an intelligent pattern classifier},
   volume = {20},
   year = {2020},
}
@article{Ignatov2015,
   abstract = {Perception, decisions, and sensations are all encoded into trains of action potentials in the brain. The relation between stimulus strength and all-or-nothing spiking of neurons is widely believed to be the basis of this coding. This initiated the development of spiking neuron models; one of today's most powerful conceptual tool for the analysis and emulation of neural dynamics. The success of electronic circuit models and their physical realization within silicon field-effect transistor circuits lead to elegant technical approaches. Recently, the spectrum of electronic devices for neural computing has been extended by memristive devices, mainly used to emulate static synaptic functionality. Their capabilities for emulations of neural activity were recently demonstrated using a memristive neuristor circuit, while a memristive neuron circuit has so far been elusive. Here, a spiking neuron model is experimentally realized in a compact circuit comprising memristive and memcapacitive devices based on the strongly correlated electron material vanadium dioxide (VO2) and on the chemical electromigration cell Ag/TiO2-x/Al. The circuit can emulate dynamical spiking patterns in response to an external stimulus including adaptation, which is at the heart of firing rate coding as first observed by E.D. Adrian in 1926.},
   author = {Marina Ignatov and Martin Ziegler and Mirko Hansen and Adrian Petraru and Hermann Kohlstedt},
   doi = {10.3389/fnins.2015.00376},
   issn = {1662453X},
   issue = {OCT},
   journal = {Frontiers in Neuroscience},
   keywords = {Memristive devices,Negative differential resistor,Neural coding,Neuromorphic systems,Spiking neuron},
   publisher = {Frontiers Media S.A.},
   title = {A memristive spiking neuron with firing rate coding},
   volume = {9},
   year = {2015},
}
@report{Richmond1987,
   abstract = {AND CONCLUSIONS 1. We seek a general approach to determine what stimulus features visual neurons are sensitive to and how those features are represented by the neuron's responses. Because lesions of inferior temporal (IT) cortex interfere with a monkey's ability to perform pattern discrimination tasks we studied IT neurons. Previous single-unit studies have shown that IT neurons sometimes respond more strongly to complex stimuli (brushes, hands, faces) than to simple stimuli (bars, slits, edges). However, it is not known how specific stimulus parameters are represented by responses. 2. We studied the responses of IT neurons in alert behaving monkeys to a large set of two-dimensional black and white patterns. The stimulus set was based on 64 Walsh functions that can be used to represent any picture with a resolution of one part in eight along each of two dimensions. The responses to these stimuli spanned a continuum from inhibition to strong excitation. A statistical test showed that the spike count was determined by which Walsh stimulus was presented. Hence, these stimuli form an adequate set for testing IT neurons. 3. The responses showed temporal modulation of the spike train that could not be represented by a change in the spike count alone. Examples of this modulation were changes in latency, changes in the duration of the response , and alternating periods of excitation and inhibition. This temporal modulation may be important in representing stimulus parameters. The next paper in this series develops a method for quantifying this temporal modulation and shows that it is dependent on the stimulus. The third paper in this series shows that this temporal modulation contains more information about stimulus parameters than is contained in the spike count alone.},
   author = {Barry J Richmond and Lance M Optican and Michael Podell and Hedva Spitzer},
   issue = {1},
   journal = {JOURNALOF NEUROPHYSIOL~GY},
   title = {Temporal Encoding of Two-Dimensional Patterns by Single Units in Primate Inferior Temporal Cortex. I. Response Characteristics},
   volume = {57},
   url = {www.physiology.org/journal/jn},
   year = {1987},
}
@article{Sakemi2020,
   abstract = {Spiking neural networks (SNNs) are brain-inspired mathematical models with the ability to process information in the form of spikes. SNNs are expected to provide not only new machine-learning algorithms, but also energy-efficient computational models when implemented in VLSI circuits. In this paper, we propose a novel supervised learning algorithm for SNNs based on temporal coding. A spiking neuron in this algorithm is designed to facilitate analog VLSI implementations with analog resistive memory, by which ultra-high energy efficiency can be achieved. We also propose several techniques to improve the performance on a recognition task, and show that the classification accuracy of the proposed algorithm is as high as that of the state-of-the-art temporal coding SNN algorithms on the MNIST dataset. Finally, we discuss the robustness of the proposed SNNs against variations that arise from the device manufacturing process and are unavoidable in analog VLSI implementation. We also propose a technique to suppress the effects of variations in the manufacturing process on the recognition performance.},
   author = {Yusuke Sakemi and Kai Morino and Takashi Morie and Kazuyuki Aihara},
   month = {1},
   title = {A Supervised Learning Algorithm for Multilayer Spiking Neural Networks Based on Temporal Coding Toward Energy-Efficient VLSI Processor Design},
   url = {http://arxiv.org/abs/2001.05348},
   year = {2020},
}
@report{Mainen2009,
   author = {Mainen and Sejnowski},
   title = {Reliability of Spike Timing in Neocortical Neurons},
   url = {www.sciencemag.org},
}
@report{Maass1999,
   abstract = {Spiking neurons are models for the computational units in biological neural systems where information is considered to be encoded mainly in the temporal patterns of their activity. In a network of spiking neurons a new set of parameters becomes relevant which has no counterpart in traditional neural network models: the time that a pulse needs to travel through a connection between two neurons (also known as delay of a connection). It is known that these delays are tuned in biological neural systems through a variety of mechanisms. In this article we consider the arguably most simple model for a spiking neuron, which can also easily be implemented in pulsed VLSI. We investigate the VapnikkChervonenkis (VC) dimension of networks of spiking neurons, where the delays are viewed as programmable parameters and we prove tight bounds for this VC dimension. Thus, we get quantitative estimates for the diversity of functions that a network with fixed architecture can compute with different settings of its delays. In particular, it turns out that a network of spiking neurons with k adjustable delays is able to compute a much richer class of functions than a threshold circuit with k adjustable weights. The results also yield bounds for the number of training examples that an algorithm needs for tuning the delays of a network of spiking neurons. Results about the computational complexity of such algorithms are also given. ]},
   author = {Wolfgang Maass and Michael Schmitt},
   journal = {Information and Computation},
   pages = {26646},
   title = {On the Complexity of Learning for Spiking Neurons with Temporal Coding 1},
   volume = {153},
   url = {www.idealibrary.com},
   year = {1999},
}
@article{Mostafa2018,
   abstract = {Gradient descent training techniques are remarkably successful in training analog-valued artificial neural networks (ANNs). Such training techniques, however, do not transfer easily to spiking networks due to the spike generation hard nonlinearity and the discrete nature of spike communication. We show that in a feedforward spiking network that uses a temporal coding scheme where information is encoded in spike times instead of spike rates, the network input-output relation is differentiable almost everywhere. Moreover, this relation is piecewise linear after a transformation of variables. Methods for training ANNs thus carry directly to the training of such spiking networks as we show when training on the permutation invariant MNIST task. In contrast to rate-based spiking networks that are often used to approximate the behavior of ANNs, the networks we present spike much more sparsely and their behavior cannot be directly approximated by conventional ANNs. Our results highlight a new approach for controlling the behavior of spiking networks with realistic temporal dynamics, opening up the potential for using these networks to process spike patterns with complex temporal information.},
   author = {Hesham Mostafa},
   doi = {10.1109/TNNLS.2017.2726060},
   issn = {21622388},
   issue = {7},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Backpropagation,spiking networks,supervised learning},
   month = {7},
   pages = {3227-3235},
   pmid = {28783639},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Supervised learning based on temporal coding in spiking neural networks},
   volume = {29},
   year = {2018},
}
@book{Kiselev2016,
   abstract = {"Part Number: CFP16IJS-USB." " ... held at the Vancouver Convention Centre, Canada, as part of the IEEE World Congress on Computational Intelligence (IEEE WCC!) 2016. IJCNN 2016 is jointly organized by the IEEE Computational Intelligence Society (CIS) and the International Neural Network Society (INNS) ... IEEE CIS is the lead society and financial sponsor."--PDF page ii.},
   author = {IEEE Computational Intelligence Society and International Neural Network Society and Institute of Electrical and Electronics Engineers and B.C.) IEEE World Congress on Computational Intelligence (2016 : Vancouver},
   isbn = {9781509006205},
   pages = {5301},
   title = {2016 International Joint Conference on Neural Networks (IJCNN) : 24-29 July 2016, Vancouver, Canada.},
}
@report{Poutr2001,
   abstract = {title = \{Unsupervised Clustering with Spiking Neurons By Sparse Temporal Coding and Multilayer RBF Networks\}, journal = \{\{IEEE\} \{T\}rans. \{N\}eural \{N\}etworks\}, year = \{2002\}, volume = \{13\}, number = \{2\}, month = \{March\}, pages = \{1-10\}, note = \{To appear; an abstract has appeared in the proceedings of IJCNN'2000\} \} This reprint corresponds to the article "Unsupervised Clustering with Spiking Neurons By Sparse Temporal Coding and Multi-layer RBF Networks", by Sander M. Bohte, Joost N. Kok, and Han La Poutré, and appeared in the IEEE Transactions on Abstract-We demonstrate that spiking neural networks encoding information in the timing of single spikes are capable of computing and learning clusters from realistic data. We show how a spiking neural network based on spike-time coding and Hebbian learning can successfully perform unsu-pervised clustering on real-world data, and we demonstrate how temporal synchrony in a multi-layer network can induce hierarchical clustering. We develop a temporal encoding of continuously valued data to obtain adjustable clustering capacity and precision with an efficient use of neurons: input variables are encoded in a population code by neurons with graded and overlapping sensitivity profiles. We also discuss methods for enhancing scale-sensitivity of the network and show how the induced synchronization of neurons within early RBF layers allows for the subsequent detection of complex clusters.},
   author = {La Poutr and Sander M Bohte and Han La Poutré and Joost N Kok},
   keywords = {Hebbian-learning,Spiking neurons,coarse coding,complex clusters,high-dimensional clustering,sparse coding,synchronous firing,temporal coding,unsupervised learning},
   title = {UNSUPERVISED CLUSTERING WITH SPIKING NEURONS BY SPARSE TEMPORAL CODING AND MULTILAYER RBF NETWORKS @Article\{ieee:bohte+kok+lapoutre:2002, author = \{Bohte Unsupervised Clustering with Spiking Neurons by Sparse Temporal Coding and Multi-Layer RBF Networks},
   year = {2001},
}
@book{IEEE2018,
   abstract = {Scholarly & Professional ISCAS is the world s premier networking and exchange forum for leading researchers in the highly active fields of theory, design and implementation of circuits and systems ISCAS is the flagship conference of the IEEE Circuits and Systems Society and aim to bring together its multidisciplinary community that has a strong history of cultivating creative, proof of concept research in a diverse range of technical domains including analog and digital circuits and systems, nanotechnology, sensors, nonlinear systems, biosystems, neural systems, signal processing, and communications.},
   author = {IEEE Staff},
   isbn = {9781538648810},
   publisher = {IEEE},
   title = {2018 IEEE International Symposium on Circuits and Systems (ISCAS).},
   year = {2018},
}
@report{Bohte2002,
   abstract = {For a network of spiking neurons that encodes information in the timing of individual spike times, we derive a supervised learning rule, SpikeProp, akin to traditional error-backpropagation. With this algorithm, we demonstrate how networks of spiking neurons with biologically reasonable action potentials can perform complex non-linear classiÿcation in fast temporal coding just as well as rate-coded networks. We perform experiments for the classical XOR problem, when posed in a temporal setting, as well as for a number of other benchmark datasets. Comparing the (implicit) number of spiking neurons required for the encoding of the interpolated XOR problem, the trained networks demonstrate that temporal coding is a viable code for fast neural information processing, and as such requires less neurons than instantaneous rate-coding. Furthermore, we ÿnd that reliable temporal computation in the spiking networks was only accomplished when using spike response functions with a time constant longer than the coding interval, as has been predicted by theoretical considerations.},
   author = {Sander M Bohte and Joost N Kok and Han La Poutrã},
   journal = {Neurocomputing},
   keywords = {Error-backpropagation,Spiking neurons,Temporal coding},
   pages = {17-37},
   title = {Error-backpropagation in temporally encoded networks of spiking neurons},
   volume = {48},
   url = {www.elsevier.com/locate/neucom},
   year = {2002},
}
@article{Huxter2003,
   abstract = {In the brain, hippocampal pyramidal cells use temporal1as wellas rate2coding to signal spatial aspects of the animal’s environ-ment or behaviour. The temporal code takes the form of a phaserelationship to the concurrent cycle of the hippocampal electro-encephalogram theta rhythm1. These two codes could eachrepresent a different variable3,4. However, this requires the rateand phase to vary independently, in contrast to recent sugges-tions5,6that they are tightly coupled, both reflecting the ampli-tude of the cell’s input. Here we show that the time of firing andfiring rate are dissociable, and can represent two independentvariables: respectively the animal’s location within the placefield, and its speed of movement through the field. Independentencoding of location together with actions and stimuli occurringthere may help to explain the dual roles of the hippocampusin spatial and episodic memory7,8, or may indicate a moregeneral role of the hippocampus in relational/declarativememory},
   author = {John Huxter and Neil Burgess and John O’Keefe},
   doi = {10.1038/nature02038},
   issn = {00280836},
   issue = {6960},
   journal = {Nature},
   month = {10},
   pages = {824-828},
   title = {Independent rate and temporal coding in hippocampal pyramidal cells},
   volume = {425},
   year = {2003},
}
@report{Yin201,
   abstract = {We present a new back propagation based training algorithm for discrete-time spiking neural networks (SNN). Inspired by recent deep learning algorithms on binarized neural networks, binary activation with a straight-through gradient estimator is used to model the leaky integrate-fire spiking neuron, overcoming the difficulty in training SNNs using back propagation. Two SNN training algorithms are proposed: (1) SNN with discontinuous integration, which is suitable for rate-coded input spikes, and (2) SNN with continuous integration, which is more general and can handle input spikes with temporal information. Neuromorphic hardware designed in 40nm CMOS exploits the spike sparsity and demonstrates high classification accuracy (>98% on MNIST) and low energy (48.4-773 nJ/image).},
   author = {Shihui Yin and Shreyas K Venkataramanaiah and Gregory K Chen and Ram Krishnamurthy and Yu Cao and Chaitali Chakrabarti and Jae-Sun Seo},
   keywords = {Spiking neural networks,back propagation,neuromorphic hardware,straight-through estimator},
   title = {Algorithm and Hardware Design of Discrete-Time Spiking Neural Networks Based on Back Propagation with Binary Activations},
}
@report{,
   abstract = {Hardware-based spiking neural networks (SNNs) are regarded as promising candidates for the cognitive computing system due to low power consumption and highly parallel operation. In this work, we train the SNN in which the firing time carries information using temporal backpropagation. The temporally encoded SNN with 512 hidden neurons showed an accuracy of 96.90% for the MNIST test set. Furthermore, the effect of the device variation on the accuracy in temporally encoded SNN is investigated and compared with that of the rate-encoded network. In a hardware configuration of our SNN, NOR-type analog memory having an asymmetric floating gate is used as a synaptic device. In addition, we propose a neuron circuit including a refractory period generator for temporally encoded SNN. The performance of the 2-layer neural network consisting of synapses and proposed neurons is evaluated through circuit simulation using SPICE. The network with 128 hidden neurons showed an accuracy of 94.9%, a 0.1% reduction compared to that of the system simulation of the MNIST dataset. Finally, the latency and power consumption of each block constituting the temporal network is analyzed and compared with those of the rate-encoded network depending on the total time step. Assuming that the total time step number of the network is 256, the temporal network consumes 15.12 times lower power than the rate-encoded network and can make decisions 5.68 times faster.},
   author = {Seongbin Oh and Dongseok Kwon and Gyuho Yeom and Won-Mook Kang and Soochang Lee and Sung Yun Woo and Jang Saeng Kim and Min Kyu Park and Jong-Ho Lee},
   keywords = {Hardware-based Neural Networks,Neuromorphic,Neuron Circuits,Spiking Neural Networks (SNNs),Temporal Encoding,Time-to-First-Spike Encoding},
   title = {Hardware Implementation of Spiking Neural Networks Using Time-To-First-Spike Encoding},
}
@article{Sengupta2017,
   abstract = {The human brain's ability to efficiently detect patterns from the continuous streaming sensory stimuli has been a source of constant intrigue for naturalists, and has set the course for the development of research into artificial intelligence. Efficient encoding of such input stimuli into discrete timing of events play a decisive role in the ability of the spiking neurons inside the human brain to compress, transmit and recognise information presented by the external environment. In this article, we introduce the spike-time or temporal encoding paradigm as an efficient general approach to data compression for the purpose of pattern recognition. The data compression through spike-time encoding not only dramatically reduces the volume of data required to capture discriminatory information leading to economical storage and transmission, but can also be used for pattern recognition in streaming data domain. We experimentally show that the spike-time data produced by the temporal encoding techniques achieve comparable (superior in some cases) performance of pattern recognition in comparison to the use of the whole raw data. This article also introduces a generalised background knowledge driven optimisation based temporal encoding framework for encoding time series data and as an illustration of this approach, further formulates a temporal encoding algorithm, namely GAGamma, designed to efficiently compress fMRI data using discrete spike-times. We have evaluated the temporal encoding algorithms on the benchmark Starplus fMRI dataset, and the results demonstrate the temporal encoding algorithm's ability to achieve significant data compression without sacrificing the performance of the pattern recognition in the compressed space. We also show that the GAGamma algorithm provides enhanced flexibility and control (compared to the state of the art temporal encoding algorithms) within the design framework of the temporal encoding problem for fMRI, leading to better quality of spike-time data. The proposed approach, by the use of efficient brain-like encoding mechanism, opens up new possibilities in information compression, communication and pattern recognition and thus is applicable to a range of new applications.},
   author = {Neelava Sengupta and Nikola Kasabov},
   doi = {10.1016/j.ins.2017.04.017},
   issn = {00200255},
   journal = {Information Sciences},
   keywords = {Data compression,Pattern recognition,Spike-time encoding},
   month = {9},
   pages = {133-145},
   publisher = {Elsevier Inc.},
   title = {Spike-time encoding as a data compression technique for pattern recognition of temporal data},
   volume = {406-407},
   year = {2017},
}
@report{,
   abstract = {This paper presents a CMOS vision sensor based on a biologically inspired data representation referred to as Time-to-First-Spike (TFS) encoding combined with a fair Address Event Representation (AER) scheme. Our approach is different from conventional methods because the read-out of information is initiated by the pixel itself while access to the read-out bus is granted only once to each pixel after which it enters into a stand-by mode. This approach allows to greatly save dynamic power consumption and to extensively reduce inefficiencies due to periodical requests of the bus in the case of spiking pixels. Transmission bandwidth is thus significantly improved using the proposed circuitry. Each pixel includes only 14 transistors and occupies an area of ½½ ¢ ½½Ñ ¾ , with a fill factor of 33% using ¼¿¿Ñ process. The average current consumption is estimated to ½¼ÒÒ per pixel, which is 3 orders of magnitude lower compared with that of the spiking pixel.},
   author = {Chen Shoushun and Amine Bermak},
   title = {A Low Power CMOS Imager based on Time-to-First-Spike encoding and Fair AER},
}
@report{,
   author = {Randy Goebel and Wolfgang Wahlster and Joerg Siekmann},
   title = {Lecture Notes in Artificial Intelligence 9896 Subseries of Lecture Notes in Computer Science LNAI Series Editors LNAI Founding Series Editor},
   url = {http://www.springer.com/series/1244},
}
@article{Dupeyroux2021,
   abstract = {The third generation of artificial intelligence (AI) introduced by neuromorphic computing is revolutionizing the way robots and autonomous systems can sense the world, process the information, and interact with their environment. The promises of high flexibility, energy efficiency, and robustness of neuromorphic systems is widely supported by software tools for simulating spiking neural networks, and hardware integration (neuromorphic processors). Yet, while efforts have been made on neuromorphic vision (event-based cameras), it is worth noting that most of the sensors available for robotics remain inherently incompatible with neuromorphic computing, where information is encoded into spikes. To facilitate the use of traditional sensors, we need to convert the output signals into streams of spikes, i.e., a series of events (+1, -1) along with their corresponding timestamps. In this paper, we propose a review of the coding algorithms from a robotics perspective and further supported by a benchmark to assess their performance. We also introduce a ROS (Robot Operating System) toolbox to encode and decode input signals coming from any type of sensor available on a robot. This initiative is meant to stimulate and facilitate robotic integration of neuromorphic AI, with the opportunity to adapt traditional off-the-shelf sensors to spiking neural nets within one of the most powerful robotic tools, ROS.},
   author = {Julien Dupeyroux},
   month = {3},
   title = {A toolbox for neuromorphic sensing in robotics},
   url = {http://arxiv.org/abs/2103.02751},
   year = {2021},
}
@article{Kasabov2016,
   abstract = {The paper describes a new type of evolving connectionist systems (ECOS) called evolving spatio-temporal data machines based on neuromorphic, brain-like information processing principles (eSTDM). These are multi-modular computer systems designed to deal with large and fast spatio/spectro temporal data using spiking neural networks (SNN) as major processing modules. ECOS and eSTDM in particular can learn incrementally from data streams, can include 'on the fly' new input variables, new output class labels or regression outputs, can continuously adapt their structure and functionality, can be visualised and interpreted for new knowledge discovery and for a better understanding of the data and the processes that generated it. eSTDM can be used for early event prediction due to the ability of the SNN to spike early, before whole input vectors (they were trained on) are presented. A framework for building eSTDM called NeuCube along with a design methodology for building eSTDM using this is presented. The implementation of this framework in MATLAB, Java, and PyNN (Python) is presented. The latter facilitates the use of neuromorphic hardware platforms to run the eSTDM. Selected examples are given of eSTDM for pattern recognition and early event prediction on EEG data, fMRI data, multisensory seismic data, ecological data, climate data, audio-visual data. Future directions are discussed, including extension of the NeuCube framework for building neurogenetic eSTDM and also new applications of eSTDM.},
   author = {Nikola Kasabov and Nathan Matthew Scott and Enmei Tu and Stefan Marks and Neelava Sengupta and Elisa Capecci and Muhaini Othman and Maryam Gholami Doborjeh and Norhanifah Murli and Reggio Hartono and Josafath Israel Espinosa-Ramos and Lei Zhou and Fahad Bashir Alvi and Grace Wang and Denise Taylor and Valery Feigin and Sergei Gulyaev and Mahmoud Mahmoud and Zeng Guang Hou and Jie Yang},
   doi = {10.1016/j.neunet.2015.09.011},
   issn = {18792782},
   journal = {Neural Networks},
   keywords = {Computational neurogenetic systems,Evolving connectionist systems,Evolving spatio-temporal data machines,Evolving spiking neural networks,NeuCube,Spatio/spectro temporal data},
   month = {6},
   pages = {1-14},
   pmid = {26576468},
   publisher = {Elsevier Ltd},
   title = {Evolving spatio-temporal data machines based on the NeuCube neuromorphic framework: Design methodology and selected applications},
   volume = {78},
   year = {2016},
}
@inproceedings{Schrauwen2003,
   abstract = {In this paper we introduce a new algorithm for encoding analog information into spike trains, given that the reconstruction will take place using a FIR filter. An older technique called HSA is reviewed and an optimal threshold value is found. A new technique called BSA is introduced. These methods are then compared experimentally.},
   author = {Benjamin Schrauwen and Jan Van Campenhout},
   doi = {10.1109/ijcnn.2003.1224019},
   journal = {Proceedings of the International Joint Conference on Neural Networks},
   pages = {2825-2830},
   title = {BSA, a Fast and Accurate Spike Train Encoding Scheme},
   volume = {4},
   year = {2003},
}
@report{Gers1999,
   abstract = {This paper presents an algorithm which converts an arbitrary analog time-varying signal into a digital spiketrain (a bit string of 0's interspersed with 1's), where the information is contained in the spacing between the spikes. This conversion is an important ingredient in the CAM-Brain Project, as it allows the user of ATR's CAM-Brain Machine (CBM) to think entirely in terms of analog signals, and not the more abstract, visually rather meaningless, spike-trains. The SPIKER conversion completes a package of which makes the evolution of individual CBM based neural network modules easier to think about and to accomplish.},
   author = {Felix Gers and Norberto Eiji Nawa and Michael Hough and Hugo De Garis and Michael Korkin},
   keywords = {CAM-Brain project,CoDi neu-ral model,evolutionary neural networks,hard-ware implementation,information representation schemes},
   title = {SPIKER: Analog Waveform to Digital Spiketrain Conversion in ATR&apos;s Artificial Brain (CAM-Brain) Project Digital Microbiology Lab View project CoDi-A CELLULAR AUTOMATA BASED NEURAL NET MODEL View project Michael Korkin Independent Researcher},
   url = {http://www.stanford.edu/mhoughhttp://www.hip.atr.co.jp/fdegaris,xnawaghttp://www.genobyte.comhttp://www.idsia.ch/felix},
   year = {1999},
}
@book{,
   abstract = {Co-located with the International Symposium on Intelligent Systems Technologies and Applications (ISTA 2015), the Third International Symposium on Women in Computing and Informatics (WCI 2015), the Third International Symposium on Security in Computing and Communications (SSCC 2015) and the Second International Symposium on Computer Vision and the Internet (VisionNet 2015)},
   author = {Jaime Lloret Mauri and IEEE Communications Society and Man IEEE Systems and Annual IEEE Computer Conference and Communications and Informatics (ICACCI) 4 2015.08.10-13 Kochi International Conference on Advances in Computing and Kerala International Symposium on Emerging Topics in Circuits and Systems (SET-CAS) 2015.08.10 Kochi and Automation International Symposium on Control and Kerala International Symposium on Recent Advances in Medical Informatics (RAMI) 4 2015.08.10 Kochi and Kerala International Symposium on Natural Language Processing (NLP) 4 2015.08.13 Kochi},
   isbn = {9781479987924},
   title = {International Conference on Advances in Computing, Communications and Informatics (ICACCI), 2015 10-13 Aug. 2015, SCMS, Aluva, Kochi, Kerala, India ; [including co-affiliated symposia]},
}
@article{Sengupta2017,
   abstract = {The human brain's ability to efficiently detect patterns from the continuous streaming sensory stimuli has been a source of constant intrigue for naturalists, and has set the course for the development of research into artificial intelligence. Efficient encoding of such input stimuli into discrete timing of events play a decisive role in the ability of the spiking neurons inside the human brain to compress, transmit and recognise information presented by the external environment. In this article, we introduce the spike-time or temporal encoding paradigm as an efficient general approach to data compression for the purpose of pattern recognition. The data compression through spike-time encoding not only dramatically reduces the volume of data required to capture discriminatory information leading to economical storage and transmission, but can also be used for pattern recognition in streaming data domain. We experimentally show that the spike-time data produced by the temporal encoding techniques achieve comparable (superior in some cases) performance of pattern recognition in comparison to the use of the whole raw data. This article also introduces a generalised background knowledge driven optimisation based temporal encoding framework for encoding time series data and as an illustration of this approach, further formulates a temporal encoding algorithm, namely GAGamma, designed to efficiently compress fMRI data using discrete spike-times. We have evaluated the temporal encoding algorithms on the benchmark Starplus fMRI dataset, and the results demonstrate the temporal encoding algorithm's ability to achieve significant data compression without sacrificing the performance of the pattern recognition in the compressed space. We also show that the GAGamma algorithm provides enhanced flexibility and control (compared to the state of the art temporal encoding algorithms) within the design framework of the temporal encoding problem for fMRI, leading to better quality of spike-time data. The proposed approach, by the use of efficient brain-like encoding mechanism, opens up new possibilities in information compression, communication and pattern recognition and thus is applicable to a range of new applications.},
   author = {Neelava Sengupta and Nikola Kasabov},
   doi = {10.1016/j.ins.2017.04.017},
   issn = {00200255},
   journal = {Information Sciences},
   keywords = {Data compression,Pattern recognition,Spike-time encoding},
   month = {9},
   pages = {133-145},
   publisher = {Elsevier Inc.},
   title = {Spike-time encoding as a data compression technique for pattern recognition of temporal data},
   volume = {406-407},
   year = {2017},
}
@inproceedings{Nuntalid2011,
   abstract = {This study investigates the feasibility of Bens Spike Algorithm (BSA) to encode continuous EEG spatio-temporal data into input spike streams for a classification in a spiking neural network classifier. A novel evolving probabilistic spiking neural network reservoir (epSNNr) architecture is used for the purpose of learning and classifying the EEG signals after the BSA transformation. Experiments are conducted with EEG data measuring a cognitive state of a single individual under 4 different stimuli. A comparison is drawn between using traditional machine learning algorithms and using BSA plus epSNNr, when different probabilistic models of neurons are utilised. The comparison demonstrates that: (1) The BSA is a suitable transformation for EEG data into spike trains; (2) The performance of the epSNNr improves when a probabilistic model of a neuron is used, compared to the use of a deterministic LIF model of a neuron; (3) The classification accuracy of the EEG data in an epSNNr depends on the type of the probabilistic neuronal model used. The results suggest that an epSNNr can be optimised in terms of neuronal models used and parameters that would better match the noise and the dynamics of EEG data. Potential applications of the proposed method for BCI and medical studies are briefly discussed. © 2011 Springer-Verlag.},
   author = {Nuttapod Nuntalid and Kshitij Dhoble and Nikola Kasabov},
   doi = {10.1007/978-3-642-24955-6_54},
   isbn = {9783642249549},
   issn = {03029743},
   issue = {PART 1},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Electroencephalograms (EEG),Spatio-Temporal Patterns,Stochastic neuron models,evolving probabilistic spiking neural networks},
   pages = {451-460},
   title = {EEG classification with BSA spike encoding algorithm and evolving probabilistic spiking neural network},
   volume = {7062 LNCS},
   year = {2011},
}
@article{Shamir2006,
   abstract = {In many cortical and subcortical areas, neurons are known to modulate their average firing rate in response to certain external stimulus features. It is widely believed that information about the stimulus features is coded by a weighted average of the neural responses. Recent theoretical studies have shown that the information capacity of such a coding scheme is very limited in the presence of the experimentally observed pairwise correlations. However, central to the analysis of these studies was the assumption of a homogeneous population of neurons. Experimental findings show a considerable measure of heterogeneity in the response properties of different neurons. In this study, we investigate the effect of neuronal heterogeneity on the information capacity of a correlated population of neurons. We show that information capacity of a heterogeneous network is not limited by the correlated noise, but scales linearly with the number of cells in the population. This information cannot be extracted by the population vector readout, whose accuracy is greatly suppressed by the correlated noise. On the other hand, we show that an optimal linear readout that takes into account the neuronal heterogeneity can extract most of this information. We study analytically the nature of the dependence of the optimal linear readout weights on the neuronal diversity. We show that simple online learning can generate readout weights with the appropriate dependence on the neuronal diversity, thereby yielding efficient readout. © 2006 Massachusetts Institute of Technology.},
   author = {Maoz Shamir and Haim Sompolinsky},
   doi = {10.1162/neco.2006.18.8.1951},
   issn = {08997667},
   issue = {8},
   journal = {Neural Computation},
   pages = {1951-1986},
   pmid = {16771659},
   title = {Implications of neuronal diversity on population coding},
   volume = {18},
   year = {2006},
}
@report{Delorme2001,
   abstract = {Rank Order Coding is an alternative to conventional rate coding schemes that uses the order in which a neuron's inputs "re to encode information. In a visual system framework, we simulated the asynchronous waves of retinal spikes produced in response to natural scenes and used them to stimulate integrate-and-"re V1 neurons that implemented a standard learning rule based on spike timing. After propagating thousands of images, orientation like receptive "elds arise in these neurons despite the fact that the input neurons never "red more than once. We also analyze the biological plausibility of such a network.},
   author = {Arnaud Delorme and Laurent Perrinet and Simon J Thorpe},
   journal = {Neurocomputing 38\}},
   keywords = {Natural scenes,One spike,Orientation emergence,Rank Order Coding,Spike timing dependant plasticity},
   pages = {545},
   title = {Networks of integrate-and-fire neurons using Rank Order Coding B: Spike timing dependent plasticity and emergence of orientation selectivity},
   volume = {40},
   year = {2001},
}
@book_section{Thorpe1998,
   abstract = {The idea that neurones transmit information using a rate code is extremely entrenched in the neuroscience community. The vast majority of neurophysiological studies simply describe neural responses in terms of firing rate, and while studies using Peri-Stimulus Time Histograms (PSTHs) are fairly common, only rarely does one get to see the underlying spikes in the form of a raster display. Even rarer are studies that provide information about how spikes are generated across a population of neurones. One consequence of this strong bias is that many alternative coding schemes, and particularly those involving patterns of activity distributed across populations of neurones, have simply not been considered seriously. It is now virtually 30 years since the publication of Perkel and Bullocks' review of Neural Coding in which a whole range of candidate coding schemes were discussed1. Few of these various candidate codes have been disproved experimentally. Even today, when increasing numbers of researchers are interested in the potential of temporal coding schemes and in particular the role played by synchrony2,3, few question the underlying assumption that this synchrony is imposed on an underlying rate code.},
   author = {Simon Thorpe and Jacques Gautrais},
   doi = {10.1007/978-1-4615-4831-7_19},
   journal = {Computational Neuroscience},
   pages = {113-118},
   publisher = {Springer US},
   title = {Rank Order Coding},
   year = {1998},
}
@article{Soltic2010,
   abstract = {This paper demonstrates how knowledge can be extracted from evolving spiking neural networks with rank order population coding. Knowledge discovery is a very important feature of intelligent systems. Yet, a disproportionally small amount of research is centered on the issue of knowledge extraction from spiking neural networks which are considered to be the third generation of artificial neural networks. The lack of knowledge representation compatibility is becoming a major detriment to end users of these networks. We show that a high-level knowledge can be obtained from evolving spiking neural networks. More specifically, we propose a method for fuzzy rule extraction from an evolving spiking network with rank order population coding. The proposed method was used for knowledge discovery on two benchmark taste recognition problems where the knowledge learnt by an evolving spiking neural network was extracted in the form of zero-order Takagi-Sugeno fuzzy IF-THEN rules. © 2010 World Scientific Publishing Company.},
   author = {Snjezana Soltic and Nikola Kasabov},
   doi = {10.1142/S012906571000253X},
   issn = {01290657},
   issue = {6},
   journal = {International Journal of Neural Systems},
   keywords = {Evolving spiking neural networks,SNN,fuzzy rules,knowledge discovery,rank order population coding},
   month = {12},
   pages = {437-445},
   pmid = {21117268},
   title = {Knowledge extraction from evolving spiking neural networks with rank order population coding},
   volume = {20},
   year = {2010},
}
@article{Dean2005,
   abstract = {Mammals can hear sounds extending over a vast range of sound levels with remarkable accuracy. How auditory neurons code sound level over such a range is unclear; firing rates of individual neurons increase with sound level over only a very limited portion of the full range of hearing. We show that neurons in the auditory midbrain of the guinea pig adjust their responses to the mean, variance and more complex statistics of sound level distributions. We demonstrate that these adjustments improve the accuracy of the neural population code close to the region of most commonly occurring sound levels. This extends the range of sound levels that can be accurately encoded, fine-tuning hearing to the local acoustic environment. © 2005 Nature Publishing Group.},
   author = {Isabel Dean and Nicol S. Harper and David McAlpine},
   doi = {10.1038/nn1541},
   issn = {10976256},
   issue = {12},
   journal = {Nature Neuroscience},
   month = {12},
   pages = {1684-1689},
   pmid = {16286934},
   title = {Neural population coding of sound level adapts to stimulus statistics},
   volume = {8},
   year = {2005},
}
@inproceedings{Loiselle2006,
   abstract = {Speech recognition is very difficult in the context of noisy and corrupted speech. Most conventional techniques need huge databases to estimate speech (or noise) density probabilities to perform recognition. We discuss the potential of perceptive speech analysis and processing in combination with biologically plausible neural network processors.We illustrate the potential of such non-linear processing of speech by means of a preliminary test with recognition of French spoken digits from a small speech database.},
   author = {S. Loiselle and J. Rouat and D. Pressnitzer and S. Thorpe},
   doi = {10.1109/ijcnn.2005.1556220},
   month = {1},
   pages = {2076-2080},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {Exploration of rank order coding with spiking neural networks for speech recognition},
   year = {2006},
}
@inproceedings{Yang2014,
   abstract = {Two in-pixel encoding mechanisms to convert analog input to spike output for vision sensors are modeled and compared with the consideration of feedback delay: one is feedback and reset (FAR), and the other is feedback and subtract (FAS). MATLAB simulations of linear signal reconstruction from spike trains generated by the two encoders show that FAR in general has a lower signal-to-distortion ratio (SDR) compared to FAS due to signal loss during the reset phase and hold period, and the SDR merit of FAS increases as the quantization bit number and input signal frequency increases. A 500 μm 2 in-pixel circuit implementation of FAS using asynchronous switched capacitors in a UMC 0.18μm 1P6M process is described, and the post-layout simulation results are given to verify the FAS encoding mechanism. © 2014 IEEE.},
   author = {Minhao Yang and Shih Chii Liu and Tobi Delbruck},
   doi = {10.1109/ISCAS.2014.6865713},
   isbn = {9781479934324},
   issn = {02714310},
   journal = {Proceedings - IEEE International Symposium on Circuits and Systems},
   pages = {2632-2635},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Comparison of spike encoding schemes in asynchronous vision sensors: Modeling and design},
   year = {2014},
}
@generic{Shamir2014,
   abstract = {Population coding theory aims to provide quantitative tests for hypotheses concerning the neural code. Over the last two decades theory has focused on analyzing the ways in which various parameters that characterize neuronal responses to external stimuli affect the information content of these responses. This article reviews and provides an intuitive explanation for the major effects of noise correlations and neuronal heterogeneity, and discusses their implications for our ability to investigate the neural code. It is argued that to test neural code hypotheses further, additional constraints are required, including relating trial-to-trial variation in neuronal population responses to behavioral decisions and specifying how information is decoded by downstream networks. © 2014 Elsevier Ltd.},
   author = {Maoz Shamir},
   doi = {10.1016/j.conb.2014.01.002},
   issn = {09594388},
   journal = {Current Opinion in Neurobiology},
   month = {4},
   pages = {140-148},
   pmid = {24487341},
   title = {Emerging principles of population coding: In search for the neural code},
   volume = {25},
   year = {2014},
}
@inproceedings{george2020,
   author = {Arun M. George and Dighanchal Banerjee and Sounak Dey and Arijit Mukherjee and P. Balamurali},
   doi = {10.1109/IJCNN48605.2020.9206681},
   isbn = {978-1-7281-6926-2},
   journal = {2020 International Joint Conference on Neural Networks (IJCNN)},
   month = {7},
   publisher = {IEEE},
   title = {A Reservoir-based Convolutional Spiking Neural Network for Gesture Recognition from DVS Input},
   year = {2020},
}
@article{markram2002,
   abstract = {<p>A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.</p>},
   author = {Wolfgang Maass and Thomas Natschläger and Henry Markram},
   doi = {10.1162/089976602760407955},
   issn = {0899-7667},
   issue = {11},
   journal = {Neural Computation},
   month = {11},
   title = {Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations},
   volume = {14},
   year = {2002},
}
@article{neftci2017,
   author = {Emre O. Neftci and Charles Augustine and Somnath Paul and Georgios Detorakis},
   doi = {10.3389/fnins.2017.00324},
   issn = {1662-453X},
   journal = {Frontiers in Neuroscience},
   month = {6},
   title = {Event-Driven Random Back-Propagation: Enabling Neuromorphic Deep Learning Machines},
   volume = {11},
   year = {2017},
}
@article{mostafa2017,
   author = {Hesham Mostafa and Vishwajith Ramesh and Gert Cauwenberghs},
   doi = {10.3389/fnins.2018.00608},
   issn = {1662-453X},
   journal = {Frontiers in Neuroscience},
   month = {8},
   title = {Deep Supervised Learning Using Local Errors},
   volume = {12},
   year = {2018},
}
@article{kaiser2020,
   author = {Jacques Kaiser and Hesham Mostafa and Emre Neftci},
   doi = {10.3389/fnins.2020.00424},
   issn = {1662-453X},
   journal = {Frontiers in Neuroscience},
   month = {5},
   title = {Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE)},
   volume = {14},
   year = {2020},
}
@article{rossum2001,
   abstract = {<p>The discrimination between two spike trains is a fundamental problem for both experimentalists and the nervous system itself. We introduce a measure for the distance between two spike trains. The distance has a time constant as a parameter. Depending on this parameter, the distance interpolates between a coincidence detector and a rate difference counter. The dependence of the distance on noise is studied with an integrate-andfire model. For an intermediate range of the time constants, the distance depends linearly on the noise. This property can be used to determine the intrinsic noise of a neuron.</p>},
   author = {M. C. W. van Rossum},
   doi = {10.1162/089976601300014321},
   issn = {0899-7667},
   issue = {4},
   journal = {Neural Computation},
   month = {4},
   title = {A Novel Spike Distance},
   volume = {13},
   year = {2001},
}
@article{bellec2020,
   abstract = {Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method–called e-prop–approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence.},
   author = {Guillaume Bellec and Franz Scherr and Anand Subramoney and Elias Hajek and Darjan Salaj and Robert Legenstein and Wolfgang Maass},
   doi = {10.1038/s41467-020-17236-y},
   issn = {20411723},
   issue = {1},
   journal = {Nature Communications},
   month = {12},
   pmid = {32681001},
   publisher = {Nature Research},
   title = {A solution to the learning dilemma for recurrent networks of spiking neurons},
   volume = {11},
   year = {2020},
}
@article{sakurai1999,
   author = {Yoshio Sakurai},
   doi = {10.1016/S0149-7634(99)00017-2},
   issn = {01497634},
   issue = {6},
   journal = {Neuroscience & Biobehavioral Reviews},
   month = {10},
   title = {How do cell assemblies encode information in the brain?},
   volume = {23},
   year = {1999},
}
@article{gerstein1978,
   author = {GL Gerstein and DH Perkel and KN Subramanian},
   title = {IEEE-Neuronal-Assemblies},
   year = {1978},
}
@report{singer1990,
   abstract = {During a critical period of postnatal development of the mammalian visual cortex, synaptic connections are susceptible to use-dependent modifications. Synaptic connections strengthen if pre-and postsynaptic elements are active simultaneously and postsynaptic depolarization is sufficient to allow for the activation of A'-methyl-D-aspartate (NMDA)-receptor-gated conductances. By contrast, synaptic gain decreases if postsynaptic activation exceeds a critical threshold and presynaptic afferents are not capable of activating NMDA-receptor-dependent conductances. These processes lead to selective stabilization of connections between neuronal elements which often exhibit correlated activity and thus modify connectivity according to functional criteria. It is suggested that such experience-dependent selection of circuits serves different purposes at different levels of visual processing. At the input stage to the striate cortex it contributes to optimize the match between the representations of the two eyes. At a later stage of processing it participates in the development of selective connections between cortical columns and thereby serves to establish neuronal representations for frequently occurring constellations of features. Use-dependent changes of synaptic gain can also be induced in the mature visual cortex. These modifications follow the same rules as those occurring during early development and appear to depend on similar molecular mechanisms. However, in the adult the changes of synaptic gain do not seem to be followed by major rearrangements of connectivity. This suggests developmental alterations in mechanisms responsible for growth, removal and stabilization of synaptic connections. Actually, many of the cellular mechanisms thought to be involved in use-dependent synaptic plasticity change during development but it is still unclear which of them are responsible for the definitive stabilization of functionally confirmed pathways. Functions requiring use-dependent selection of neuronal connections Fusion and stereopsis Higher mammals and humans, which have frontally positioned eyes with overlapping visual fields, can fuse the images of the two eyes and compute from their differences the distance of objects in space. The basis for this function are J|eurons in the visual cortex which possess two receptive fields, one in each eye,},
   author = {W Singer},
   journal = {^ exp. Biol},
   keywords = {cell assemblies,development,synaptic plasticity,visual cortex},
   pages = {7},
   title = {THE FORMATION OF COOPERATIVE CELL ASSEMBLIES IN THE VISUAL CORTEX},
   volume = {153},
   year = {1990},
}
@article{nguyen2013,
   author = {Thai Nguyen},
   doi = {10.5038/2326-3652.3.1.26},
   issn = {2326-3652},
   issue = {1},
   journal = {Undergraduate Journal of Mathematical Modeling: One + Two},
   month = {5},
   title = {Total Number of Synapses in the Adult Human Neocortex},
   volume = {3},
   year = {2013},
}
@article{williams1989,
   abstract = {<p>The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.</p>},
   author = {Ronald J. Williams and David Zipser},
   doi = {10.1162/neco.1989.1.2.270},
   issn = {0899-7667},
   issue = {2},
   journal = {Neural Computation},
   month = {6},
   title = {A Learning Algorithm for Continually Running Fully Recurrent Neural Networks},
   volume = {1},
   year = {1989},
}
@article{fremaux2016,
   author = {Nicolas Frémaux and Wulfram Gerstner},
   doi = {10.3389/fncir.2015.00085},
   issn = {1662-5110},
   journal = {Frontiers in Neural Circuits},
   month = {1},
   title = {Neuromodulated Spike-Timing-Dependent Plasticity, and Theory of Three-Factor Learning Rules},
   volume = {9},
   year = {2016},
}
@article{izike2007,
   author = {E. M. Izhikevich},
   doi = {10.1093/cercor/bhl152},
   issn = {1047-3211},
   issue = {10},
   journal = {Cerebral Cortex},
   month = {10},
   title = {Solving the Distal Reward Problem through Linkage of STDP and Dopamine Signaling},
   volume = {17},
   year = {2007},
}
@article{persike2016,
   author = {Malte Persike and Günter Meinhardt},
   doi = {10.1016/j.visres.2016.07.010},
   issn = {00426989},
   journal = {Vision Research},
   month = {10},
   title = {Contour integration with corners},
   volume = {127},
   year = {2016},
}
@book_section{molnar2020,
   author = {Zoltán Molnár and Kathleen S. Rockland},
   doi = {10.1016/B978-0-12-814411-4.00005-6},
   journal = {Neural Circuit and Cognitive Development},
   publisher = {Elsevier},
   title = {Cortical columns},
   year = {2020},
}
@article{williams1989,
   abstract = {<p>The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.</p>},
   author = {Ronald J. Williams and David Zipser},
   doi = {10.1162/neco.1989.1.2.270},
   issn = {0899-7667},
   issue = {2},
   journal = {Neural Computation},
   month = {6},
   title = {A Learning Algorithm for Continually Running Fully Recurrent Neural Networks},
   volume = {1},
   year = {1989},
}
@article{izike2007,
   author = {E. M. Izhikevich},
   doi = {10.1093/cercor/bhl152},
   issn = {1047-3211},
   issue = {10},
   journal = {Cerebral Cortex},
   month = {10},
   title = {Solving the Distal Reward Problem through Linkage of STDP and Dopamine Signaling},
   volume = {17},
   year = {2007},
}
@article{fremaux2016,
   author = {Nicolas Frémaux and Wulfram Gerstner},
   doi = {10.3389/fncir.2015.00085},
   issn = {1662-5110},
   journal = {Frontiers in Neural Circuits},
   month = {1},
   title = {Neuromodulated Spike-Timing-Dependent Plasticity, and Theory of Three-Factor Learning Rules},
   volume = {9},
   year = {2016},
}

@report{Heeger2000,
   abstract = {In the cortex, the timing of successive action potentials is highly irregular. The interpretation of this irregularity has led to two divergent views of cortical organization. On the one hand, the irregularity might arise from stochastic forces. If so, the irregular interspike interval reflects a random process and implies that an instantaneous estimate of the spike rate can be obtained by averaging the pooled responses of many individual neurons. In keeping with this theory, one would expect that the precise timing of individual spikes conveys little information. Alternatively, the irregular ISI may result from precise coincidences of presynaptic events. In this scenario, it is postulated that the timing of spikes, their intervals and patterns can convey information. According to this view, the irregularity of the ISI reflects a rich bandwidth for information transfer. In this handout, we take the former point of view, that the irregular interspike interval reflects a random process. We assume that the generation of each spike depends only on an underlying continuous/analog driving signal, Ö´Øµ, that we will refer to as the instantaneous firing rate. It follows that the generation of each spike is independent of all the other spikes, hence we refer to this as the independent spike hypothesis. If the independent spike hypothesis were true, then the spike train would be completely described a particular kind of random process called a Poisson process. Note that even though a Poisson spike train is generated by a random process, some stimuli could still evoke spikes very reliably by forcing the instantaneous firing rate to be very large at particular moments in time so that the probability of firing would then be arbitrarily close to 1. Certain features of neuronal firing, however, violate the independent spike hypothesis. Following the generation of an action potential, there is an interval of time known as the absolute refractory period during which the neuron can not fire another spike. For a longer interval known as the relative refractory period, the likelihood of a spike being fired is much reduced. Bursting is another non-Poisson feature of neuronal spiking. Some neurons fire action potentials is clusters or bursts, and these tend to be poorly described a purely Poisson spike-generation process. Below, I present ways of extending the Poisson model to account for refractoriness and bursting.},
   author = {David Heeger},
   title = {Poisson Model of Spike Generation},
   year = {2000},
}

@article{ito2008,
   author = {Hiroshi T Ito},
   doi = {10.3389/neuro.01.027.2008},
   issn = {16624548},
   issue = {2},
   journal = {frontiers in Neuroscience},
   month = {12},
   title = {Frequency-dependent signal transmission and modulation by neuromodulators},
   volume = {2},
   year = {2008},
}
