@article{Salzman1992,
   author = {CD Salzman and CM Murasugi and KH Britten and WT Newsome},
   doi = {10.1523/JNEUROSCI.12-06-02331.1992},
   issn = {0270-6474},
   issue = {6},
   journal = {The Journal of Neuroscience},
   month = {6},
   title = {Microstimulation in visual area MT: effects on direction discrimination performance},
   volume = {12},
   year = {1992},
}
@article{Tovee1993,
   abstract = {<p>1. The possibility of temporal encoding in the spike trains of single neurons recorded in the temporal lobe visual cortical areas of rhesus macaques was analyzed with the use of principal component and information theory analyses of smoothed spike trains. The neurons analyzed had responses selective for faces. 2. Provided that a correction was applied to earlier methods of principal component analysis used for neuronal spike trains, it was shown that the first principal component provides by a great extent the most information, with the second and third adding only small proportions (on average 18.8 and 8.4%, respectively). 3. It was shown that the magnitude of the second and higher principal components is even smaller if the spike train analysis is started after the onset of the neuronal response, instead of before the neuronal response has started. This suggests that variations in response latency are at least a part of what is reflected by the second and higher principal components. 4. The first principal component was correlated with the mean firing rate of the neurons. The second and higher principal components reflected at least partly the onset properties of the neuronal responses, such as response latency differences between the stimuli. 5. A considerable proportion of the information available from principal components 1-3 is available in the firing rate of the neuron. 6. Periods of the firing rate of as little as 50 or even 20 ms are sufficient to give a reasonable estimate of the firing rate of the neuron. 7. Information theory analysis showed that in short epochs (e.g., 50 ms) the information available from the firing rate can be as high, on average, as 84.4% of that available from the firing rate calculated over 400 ms, and 52.0% of that available from principal components 1-3 in the 400-ms period. It was also found that 44.0% of the information calculated from the first three principal components is available in the firing rates calculated over epochs as short as 20 ms. 8. More information was available near the start of the neuronal response, and the information available from short epochs became less later in the neuronal response. 9. Taken together, these analyses provide evidence that a short period of firing taken close to the start of the neuronal response provides a reasonable proportion of the total information that would be available if a long period of neuronal firing (e.g., 400 ms) were utilized to extract it, even if temporal encoding were used.(ABSTRACT TRUNCATED AT 400 WORDS)</p>},
   author = {M. J. Tovee and E. T. Rolls and A. Treves and R. P. Bellis},
   doi = {10.1152/jn.1993.70.2.640},
   issn = {0022-3077},
   issue = {2},
   journal = {Journal of Neurophysiology},
   month = {8},
   title = {Information encoding and the responses of single neurons in the primate temporal visual cortex},
   volume = {70},
   year = {1993},
}
@article{Bair1996,
   abstract = {<p>How reliably do action potentials in cortical neurons encode information about a visual stimulus? Most physiological studies do not weigh the occurrences of particular action potentials as significant but treat them only as reflections of average neuronal excitation. We report that single neurons recorded in a previous study by Newsome et al. (1989; see also Britten et al. 1992) from cortical area MT in the behaving monkey respond to dynamic and unpredictable motion stimuli with a markedly reproducible temporal modulation that is precise to a few milliseconds. This temporal modulation is stimulus dependent, being present for highly dynamic random motion but absent when the stimulus translates rigidly.</p>},
   author = {Wyeth Bair and Christof Koch},
   doi = {10.1162/neco.1996.8.6.1185},
   issn = {0899-7667},
   issue = {6},
   journal = {Neural Computation},
   month = {8},
   title = {Temporal Precision of Spike Trains in Extrastriate Cortex of the Behaving Macaque Monkey},
   volume = {8},
   year = {1996},
}
@article{,
   author = {Giedrius T. Burac̆as and Anthony M. Zador and Michael R. DeWeese and Thomas D. Albright},
   doi = {10.1016/S0896-6273(00)80477-8},
   issn = {08966273},
   issue = {5},
   journal = {Neuron},
   month = {5},
   title = {Efficient Discrimination of Temporal Patterns by Motion-Sensitive Neurons in Primate Visual Cortex},
   volume = {20},
   year = {1998},
}
@report{Koch1994,
   author = {Christof Koch and Joel L Davis and A Bradford Book and Patricia S Churchland and V S Ramachandran and Terrence J Sejnowski},
   title = {Large-Scale Neuronal Theories of the Brain edited by 2 A Critique of Pure Vision'},
   year = {1994},
}
@article{Iacaruso2017,
   abstract = {How a sensory stimulus is processed and perceived depends on the surrounding sensory scene. In the visual cortex, contextual signals can be conveyed by an extensive network of intra- and inter-areal excitatory connections that link neurons representing stimulus features separated in visual space. However, the connectional logic of visual contextual inputs remains unknown; it is not clear what information individual neurons receive from different parts of the visual field, nor how this input relates to the visual features that a neuron encodes, defined by its spatial receptive field. Here we determine the organization of excitatory synaptic inputs responding to different locations in the visual scene by mapping spatial receptive fields in dendritic spines of mouse visual cortex neurons using two-photon calcium imaging. We find that neurons receive functionally diverse inputs from extended regions of visual space. Inputs representing similar visual features from the same location in visual space are more likely to cluster on neighbouring spines. Inputs from visual field regions beyond the receptive field of the postsynaptic neuron often synapse on higher-order dendritic branches. These putative long-range inputs are more frequent and more likely to share the preference for oriented edges with the postsynaptic neuron when the receptive field of the input is spatially displaced along the axis of the receptive field orientation of the postsynaptic neuron. Therefore, the connectivity between neurons with displaced receptive fields obeys a specific rule, whereby they connect preferentially when their receptive fields are co-oriented and co-axially aligned. This organization of synaptic connectivity is ideally suited for the amplification of elongated edges, which are enriched in the visual environment, and thus provides a potential substrate for contour integration and object grouping.},
   author = {M. Florencia Iacaruso and Ioana T. Gasler and Sonja B. Hofer},
   doi = {10.1038/nature23019},
   issn = {14764687},
   issue = {7664},
   journal = {Nature},
   month = {7},
   pages = {449-452},
   pmid = {28700575},
   publisher = {Nature Publishing Group},
   title = {Synaptic organization of visual space in primary visual cortex},
   volume = {547},
   year = {2017},
}
@generic{London2005,
   abstract = {One of the central questions in neuroscience is how particular tasks, or computations, are implemented by neural networks to generate behavior. The prevailing view has been that information processing in neural networks results primarily from the properties of synapses and the connectivity of neurons within the network, with the intrinsic excitability of single neurons playing a lesser role. As a consequence, the contribution of single neurons to computation in the brain has long been underestimated. Here we review recent work showing that neuronal dendrites exhibit a range of linear and nonlinear mechanisms that allow them to implement elementary computations. We discuss why these dendritic properties may be essential for the computations performed by the neuron and the network and provide theoretical and experimental examples to support this view. Copyright © 2005 by Annual Reviews. All rights reserved.},
   author = {Michael London and Michael Häusser},
   doi = {10.1146/annurev.neuro.28.061604.135703},
   issn = {0147006X},
   journal = {Annual Review of Neuroscience},
   keywords = {Coding,Dendrites,Ion channels,Spikes,Synaptic integration},
   pages = {503-532},
   pmid = {16033324},
   title = {Dendritic computation},
   volume = {28},
   year = {2005},
}
@article{Fischbach1972,
   author = {Gerald D. Fischbach},
   doi = {10.1016/0012-1606(72)90023-1},
   issn = {00121606},
   issue = {2},
   journal = {Developmental Biology},
   month = {6},
   title = {Synapse formation between dissociated nerve and muscle cells in low density cell cultures},
   volume = {28},
   year = {1972},
}
@article{Furshpan1959,
   author = {E. J. Furshpan and D. D. Potter},
   doi = {10.1113/jphysiol.1959.sp006143},
   issn = {00223751},
   issue = {2},
   journal = {The Journal of Physiology},
   month = {3},
   title = {Transmission at the giant motor synapses of the crayfish},
   volume = {145},
   year = {1959},
}
@book{Gerstner2014,
   author = {Wulfram Gerstner and Werner M. Kistler and Richard Naud and Liam Paninski},
   city = {Cambridge},
   doi = {10.1017/CBO9781107447615},
   isbn = {9781107447615},
   publisher = {Cambridge University Press},
   title = {Neuronal Dynamics},
   year = {2014},
}
@article{Tanimizu2018,
   abstract = {Object recognition memory allows discrimination of familiar and novel objects. Previous studies have shown the importance of several brain regions for object recognition memories; however, the mechanisms underlying the consolidation of object recognition (OR) memory at the anatomic level remain unknown. Here, we analyzed the brain network for the generation of OR memory in mice by measuring the expression of the immediate-early gene c-fos. We found that c-fos expression was induced in the hippocampus (CA1 and CA3 regions), insular cortex (IC), perirhinal cortex (PRh), and medial prefrontal cortex (mPFC) when OR memory was generated, suggesting that gene expression in these brain regions contributes to the formation of OR memory. Consistently, inhibition of protein synthesis in the mPFC blocked the formation of long-term OR memory. Importantly, network analyses suggested that the hippocampus, IC, PRh and mPFC show increased connectivity with other brain regions when OR memory is formed. Thus, we suggest that a brain network composed of the hippocampus, IC, PRh, and mPFC is required for the generation of OR memory by connecting with other brain regions.},
   author = {Toshiyuki Tanimizu and Kyohei Kono and Satoshi Kida},
   doi = {10.1016/j.brainresbull.2017.05.017},
   issn = {18732747},
   journal = {Brain Research Bulletin},
   keywords = {Brain network,Consolidation,Object recognition memory},
   month = {7},
   pages = {27-34},
   pmid = {28587862},
   publisher = {Elsevier Inc.},
   title = {Brain networks activated to form object recognition memory},
   volume = {141},
   year = {2018},
}
@article{,
   abstract = {Many cognitive and behavioral tasks—such as interval timing, spatial navigation, motor control, and speech—require the execution of precisely-timed sequences of neural activation that cannot be fully explained by a succession of external stimuli. We show how repeatable and reliable patterns of spatiotemporal activity can be generated in chaotic and noisy spiking recurrent neural networks. We propose a general solution for networks to autonomously produce rich patterns of activity by providing a multi-periodic oscillatory signal as input. We show that the model accurately learns a variety of tasks, including speech generation, motor control, and spatial navigation. Further, the model performs temporal rescaling of natural spoken words and exhibits sequential neural activity commonly found in experimental data involving temporal processing. In the context of spatial navigation, the model learns and replays compressed sequences of place cells and captures features of neural activity such as the emergence of ripples and theta phase precession. Together, our findings suggest that combining oscillatory neuronal inputs with different frequencies provides a key mechanism to generate precisely timed sequences of activity in recurrent circuits of the brain.},
   author = {Philippe Vincent-Lamarre and Matias Calderini and Jean Philippe Thivierge},
   doi = {10.3389/fncom.2020.00078},
   issn = {16625188},
   journal = {Frontiers in Computational Neuroscience},
   keywords = {balanced networks,neural oscillations,recurrent neural networks,spiking neural networks,temporal processing},
   month = {9},
   publisher = {Frontiers Media S.A.},
   title = {Learning Long Temporal Sequences in Spiking Networks by Multiplexing Neural Oscillations},
   volume = {14},
   year = {2020},
}
@article{,
   abstract = {The contrast sensitivity function (CSF), how sensitivity varies with the frequency of the stimulus, is a fundamental assessment of visual performance. The CSF is generally assumed to be determined by low-level sensory processes. However, the spatial sensitivities of neurons in the early visual pathways, as measured in experiments with immobilized eyes, diverge from psychophysical CSF measurements in primates. Under natural viewing conditions, as in typical psychophysical measurements, humans continually move their eyes even when looking at a fixed point. Here, we show that the resulting transformation of the spatial scene into temporal modulations on the retina constitutes a processing stage that reconciles human CSF and the response characteristics of retinal ganglion cells under a broad range of conditions. Our findings suggest a fundamental integration between perception and action: eye movements work synergistically with the spatio-temporal sensitivities of retinal neurons to encode spatial information.},
   author = {Antonino Casile and Jonathan D Victor and Michele Rucci},
   doi = {10.7554/eLife.40924.001},
   note = {read temporal coding of visual space 2018first},
   title = {Contrast sensitivity reveals an oculomotor strategy for temporally encoding space},
   url = {https://doi.org/10.7554/eLife.40924.001},
}
@article{Ahissar2012,
   abstract = {During natural viewing, the eyes are never still. Even during fixation, miniature movementsof the eyes move the retinal image across tens of foveal photoreceptors. Most theories of vision implicitly assume that the visual system ignores these movements and somehow overcomes the resulting smearing. However, evidence has accumulated to indicate that fixational eye movements cannot be ignored by the visual system if fine spatial details are to be resolved. We argue that the only way the visual system can achieve its high resolution given its fixational movements is by seeing via these movements. Seeing via eye movements also eliminates the instability of the image, which would be induced by them otherwise. Here we present a hypothesis for vision, in which coarse details are spatially-encoded in gazerelated coordinates, and fine spatial details are temporally-encoded in relative retinal coordinates. The temporal encoding presented here achieves its highest resolution by encoding along the elongated axes of simple-cell receptive fields and not across these axes as suggested by spatial models of vision. According to our hypothesis, fine details of shape are encoded by inter-receptor temporal phases, texture by instantaneous intra-burst rates of individual receptors, and motion by inter-burst temporal frequencies. We further describe the ability of the visual system to readout the encoded information and recode it internally. We show how reading out of retinal signals can be facilitated by neuronal phase-locked loops (NPLLs), which lock to the retinal jitter; this locking enables recoding of motion information and temporal framing of shape and texture processing. A possible implementation of this locking-and-recoding process by specific thalamocortical loops is suggested. Overall it is suggested that high-acuity vision is based primarily on temporal mechanisms of the sort presented here and low-acuity vision is based primarily on spatial mechanisms. © 2012 Ahissar and Arieli.},
   author = {Ehud Ahissar and Amos Arieli},
   doi = {10.3389/fncom.2012.00089},
   issn = {16625188},
   issue = {OCTOBER 2012},
   journal = {Frontiers in Computational Neuroscience},
   keywords = {Active vision,Feedback,Fixational eye movements,Neural coding,Neuronal phase-locked loop,Simple cells,Temporal coding,Thalamocortical loop},
   month = {10},
   note = {difficut as fuck<br/><br/>},
   title = {Seeing via miniature eye movements: A dynamic hypothesis for vision},
   year = {2012},
}
@article{Callaway2004,
   abstract = {Visual cortical circuits are organized at multiple levels of complexity including cortical areas, layers and columns, and specific cell types within these modules. Making sense of the functions of these circuits from anatomical observations requires linking these circuits to function at each of these levels of complexity. Observations of these relationships have become increasingly sophisticated over the last several decades, beginning with correlations between the connectivities and functions of various visual cortical areas and progressing toward cell type-specificity. These studies have informed current views about the functional interactions between cortical areas and modules and the mechanisms by which fine scale microcircuits influence interactions at more coarse levels of organization. © 2004 Elsevier Ltd. All rights reserved.},
   author = {Edward M. Callaway},
   doi = {10.1016/j.neunet.2004.04.004},
   issn = {08936080},
   issue = {5-6},
   journal = {Neural Networks},
   keywords = {Driving,Excitation,Gating,Inhibition,Local circuit,Modulatory},
   pages = {625-632},
   pmid = {15288888},
   publisher = {Elsevier Ltd},
   title = {Feedforward, feedback and inhibitory connections in primate visual cortex},
   volume = {17},
   year = {2004},
}
@generic{Salinas2001,
   abstract = {For years we have known that cortical neurons collectively have synchronous or oscillatory patterns of activity, the frequencies and temporal dynamics of which are associated with distinct behavioural states. Although the function of these oscillations has remained obscure, recent experimental and theoretical results indicate that correlated fluctuations might be important for cortical processes, such as attention, that control the flow of information in the brain.},
   author = {Emilio Salinas and Terrence J. Sejnowski},
   doi = {10.1038/35086012},
   issn = {14710048},
   issue = {8},
   journal = {Nature Reviews Neuroscience},
   month = {8},
   pages = {539-550},
   pmid = {11483997},
   title = {Correlated neuronal activity and the flow of neural information},
   volume = {2},
   year = {2001},
}
@article{Zhou2020,
   abstract = {Memory is thought to be encoded by sparsely distributed neuronal ensembles in memory-related regions. However, it is unclear how memory-eligible neurons react during learning to encode trace fear memory and how they retrieve a memory. We implemented a fiber-optic confocal fluorescence endomicroscope to directly visualize calcium dynamics of hippocampal CA1 neurons in freely behaving mice subjected to trace fear conditioning. Here we report that the overall activity levels of CA1 neurons showed a right-skewed lognormal distribution, with a small portion of highly active neurons (termed Primed Neurons) filling the long-tail. Repetitive training induced Primed Neurons to shift from random activity to well-tuned synchronization. The emergence of activity synchronization coincided with the appearance of mouse freezing behaviors. In recall, a partial synchronization among the same subset of Primed Neurons was induced from random dynamics, which also coincided with mouse freezing behaviors. Additionally, training-induced synchronization facilitated robust calcium entry into Primed Neurons. In contrast, most CA1 neurons did not respond to tone and foot shock throughout the training and recall cycles. In conclusion, Primed Neurons are preferably recruited to encode trace fear memory and induction of activity synchronization among Primed Neurons out of random dynamics is critical for trace memory formation and retrieval.},
   author = {Yuxin Zhou and Liyan Qiu and Haiying Wang and Xuanmao Chen},
   doi = {10.1096/fj.201902274R},
   issn = {15306860},
   issue = {3},
   journal = {FASEB Journal},
   keywords = {GCaMP6,calcium imaging in freely behaving animals,declarative memory,memory consolidation,right-skewed lognormal distribution,synaptic plasticity},
   month = {3},
   pages = {3658-3676},
   pmid = {31944374},
   publisher = {John Wiley and Sons Inc.},
   title = {Induction of activity synchronization among primed hippocampal neurons out of random dynamics is key for trace memory formation and retrieval},
   volume = {34},
   url = {https://pubmed.ncbi.nlm.nih.gov/31944374/},
   year = {2020},
}
@report{Buonomano1998,
   abstract = {It has been clear for almost two decades that cortical representations in adult animals are not fixed entities, but rather, are dynamic and are continuously modified by experience. The cortex can preferentially allocate area to represent the particular peripheral input sources that are proportionally most used. Alterations in cortical representations appear to underlie learning tasks dependent on the use of the behaviorally important peripheral inputs that they represent. The rules governing this cortical representational plasticity following manipulations of inputs, including learning, are increasingly well understood. In parallel with developments in the field of cortical map plasticity, studies of synaptic plasticity have characterized specific elementary forms of plasticity, including associative long-term potentiation and long-term depression of exci-tatory postsynaptic potentials. Investigators have made many important strides toward understanding the molecular underpinnings of these fundamental plasticity processes and toward defining the learning rules that govern their induction. The fields of cortical synaptic plasticity and cortical map plasticity have been implicitly linked by the hypothesis that synaptic plasticity underlies cortical map reorganization. Recent experimental and theoretical work has provided increasingly stronger support for this hypothesis. The goal of the current paper is to review the fields of both synaptic and cortical map plasticity with an emphasis on the work that attempts to unite both fields. A second objective is to highlight the gaps in our understanding of synaptic and cellular mechanisms underlying cortical representational plasticity. 149},
   author = {Dean V Buonomano and Michael M Merzenich},
   journal = {Annu. Rev. Neurosci},
   keywords = {Hebbian,LTD,LTP,cortex,topographic},
   pages = {149-86},
   title = {CORTICAL PLASTICITY: From Synapses to Maps},
   volume = {21},
   year = {1998},
}
@article{Kaschube2010,
   abstract = {The brain's visual cortex processes information concerning form, pattern, and motion within functional maps that reflect the layout of neuronal circuits. We analyzed functional maps of orientation preference in the ferret, tree shrew, and galago - three species separated since the basal radiation of placental mammals more than 65 million years ago - and found a common organizing principle. A symmetry-based class of models for the self-organization of cortical networks predicts all essential features of the layout of these neuronal circuits, but only if suppressive long-range interactions dominate development. We show mathematically that orientation-selective long-range connectivity can mediate the required interactions. Our results suggest that self-organization has canalized the evolution of the neuronal circuitry underlying orientation preference maps into a single common design.},
   author = {Matthias Kaschube and Michael Schnabel and Siegrid Löwel and David M. Coppola and Leonard E. White and Fred Wolf},
   doi = {10.1126/science.1194869},
   issn = {00368075},
   issue = {6007},
   journal = {Science},
   month = {11},
   pages = {1113-1116},
   pmid = {21051599},
   title = {Universality in the evolution of orientation columns in the visual cortex},
   volume = {330},
   year = {2010},
}
@article{Braitenberg1979,
   abstract = {The optimal direction of lines in the visual field to which neurons in the visual cortex respond changes in a regular way when the recording electrode progresses tangentially through the cortex (Hubel and Wiesel, 1962). It is possible to reconstruct the field of orientations from long, sometimes multiple parallel penetrations (Hubel and Wiesel, 1974; Albus, 1975) by assuming that the orientations are arranged radially around centers. A method is developed which makes it possible to define uniquely the position of the centers in the vicinity of the electrode track. They turn out to be spaced at distances of about 0.5 mm and may be tentatively identified with the positions of the giant cells of Meynert. © 1979 Springer-Verlag.},
   author = {V. Braitenberg and C. Braitenberg},
   doi = {10.1007/BF00337296},
   issn = {03401200},
   issue = {3},
   journal = {Biological Cybernetics},
   month = {8},
   pages = {179-186},
   pmid = {497262},
   publisher = {Springer-Verlag},
   title = {Geometry of orientation columns in the visual cortex},
   volume = {33},
   url = {https://www.researchgate.net/publication/22645543_Geometry_of_orientation_columns_in_the_visual_cortex},
   year = {1979},
}
@report{Gray1989,
   abstract = {In areas 17and 18 of the cat visual cortex the firing probability of neurons, in response to the presentation of optimally aligned light bars within their receptive field, oscillates with a peak frequency near 40 Hz. The neuronal firing pattern is tightly correlated with the phase and amplitude of an oscillatory local field potential recorded through the same electrode. The amplitude of the local field-potential oscillations are maximal in response to stimuli that match the orientation and direction preference of the local cluster of neurons. Single and multiunit recordings from the dorsal lateral geniculate nucleus of the thalamus showed no evidence of oscillations of the neuronal firing probability in the range of 20-70 Hz. The results demonstrate that local neuronal populations in the visual cortex engage in stimulus-specific synchronous oscillations resulting from an intracortical mechanism. The oscilla-tory responses may provide a general mechanism by which activity patterns in spatially separate regions of the cortex are temporally coordinated. The mechanism by which populations of neurons in the cerebral cortex temporally coordinate their activity patterns in response to specific sensory stimuli constitutes a basic unresolved question in sensory physiology. In the vertebrate olfactory system the spatiotemporal coordination of neuronal activity is achieved by the synchronization of oscillatory responses having a frequency in the range of 40-80 Hz (1-5). Evidence suggesting that a similar mechanism for the synchronization of activity may operate in the neocortex has come from field potential recordings in awake animals. It has been demonstrated that oscillatory activity in the high beta-frequency range (20-50 Hz) occurs in sensory areas of the neocortex when the animals direct their attention to meaningful stimuli (6-11). Previously we have discovered, from recordings in area 17 of awake kittens, that neuronal responses recorded during periods of behavioral attention exhibit a rhythmic firing pattern that is tightly correlated with an oscillatory local field potential (LFP) having a frequency near 40 Hz (12). Thus, we sought to determine whether the oscillatory responses could also be recorded under varying conditions of anesthesia that would permit a more quantitative analysis of both their stimulus specificity as well as their temporal properties. Here, we extend our previous observations (13) and report that local groups of neurons, within functional columns ofthe visual cortex, engage in stimulus-specific oscillatory responses having a frequency near 40 Hz. This periodic neuronal activity is tightly correlated to the simultaneously recorded LFP, which in the majority of recordings has a similar orientation and direction preference as the local cluster of neurons. No comparable oscillations of firing probability were found for the thalamic input to visual cortex, indicating that the generation of oscillatory responses is a cortical phenomenon. The results suggest the hypothesis that the temporal pattern of the oscillatory response is used to synchronize the activity of neuronal populations in spatially separate regions of the cortex. MATERIALS AND METHODS Experiments were performed on a total of 15 adult cats and 12 kittens 4-6 weeks of age. The experimental procedures for preparation of the animals, the presentation of visual stimuli, and the recording of neuronal responses in the visual cortex have been described (14). For surgery anesthesia was induced by injection of a short-acting anesthetic (ketamine at 15 mg/kg or hexobarbital at 15 mg/kg). During recording, anesthesia was maintained with a mixture of 30% 02/70% N20, supplemented by 0.1-0.3% halothane or hexobarbital (5 mg/kg). Recordings usually began 3-5 hr after the initial anesthesia to insure that the effects of ketamine, when used, had worn off. The wound edges were infiltrated with lido-caine. Muscle paralysis was achieved by a continuous i.v. infusion of hexocarbacholinbromide (Imbretil). Multiunit activity (MUA) and LFPs were recorded from one to five 25-I&m diameter Teflon-coated platinum-iridium electrodes whose tips were etched to a point and coated with platinum black (15). The LFP and MUA were differentially amplified (5000-20,000 and 5000, respectively) and filtered (1-100 Hz and 1-3 kHz, respectively). The output of the MUA amplifier was fed through a threshold detector to isolate units, on the average, at twice the noise level, and the LFP and Schmitt trigger output were digitized on separate channels at a rate of 1 kHz each. The receptive field properties of the MUA recorded from each electrode were assessed with light stimuli projected on a tangent screen located 1.12 m in front of the cat's eye plane. All stimulus trials were 10 sec in duration, and trial sets were composed of 10 trials to each stimulus. On each trial the light stimulus was on and moving over the receptive field first in one direction (first through fourth sec) and then in the other direction (sixth through ninth sec). For histological verification of the electrode tip locations DC currents (5-10 1LA for 20 sec) were applied to the electrodes at the end of measurements , and the lesion locations were retrieved in Nissl-stained sections of the perfused fixed brains. A sample of the data, to be used for quantitative analysis, was taken from 30 recording sites in areas 17 and 18 of 14 animals. In four of the adult cats recordings of single and multiunit activity were also made in the dorsal lateral gen-iculate nucleus of the thalamus. In 25 recordings from area 17 a quantitative assay of orientation selectivity was performed at angular intervals of 22.50. Four separate sets of calculations were performed on the data to examine the temporal properties ofthe MUA and LFP Abbreviations: LFP, local field potential; MUA, multiunit activity; ACF, autocorrelation function. 1698 The publication costs of this article were defrayed in part by page charge payment. This article must therefore be hereby marked "advertisement" in accordance with 18 U.S.C. §1734 solely to indicate this fact.},
   author = {Charles M Gray and Wolf Singer},
   journal = {Proc. Nati. Acad. Sci. USA},
   pages = {1698-1702},
   title = {Stimulus-specific neuronal oscillations in orientation columns of cat visual cortex (thalamus/area 17)},
   volume = {86},
   year = {1989},
}
@article{,
   author = {Simon Thorpe and  Denis Fize and  Catherine Marlot},
   journal = {NATURE},
   month = {6},
   note = {Not needed, maybe just for the number<br/>we need it to prove that firing rate is too slow for fast reaction times[George]<br/><br/>},
   pages = {520-522},
   title = {Speed of processing in the human visual system},
   volume = {381},
   year = {1996},
}
@article{Jacobs2007,
   abstract = {A growing body of animal research suggests that neurons represent information not only in terms of their firing rates but also by varying the timing of spikes relative to neuronal oscillations. Although researchers have argued that this temporal coding is critical in human memory and perception, no supporting data from humans have been reported. This study provides the first analysis of the temporal relationship between brain oscillations and single-neuron activity in humans. Recording from 1924 neurons, we find that neuronal activity in various brain regions increases at specific phases of brain oscillations. Neurons in widespread brain regions were phase locked to oscillations in the theta- (4-8 Hz) andgamma-(30-90 Hz) frequency bands. In hippocampus, phase locking was prevalent in the delta-(1-4 Hz) and gamma-frequency bands. Individual neurons were phase locked to various phases of theta and delta oscillations, but they only were active at the trough of gamma oscillations. These findings provide support for the temporal-coding hypothesis in humans. Specifically, they indicate that theta and delta oscillations facilitate phase coding and that gamma oscillations help to decode combinations of simultaneously active neurons. Copyright © 2007 Society for Neuroscience.},
   author = {Joshua Jacobs and Michael J. Kahana and Arne D. Ekstrom and Itzhak Fried},
   doi = {10.1523/JNEUROSCI.4636-06.2007},
   issn = {02706474},
   issue = {14},
   journal = {Journal of Neuroscience},
   keywords = {Gamma,Intracranial EEG,Local field potential,Navigation,Phase locking,Theta},
   month = {4},
   pages = {3839-3844},
   pmid = {17409248},
   title = {Brain oscillations control timing of single-neuron activity in humans},
   volume = {27},
   year = {2007},
}
@report{,
   abstract = {The human brain spontaneously generates neural oscillations with a large variability in frequency, amplitude, duration, and recurrence. Little, however, is known about the long-term spa-tiotemporal structure of the complex patterns of ongoing activity. A central unresolved issue is whether fluctuations in oscil-latory activity reflect a memory of the dynamics of the system for more than a few seconds. We investigated the temporal correlations of network oscillations in the normal human brain at time scales ranging from a few seconds to several minutes. Ongoing activity during eyes-open and eyes-closed conditions was recorded with simultaneous magnetoencephalogra-phy and electroencephalography. Here we show that amplitude fluctuations of 10 and 20 Hz oscillations are correlated over thousands of oscillation cycles. Our analyses also indicated that these amplitude fluctuations obey power-law scaling behavior. The scaling exponents were highly invariant across subjects. We propose that the large variability, the long-range correlations, and the power-law scaling behavior of spontaneous oscillations find a unifying explanation within the theory of self-organized criticality, which offers a general mechanism for the emergence of correlations and complex dynamics in stochastic multiunit systems. The demonstrated scaling laws pose novel quantitative constraints on computational models of network oscillations. We argue that critical-state dynamics of spontaneous oscillations may lend neural networks capable of quick reorganization during processing demands.},
   author = {Klaus Linkenkaer-Hansen and Vadim V Nikouline and J Matias Palva and Risto J Ilmoniemi},
   keywords = {complexity,correlations,large-scale dynamics,scaling behavior,self-organized criticality,spontaneous oscillations,temporal properties},
   title = {Long-Range Temporal Correlations and Scaling Behavior in Human Brain Oscillations},
   url = {http://paos.colorado.edu/research/wavelets},
   year = {2001},
}
@generic{,
   abstract = {Oscillatory fluctuations of local field potentials (LFPs) in the theta (4-8. Hz) and gamma (25-140. Hz) band are held to play a mechanistic role in various aspects of memory including the representation and off-line maintenance of events and sequences of events, the assessment of novelty, the induction of plasticity during encoding, as well as the consolidation and the retrieval of stored memories. Recent findings indicate that theta and gamma related mechanisms identified in rodent studies have significant parallels in the neurophysiology of human and non-human primate memory. This correspondence between species opens new perspectives for a mechanistic investigation of human memory function. © 2010 Elsevier Ltd.},
   author = {Emrah Düzel and Will D. Penny and Neil Burgess},
   doi = {10.1016/j.conb.2010.01.004},
   issn = {09594388},
   issue = {2},
   journal = {Current Opinion in Neurobiology},
   pages = {143-149},
   pmid = {20181475},
   publisher = {Elsevier Ltd},
   title = {Brain oscillations and memory},
   volume = {20},
   url = {https://www.sciencedirect.com/science/article/pii/S095943881000005X},
   year = {2010},
}
@report{,
   abstract = {Gamma oscillations, now widely regarded as functionally relevant signals of the brain, illustrate that the concept of event-related oscillations bridges the gap between single neurons and neural assemblies. Taking this concept further, we review experiments showing that oscillatory phenomena such as alpha, theta, or delta responses to events are strongly interwoven with sensory and cognitive functions. This review argues that selecti¨ely distributed delta, theta, alpha, and gamma oscillatory systems act as resonant communication networks through large populations of neurons. Thus, oscillatory processes might play a major role in relation with memory and integrati¨e functions. A new 'neurons-brain' doctrine is also proposed to extend the neuron doctrine of Sherrington.},
   author = {E Bas¸arbas¸ar and C Bas¸arbas¸ar-Eroglu and S Karakas¸b and Karakas¸ Karakas¸b and M Schurmann},
   journal = {International Journal of Psychophysiology},
   keywords = {Brain oscillations delta, theta, alpha, gamma,Cognitive processing,Distributed networks,Event-related potentials,Evoked potentials,Memory,Sensory process-ing},
   note = {MEXRI 19 SEL GT META LEEI GI WORKING MEMORY<br/><br/>},
   pages = {95124},
   title = {Brain oscillations in perception and memory},
   volume = {35},
   url = {https://www.sciencedirect.com/science/article/pii/S0167876099000471},
   year = {2000},
}
@article{Watts2005,
   abstract = {The cerebral cortex is pivotal in information processing and higher brain function and its laminar structure of six distinct layers, each in receipt of a different constellation of inputs, makes it important to identify connectivity patterns and distinctions between excitatory and inhibitory pathways. The 'feedforward' projections from layer 4-3 and from 3-5 target pyramidal cells and to lesser degrees interneurones. 'Feedback' projections from layer 5-3 and from 3-4, on the other hand, mainly target interneurones. Understanding the microcircuitry may give some insight into the computation and information processing performed in this brain region. © The Physiological Society 2004.},
   author = {Joanne Watts and Alex M. Thomson},
   doi = {10.1113/jphysiol.2004.076984},
   issn = {00223751},
   issue = {1},
   journal = {Journal of Physiology},
   month = {1},
   pages = {89-97},
   pmid = {15539397},
   title = {Excitatory and inhibitory connections show selectivity in the neocortex},
   volume = {562},
   url = {https://physoc.onlinelibrary.wiley.com/doi/full/10.1113/jphysiol.2004.076984},
   year = {2005},
}
@article{,
   abstract = {Microcircuits in different brain areas share similar architectural and biophysical properties with compact motor networks known as central pattern generators (CPGs). Consequently, CPGs have been suggested as valuable biological models for understanding of microcircuit dynamics and particularly, their synchronization. We use a well known compact motor network, the lobster pyloric CPG to study principles of intercircuit synchronization. We couple separate pyloric circuits obtained from two animals via artificial synapses and observe how their synchronization depends on the topology and kinetic parameters of the computer-generated synapses. Stable in-phase synchronization appears when electrically coupling the pacemaker groups of the two networks, but reciprocal inhibitory connections produce more robust and regular cooperative activity. Contralateral inhibitory connections offer effective synchronization and flexible setting of the burst phases of the interacting networks. We also show that a conductance-based mathematical model of the coupled circuits correctly reproduces the observed dynamics illustrating the generality of the phenomena. © 2009 Elsevier Inc. All rights reserved.},
   author = {Attila Szücs and Ramon Huerta and Mikhail I. Rabinovich and Allen I. Selverston},
   doi = {10.1016/j.neuron.2008.12.032},
   issn = {08966273},
   issue = {3},
   journal = {Neuron},
   keywords = {SYSNEURO},
   month = {2},
   pages = {439-453},
   pmid = {19217380},
   title = {Robust Microcircuit Synchronization by Inhibitory Connections},
   volume = {61},
   url = {https://www.sciencedirect.com/science/article/pii/S0896627309000385},
   year = {2009},
}
@report{Sohal2000,
   abstract = {Mice with an inactivated GABA A receptor 3 subunit gene have features of Angelman syndrome, including absence-like seizures. This suggests the occurrence of abnormal hypersyn-chrony in the thalamocortical system. Within the thalamus, the efficacy of inhibitory synapses between thalamic reticular (RE) neurons is selectively compromised, and thalamic oscillations in vitro are prolonged and lack spatial phase gradients (Hunts-man et al., 1999). Here we used computational models to examine how intra-RE inhibition regulates intrathalamic oscillations. A major effect is an abbreviation of network responses, which is caused by long-lasting intra-RE inhibition that shunts recurrent excitatory input. In addition, differential activation of RE cells desynchronizes network activity. Near the slice center, where many cells are initially activated, there is a resultant high level of intra-RE inhibition. This leads to RE cell burst truncation in the central region and a gradient in the timing of thalamo-cortical cell activity similar to that observed in vitro. Although RE cell burst durations were shortened by this mechanism, there was very little effect on the times at which RE cells began to burst. The above results depended on widespread stimuli that activated RE cells in regions larger than the diameter of intra-RE connections. By contrast, more focal stimuli could elicit oscillations that lasted several cycles and remained confined to a small region. These results suggest that intra-RE inhibition restricts intrathalamic activity to particular spatiotem-poral patterns to allow focal recurrent activity that may be relevant for normal thalamocortical function while preventing widespread synchronization as occurs in seizures. The thalamus participates in a wide range of thalamocortical oscillations, including 7-14 Hz sleep spindles (Steriade et al., 1993). Spindle activity occurs in the thalamus of decorticated cats (Morison and Basset, 1945) and in thalamic slices (von Krosigk et al., 1993; Huguenard and Prince, 1994a; Bal et al., 1995a,b; Kim et al., 1995), but not in cortex that has been disconnected from thalamus (Burns, 1950), suggesting that the thalamus plays an important role in generating the spindle rhythm. Several experimental observations suggest that reciprocal in-hibitory connections between thalamic reticular (RE) neurons regulate thalamic spindle oscillations and prevent hypersynchrony characteristic of some epilepsies. The anti-absence drug clonaz-epam may function by enhancing GABA A connections between RE cells, thereby reducing the output of RE cells to thalamocor-tical (TC) cells (Huguenard and Prince, 1994b). Additional evidence comes from mice lacking the 3 subunit of the GABA A receptor. Knockout of the 3 subunit reduces the strength and duration of GABA A synapses between RE cells without affecting those from RE to TC cells (Huntsman et al., 1999). This highly selective change has important consequences for intrathalamic oscillations elicited by stimulation of internal capsule in vitro (Hunstman et al., 1999). First, oscillations last much longer in thalamic slices from knockout (3 /) mice than in those from wild-type (3 /) animals. Second, phase differences between TC cell activity at different locations along knockout slices are negligible. In wild-type slices by contrast, phase differences between TC cell activity at different locations grow with distance. The phase lags observed in wild-type slices are at least an order of magnitude larger than those in knockout slices. These in vitro findings may help to explain why 3 / knockout mice have many features of Angelman syndrome, including seizures (Ho-manics et al., 1997; DeLorey et al., 1998). Here we used computational models to evaluate mechanisms by which intact intra-RE inhibition could produce these differences between oscillations in wild-type and knockout slices. We then studied how these mechanisms depend on the strength, kinetics, and spatial organization of intra-RE inhibition. Finally, we explore the functional consequences of these mechanisms for in-trathalamic activity. Our findings suggest that particular properties of intra-RE inhibition, such as its slow decay, enable it to restrict intrathalamic activity to particular spatiotemporal patterns and thus prevent epileptiform activity. MATERIALS AND METHODS Model neurons. We studied a network model that included 400 TC and 400 RE neurons. Models for both types of neurons were presented in an earlier study (Sohal and Huguenard, 1998). Each neuron was modeled as a single compartment. V T and V R , the membrane potentials of TC and RE cells, respectively, evolved according to: C m V ˙ T g L V T E L I T I h I K I Na I GABAATC , (1) C m V ˙ R g L V R E L I Ts I K I Na I AM PA I GABAARE , (2) where the specific capacitance of the membrane, C m , equals 1 F/cm 2 , g L is the leak conductance, E L is the reversal potential of the leak current, I T and I Ts are low-threshold calcium currents, I h is the hyperpolarization-activated cation current, I K and I Na are the potassium and sodium currents underlying action potentials, I GABA-A(TC) and I GABA-A(RE) are},
   author = {Vikaas S Sohal and Molly M Huntsman and John R Huguenard},
   issue = {5},
   journal = {The Journal of Neuroscience},
   keywords = {An-gelman syndrome,GABA A receptors,absence seizures,computational model,spindle rhythm,thalamus},
   pages = {1735-1745},
   title = {Reciprocal Inhibitory Connections Regulate the Spatiotemporal Properties of Intrathalamic Oscillations},
   volume = {20},
   url = {https://www.jneurosci.org/content/20/5/1735.short},
   year = {2000},
}
@article{Callaway2004,
   abstract = {Visual cortical circuits are organized at multiple levels of complexity including cortical areas, layers and columns, and specific cell types within these modules. Making sense of the functions of these circuits from anatomical observations requires linking these circuits to function at each of these levels of complexity. Observations of these relationships have become increasingly sophisticated over the last several decades, beginning with correlations between the connectivities and functions of various visual cortical areas and progressing toward cell type-specificity. These studies have informed current views about the functional interactions between cortical areas and modules and the mechanisms by which fine scale microcircuits influence interactions at more coarse levels of organization. © 2004 Elsevier Ltd. All rights reserved.},
   author = {Edward M. Callaway},
   doi = {10.1016/j.neunet.2004.04.004},
   issn = {08936080},
   issue = {5-6},
   journal = {Neural Networks},
   keywords = {Driving,Excitation,Gating,Inhibition,Local circuit,Modulatory},
   pages = {625-632},
   pmid = {15288888},
   publisher = {Elsevier Ltd},
   title = {Feedforward, feedback and inhibitory connections in primate visual cortex},
   volume = {17},
   url = {https://www.sciencedirect.com/science/article/pii/S0893608004000887},
   year = {2004},
}
@report{Buonomano1998,
   abstract = {It has been clear for almost two decades that cortical representations in adult animals are not fixed entities, but rather, are dynamic and are continuously modified by experience. The cortex can preferentially allocate area to represent the particular peripheral input sources that are proportionally most used. Alterations in cortical representations appear to underlie learning tasks dependent on the use of the behaviorally important peripheral inputs that they represent. The rules governing this cortical representational plasticity following manipulations of inputs, including learning, are increasingly well understood. In parallel with developments in the field of cortical map plasticity, studies of synaptic plasticity have characterized specific elementary forms of plasticity, including associative long-term potentiation and long-term depression of exci-tatory postsynaptic potentials. Investigators have made many important strides toward understanding the molecular underpinnings of these fundamental plasticity processes and toward defining the learning rules that govern their induction. The fields of cortical synaptic plasticity and cortical map plasticity have been implicitly linked by the hypothesis that synaptic plasticity underlies cortical map reorganization. Recent experimental and theoretical work has provided increasingly stronger support for this hypothesis. The goal of the current paper is to review the fields of both synaptic and cortical map plasticity with an emphasis on the work that attempts to unite both fields. A second objective is to highlight the gaps in our understanding of synaptic and cellular mechanisms underlying cortical representational plasticity. 149},
   author = {Dean V Buonomano and Michael M Merzenich},
   journal = {Annu. Rev. Neurosci},
   keywords = {Hebbian,LTD,LTP,cortex,topographic},
   pages = {149-86},
   title = {CORTICAL PLASTICITY: From Synapses to Maps},
   volume = {21},
   year = {1998},
}
@report{Buonomano1998,
   abstract = {It has been clear for almost two decades that cortical representations in adult animals are not fixed entities, but rather, are dynamic and are continuously modified by experience. The cortex can preferentially allocate area to represent the particular peripheral input sources that are proportionally most used. Alterations in cortical representations appear to underlie learning tasks dependent on the use of the behaviorally important peripheral inputs that they represent. The rules governing this cortical representational plasticity following manipulations of inputs, including learning, are increasingly well understood. In parallel with developments in the field of cortical map plasticity, studies of synaptic plasticity have characterized specific elementary forms of plasticity, including associative long-term potentiation and long-term depression of exci-tatory postsynaptic potentials. Investigators have made many important strides toward understanding the molecular underpinnings of these fundamental plasticity processes and toward defining the learning rules that govern their induction. The fields of cortical synaptic plasticity and cortical map plasticity have been implicitly linked by the hypothesis that synaptic plasticity underlies cortical map reorganization. Recent experimental and theoretical work has provided increasingly stronger support for this hypothesis. The goal of the current paper is to review the fields of both synaptic and cortical map plasticity with an emphasis on the work that attempts to unite both fields. A second objective is to highlight the gaps in our understanding of synaptic and cellular mechanisms underlying cortical representational plasticity. 149},
   author = {Dean V Buonomano and Michael M Merzenich},
   journal = {Annu. Rev. Neurosci},
   keywords = {Hebbian,LTD,LTP,cortex,topographic},
   pages = {149-86},
   title = {CORTICAL PLASTICITY: From Synapses to Maps},
   volume = {21},
   year = {1998},
}
@article{Neftci2016,
   abstract = {Recent studies have shown that synaptic unreliability is a robust and sufficient mechanism for inducing the stochasticity observed in cortex. Here, we introduce Synaptic Sampling Machines (S2Ms), a class of neural network models that uses synaptic stochasticity as a means to Monte Carlo sampling and unsupervised learning. Similar to the original formulation of Boltzmann machines, these models can be viewed as a stochastic counterpart of Hopfield networks, but where stochasticity is induced by a random mask over the connections. Synaptic stochasticity plays the dual role of an efficient mechanism for sampling, and a regularizer during learning akin to DropConnect. A local synaptic plasticity rule implementing an event-driven form of contrastive divergence enables the learning of generative models in an on-line fashion. S2Ms perform equally well using discrete-timed artificial units (as in Hopfield networks) or continuous-timed leaky integrate and fire neurons. The learned representations are remarkably sparse and robust to reductions in bit precision and synapse pruning: removal of more than 75% of the weakest connections followed by cursory re-learning causes a negligible performance loss on benchmark classification tasks. The spiking neuron-based S2Ms outperform existing spike-based unsupervised learners, while potentially offering substantial advantages in terms of power and complexity, and are thus promising models for on-line learning in brain-inspired hardware.},
   author = {Emre O. Neftci and Bruno U. Pedroni and Siddharth Joshi and Maruan Al-Shedivat and Gert Cauwenberghs},
   doi = {10.3389/fnins.2016.00241},
   issn = {1662453X},
   issue = {JUN},
   journal = {Frontiers in Neuroscience},
   keywords = {Hopfield networks,Regularization,Spiking neural networks,Stochastic processes,Synaptic plasticty,Synaptic transmission,Unsupervised learning},
   month = {6},
   note = {Not for this Chapter. Has to do with ML},
   publisher = {Frontiers Research Foundation},
   title = {Stochastic synapses enable efficient brain-inspired learning machines},
   volume = {10},
   year = {2016},
}
@report{Buonomano1998,
   abstract = {It has been clear for almost two decades that cortical representations in adult animals are not fixed entities, but rather, are dynamic and are continuously modified by experience. The cortex can preferentially allocate area to represent the particular peripheral input sources that are proportionally most used. Alterations in cortical representations appear to underlie learning tasks dependent on the use of the behaviorally important peripheral inputs that they represent. The rules governing this cortical representational plasticity following manipulations of inputs, including learning, are increasingly well understood. In parallel with developments in the field of cortical map plasticity, studies of synaptic plasticity have characterized specific elementary forms of plasticity, including associative long-term potentiation and long-term depression of exci-tatory postsynaptic potentials. Investigators have made many important strides toward understanding the molecular underpinnings of these fundamental plasticity processes and toward defining the learning rules that govern their induction. The fields of cortical synaptic plasticity and cortical map plasticity have been implicitly linked by the hypothesis that synaptic plasticity underlies cortical map reorganization. Recent experimental and theoretical work has provided increasingly stronger support for this hypothesis. The goal of the current paper is to review the fields of both synaptic and cortical map plasticity with an emphasis on the work that attempts to unite both fields. A second objective is to highlight the gaps in our understanding of synaptic and cellular mechanisms underlying cortical representational plasticity. 149},
   author = {Dean V Buonomano and Michael M Merzenich},
   journal = {Annu. Rev. Neurosci},
   keywords = {Hebbian,LTD,LTP,cortex,topographic},
   note = {I read a little but then it got way too complicated ,probably bc we need medical background<br/><br/>},
   pages = {149-86},
   title = {CORTICAL PLASTICITY: From Synapses to Maps},
   volume = {21},
   year = {1998},
}
@article{,
   doi = {10.1038/nature23098},
   note = {Haven't noticed anything interesting. Take a look if you want<br/><br/>* Visual space is mapped onto the dendrites in an organized way<br/>* neurons that fire together wire together<br/>[George]<br/><br/><br/>},
   title = {Synapses get together for vision},
}
@article{Abraham2019,
   abstract = {It has been 70 years since Donald Hebb published his formalized theory of synaptic adaptation during learning. Hebb’s seminal work foreshadowed some of the great neuroscientific discoveries of the following decades, including the discovery of long-term potentiation and other lasting forms of synaptic plasticity, and more recently the residence of memories in synaptically connected neuronal assemblies. Our understanding of the processes underlying learning and memory has been dominated by the view that synapses are the principal site of information storage in the brain. This view has received substantial support from research in several model systems, with the vast majority of studies on the topic corroborating a role for synapses in memory storage. Yet, despite the neuroscience community’s best efforts, we are still without conclusive proof that memories reside at synapses. Furthermore, an increasing number of non-synaptic mechanisms have emerged that are also capable of acting as memory substrates. In this review, we address the key findings from the synaptic plasticity literature that make these phenomena such attractive memory mechanisms. We then turn our attention to evidence that questions the reliance of memory exclusively on changes at the synapse and attempt to integrate these opposing views.},
   author = {Wickliffe C. Abraham and Owen D. Jones and David L. Glanzman},
   doi = {10.1038/s41539-019-0048-y},
   issn = {2056-7936},
   issue = {1},
   journal = {npj Science of Learning},
   month = {12},
   publisher = {Springer Science and Business Media LLC},
   title = {Is plasticity of synapses the mechanism of long-term memory storage?},
   volume = {4},
   year = {2019},
}
@article{Balduzzi2013,
   abstract = {Neurons deep in cortex interact with the environment extremely indirectly; the spikes they receive and produce are pre- and post-processed by millions of other neurons. This paper proposes two information-theoretic constraints guiding the production of spikes, that help ensure bursting activity deep in cortex relates meaningfully to events in the environment. First, neurons should emphasize selective responses with bursts. Second, neurons should propagate selective inputs by burst-firing in response to them. We show the constraints are necessary for bursts to dominate information-transfer within cortex, thereby providing a substrate allowing neurons to distribute credit amongst themselves. Finally, since synaptic plasticity degrades the ability of neurons to burst selectively, we argue that homeostatic regulation of synaptic weights is necessary, and that it is best performed offline during sleep. © 2012 Springer-Verlag.},
   author = {David Balduzzi and Giulio Tononi},
   doi = {10.1007/s12064-012-0165-0},
   issn = {16117530},
   issue = {1},
   journal = {Theory in Biosciences},
   keywords = {Credit assignment,Information theory,Selectivity,Synaptic plasticity},
   month = {3},
   pages = {27-39},
   pmid = {22956291},
   publisher = {Springer Verlag},
   title = {What can neurons do for their brain? Communicate selectivity with bursts},
   volume = {132},
   year = {2013},
}
@article{Sengupta2010,
   abstract = {The initiation and propagation of action potentials (APs) places high demands on the energetic resources of neural tissue. Each AP forces ATP-driven ion pumps to work harder to restore the ionic concentration gradients, thus consuming more energy. Here, we ask whether the ionic currents underlying the AP can be predicted theoretically from the principle of minimum energy consumption. A long-held supposition that APs are energetically wasteful, based on theoretical analysis of the squid giant axon AP, has recently been overturned by studies that measured the currents contributing to the AP in several mammalian neurons. In the single compartment models studied here, AP energy consumption varies greatly among vertebrate and invertebrate neurons, with several mammalian neuron models using close to the capacitive minimum of energy needed. Strikingly, energy consumption can increase by more than ten-fold simply by changing the overlap of the Na+ and K+ currents during the AP without changing the APs shape. As a consequence, the height and width of the AP are poor predictors of energy consumption. In the Hodgkin-Huxley model of the squid axon, optimizing the kinetics or number of Na+ and K+ channels can whittle down the number of ATP molecules needed for each AP by a factor of four. In contrast to the squid AP, the temporal profile of the currents underlying APs of some mammalian neurons are nearly perfectly matched to the optimized properties of ionic conductances so as to minimize the ATP cost. © 2010 Sengupta et al.},
   author = {Biswa Sengupta and Martin Stemmler and Simon B. Laughlin and Jeremy E. Niven},
   doi = {10.1371/journal.pcbi.1000840},
   issn = {1553734X},
   issue = {7},
   journal = {PLoS Computational Biology},
   month = {7},
   note = {a little too specific and not that relevant maybe?<br/><br/>},
   pages = {35},
   pmid = {20617202},
   title = {Action potential energy efficiency varies among neuron types in vertebrates and invertebrates},
   volume = {6},
   year = {2010},
}
@article{Baslow2009,
   abstract = {In this paper evidence is provided that individual neurons possess language, and that the basic unit for communication consists of two neurons and their entire field of interacting dendritic and synaptic connections. While information processing in the brain is highly complex, each neuron uses a simple mechanism for transmitting information. This is in the form of temporal electrophysiological action potentials or spikes (S) operating on a millisecond timescale that, along with pauses (P) between spikes constitute a two letter "alphabet"that generates meaningful frequency-encoded signals or neuronal S/P "words"in a primary language. However, when a word from an afferent neuron enters the dendritic-synaptic-dendritic field between two neurons, it is translated into a new frequency-encoded word with the same meaning, but in a different spike-pause language, that is delivered to and understood by the efferent neuron. It is suggested that this unidirectional inter-neuronal language-based word translation step is of utmost importance to brain function in that it allows for variations in meaning to occur. Thus, structural or biochemical changes in dendrites or synapses can produce novel words in the second language that have changed meanings, allowing for a specific signaling experience, either external or internal, to modify the meaning of an original word (learning), and store the learned information of that experience (memory) in the form of an altered dendritic-synaptic-dendritic field. © 2009 by the authors; licensee Molecular Diversity Preservation International, Basel, Switzerland.},
   author = {Morris H. Baslow},
   doi = {10.3390/e11040782},
   issn = {10994300},
   issue = {4},
   journal = {Entropy},
   keywords = {Biosemiotics,Codes,Cognition,Language,Learning,Memory,Neurons,Semiosis},
   note = {weird af den m arese},
   pages = {782-797},
   publisher = {MDPI AG},
   title = {The languages of neurons: An analysis of coding mechanisms by which neurons communicate, learn and store information},
   volume = {11},
   year = {2009},
}
@generic{Fellin2009,
   abstract = {Neuromodulation is a fundamental process in the brain that regulates synaptic transmission, neuronal network activity and behavior. Emerging evidence demonstrates that astrocytes, a major population of glial cells in the brain, play previously unrecognized functions in neuronal modulation. Astrocytes can detect the level of neuronal activity and release chemical transmitters to influence neuronal function. For example, recent findings show that astrocytes play crucial roles in the control of Hebbian plasticity, the regulation of neuronal excitability and the induction of homeostatic plasticity. This review discusses the importance of astrocyte-to-neuron signaling in different aspects of neuronal function from the activity of single synapses to that of neuronal networks. © 2008 The Author.},
   author = {Tommaso Fellin},
   doi = {10.1111/j.1471-4159.2008.05830.x},
   issn = {00223042},
   issue = {3},
   journal = {Journal of Neurochemistry},
   keywords = {Astrocytes,Ca2+ signaling,Glia,Gliotransmission,Synaptic plasticity,Synaptic transmission},
   month = {2},
   pages = {533-544},
   pmid = {19187090},
   title = {Communication between neurons and astrocytes: Relevance to the modulation of synaptic and network activity},
   volume = {108},
   year = {2009},
}
@generic{Disterhoft2006,
   abstract = {In vitro experiments indicate that intrinsic neuronal excitability, as evidenced by changes in the post-burst afterhyperpolarization (AHP) and spike-frequency accommodation, is altered during learning and normal aging in the brain. Here we review these studies, highlighting two consistent findings: (i) that AHP and accommodation are reduced in pyramidal neurons from animals that have learned a task; and (ii) that AHP and accommodation are enhanced in pyramidal neurons from aging subjects, a cellular change that might contribute to age-related learning impairments. Findings from in vivo single-neuron recording studies complement the in vitro data. From these consistently reproduced findings, we propose that the intrinsic AHP level might determine the degree of synaptic plasticity and learning. Furthermore, it seems that reductions in the AHP must occur before learning if young and aging subjects are to learn a task successfully. © 2006 Elsevier Ltd. All rights reserved.},
   author = {John F. Disterhoft and M. Matthew Oh},
   doi = {10.1016/j.tins.2006.08.005},
   issn = {01662236},
   issue = {10},
   journal = {Trends in Neurosciences},
   month = {10},
   pages = {587-599},
   pmid = {16942805},
   title = {Learning, aging and intrinsic neuronal plasticity},
   volume = {29},
   year = {2006},
}
@article{Long2005,
   abstract = {In the suprachiasmatic nucleus (SCN), the master circadian pacemaker, neurons show circadian variations in firing frequency. There is also considerable synchrony of spiking across SCN neurons on a scale of milliseconds, but the mechanisms are poorly understood. Using paired whole-cell recordings, we have found that many neurons in the rat SCN communicate via electrical synapses. Spontaneous spiking was often synchronized in pairs of electrically coupled neurons, and the degree of this synchrony could be predicted from the magnitude of coupling. In wild-type mice, as in rats, the SCN contained electrical synapses, but electrical synapses were absent in connexin36-knockout mice. The knockout mice also showed dampened circadian activity rhythms and a delayed onset of activity during transition to constant darkness. We suggest that electrical synapses in the SCN help to synchronize its spiking activity, and that such synchrony is necessary for normal circadian behavior.},
   author = {Michael A. Long and Michael J. Jutras and Barry W. Connors and Rebecca D. Burwell},
   doi = {10.1038/nn1361},
   issn = {10976256},
   issue = {1},
   journal = {Nature Neuroscience},
   month = {1},
   note = {ithas to do twith circadian rythm? so not that important?<br/><br/>},
   pages = {61-66},
   pmid = {15580271},
   title = {Electrical synapses coordinate activity in the suprachiasmatic nucleus},
   volume = {8},
   year = {2005},
}
@generic{Hormuzdi2004,
   abstract = {Gap junctions consist of intercellular channels dedicated to providing a direct pathway for ionic and biochemical communication between contacting cells. After an initial burst of publications describing electrical coupling in the brain, gap junctions progressively became less fashionable among neurobiologists, as the consensus was that this form of synaptic transmission would play a minimal role in shaping neuronal activity in higher vertebrates. Several new findings over the last decade (e.g. the implication of connexins in genetic diseases of the nervous system, in processing sensory information and in synchronizing the activity of neuronal networks) have brought gap junctions back into the spotlight. The appearance of gap junctional coupling in the nervous system is developmentally regulated, restricted to distinct cell types and persists after the establishment of chemical synapses, thus suggesting that this form of cell-cell signaling may be functionally interrelated with, rather than alternative to chemical transmission. This review focuses on gap junctions between neurons and summarizes the available data, derived from molecular, biological, electrophysiological, and genetic approaches, that are contributing to a new appreciation of their role in brain function. © 2004 Elsevier B.V. All rights reserved.},
   author = {Sheriar G. Hormuzdi and Mikhail A. Filippov and Georgia Mitropoulou and Hannah Monyer and Roberto Bruzzone},
   doi = {10.1016/j.bbamem.2003.10.023},
   issn = {00052736},
   issue = {1-2},
   journal = {Biochimica et Biophysica Acta - Biomembranes},
   keywords = {Connexin,Coupling,Gap junction,Neuron,Oscillation,Retina},
   month = {3},
   note = {use part of the paper, (synasapses,<br/>connexins)},
   pages = {113-137},
   pmid = {15033583},
   title = {Electrical synapses: A dynamic signaling system that shapes the activity of neuronal networks},
   volume = {1662},
   year = {2004},
}
@report{Maclean2003,
   abstract = {strate that a neuron can sense and adapt to overexpres-sion of one ion channel type in the absence of significant Ithaca, New York 14853 changes in physiological activity. This is accomplished by increasing the amplitude of a second current with some opposing physiological effects, providing a bal-Summary ancing electrophysiological compensation for the induced current. The shal gene encodes the transient potassium cur-We set out to study the role of the shal gene, a member rent (I A) in neurons of the lobster stomatogastric gan-of the shaker family of voltage-dependent K channel glion. Overexpression of Shal by RNA injection into genes, in rhythmically active pyloric dilator (PD) neurons neurons produces a large increase in I A , but surpris-in the stomatogastric ganglion (STG) of the spiny lobster, ingly little change in the neuron's firing properties. Panulirus interruptus. These neurons are members of Accompanying the increase in I A is a dramatic and the pacemaker kernel of the pyloric network, a well un-linearly correlated increase in the hyperpolarization-derstood rhythmic motor pattern generator (Johnson activated inward current (I h). The enhanced I h electro-and Hooper, 1992; Ayali and Harris-Warrick, 1999). The physiologically compensates for the enhanced I A , PD neurons fire rhythmic bursts of action potentials su-since pharmacological blockade of I h uncovers the perimposed on slow membrane potential oscillations. physiological effects of the increased I A. Expression The relatively stereotyped oscillatory properties of these of a nonfunctional mutant Shal also induces a large neurons are determined by the complement of ion chan-increase in I h , demonstrating a novel activity-indepen-nels they express and their synaptic inputs (Getting, dent coupling between the Shal protein and I h en-1989; Selverston and Moulins, 1985). The fast transient hancement. Since I A and I h influence neuronal activity potassium current (I A) helps to shape PD neuron firing in opposite directions, our results suggest a selective properties, including the maximal spike frequency dur-coregulation of these channels as a mechanism for ing a burst and the rate of postinhibitory rebound (Tier-constraining cell activity within appropriate physiolog-ney and Harris-Warrick, 1992). Single-cell RT-PCR and ical parameters. immunocytochemistry have shown that the shal gene encodes I A in pyloric neurons within the STG (Baro et Introduction al., 1997, 2000, 2001; Baro and Harris-Warrick, 1998). To further study the role of I A , we artificially increased Many neurons, as functional units in networks generat-I A amplitude by microinjecting shal RNA into the PD ing complex behaviors, must develop and maintain a neurons. Despite large increases in I A amplitude, the stable physiological identity despite continuously firing properties of the neurons were essentially un-changing inputs both during development and in the changed. Surprisingly, the increased I A is compensated adult. Each neuron's intrinsic excitable properties and by a corresponding endogenous upregulation of the hy-preferred activity range are defined by the pattern of perpolarization-activated inward current, I h. Our findings specific ion channels, receptors, and enzymes that it suggest a novel level of regulation for specific ion chan-expresses. One important mechanism for developing nels that promotes stability in the activity of individual and maintaining stable neuronal function is activity-neurons as well as the entire network. dependent homeostasis; both vertebrate and invertebrate neurons can restore normal firing patterns after Results imposed changes in firing activity induced, for example, by loss of normal synaptic inputs or prolonged pharma-Significant Increase in I A 72 hr after Microinjection cological block of activity. Such activity-dependent re-of shal-GFP RNA sponses involve slow compensatory changes in the bal-To visualize protein expression after shal RNA injection, ance of ionic currents expressed in the neuron the sequence for green fluorescent protein (GFP) was (LeMasson et al., 1993; Turrigiano, 1999; Turrigiano et ligated near the C-terminal end of the shal clone. GFP al., 1994, 1995, 1998; Desai et al., 1999; Golowasch et ligation had no effect on the biophysical properties of al., 1999; Spitzer, 1999; Galante et al., 2001). However, the Shal current when expressed in Xenopus oocytes: since the trigger for the homeostatic response is a the amplitude of the current, voltage dependence of change in activity, the neuron must at least temporarily activation and inactivation, and rates of inactivation fire in an abnormal fashion. were all unaffected (n 6). To be sure that appending Here we report a novel activity-independent homeo-GFP had no effect, we carried out parallel measurements with both shal-GFP and shal RNA injections into the PDan anti-Shal antibody to visualize successful Shal ex-versity,},
   author = {Jason N Maclean and Ying Zhang and Bruce R Johnson and Ronald M Harris-Warrick},
   journal = {Neuron},
   title = {Activity-Independent Homeostasis in Rhythmically Active Neurons static mechanism that could function in concert with activity-dependent mechanisms to maintain neuronal firing patterns within their normal ranges. We demon},
   volume = {37},
   year = {2003},
}
@generic{Li2003,
   abstract = {Neurons in the brain touch and communicate with each other at specialized contacts called synapses. So, how do these tiny intercellular junctions form during development? The assembly of neuronal synapses proceeds through several stages, and occurs with surprising speed. Recent biochemical, genetic and imaging studies are beginning to disclose the molecular mechanisms that underlie synapse formation, growth and maturation.},
   author = {Zheng Li and Morgan Sheng},
   doi = {10.1038/nrm1242},
   issn = {14710072},
   issue = {11},
   journal = {Nature Reviews Molecular Cell Biology},
   month = {11},
   note = {it has a nice description of synapses but then it is focused on thea early steps of synapse formation so maybe the rest is not important?},
   pages = {833-841},
   pmid = {14625534},
   title = {Some assembly required: The development of neuronal synapses},
   volume = {4},
   year = {2003},
}
@generic{Salinas2001,
   abstract = {For years we have known that cortical neurons collectively have synchronous or oscillatory patterns of activity, the frequencies and temporal dynamics of which are associated with distinct behavioural states. Although the function of these oscillations has remained obscure, recent experimental and theoretical results indicate that correlated fluctuations might be important for cortical processes, such as attention, that control the flow of information in the brain.},
   author = {Emilio Salinas and Terrence J. Sejnowski},
   doi = {10.1038/35086012},
   issn = {14710048},
   issue = {8},
   journal = {Nature Reviews Neuroscience},
   month = {8},
   note = {Talks about how neurons correlate their activity together},
   pages = {539-550},
   pmid = {11483997},
   title = {Correlated neuronal activity and the flow of neural information},
   volume = {2},
   year = {2001},
}
@generic{Cohen1973,
   author = {L. B. Cohen},
   doi = {10.1152/physrev.1973.53.2.373},
   issn = {00319333},
   issue = {2},
   journal = {Physiological reviews},
   pages = {373-418},
   pmid = {4349816},
   title = {Changes in neuron structure during action potential propagation and synaptic transmission.},
   volume = {53},
   year = {1973},
}
@report{,
   abstract = {The brain acts as an integrated information processing system, which methods in cognitive neuroscience have so far depicted in a fragmented fashion. Here, we propose a simple and robust way to integrate functional MRI (fMRI) with single trial event-related potentials (ERP) to provide a more complete spatiotemporal characterization of evoked responses in the human brain. The idea behind the approach is to find brain regions whose fMRI responses can be predicted by paradigm-induced amplitude modulations of simultaneously acquired single trial ERPs. The method was used to study a variant of a two-stimulus auditory target detection (odd-ball) paradigm that manipulated predictability through alternations of stimulus sequences with random or regular target-to-target intervals. In addition to electrophysiologic and hemodynamic evoked responses to auditory targets per se, single-trial modulations were expressed during the latencies of the P2 (170-ms), N2 (200-ms), and P3 (320-ms) components and predicted spatially separated fMRI activation patterns. These spatiotemporal matches, i.e., the prediction of hemodynamic activation by time-variant information from single trial ERPs, permit inferences about regional responses using fMRI with the temporal resolution provided by electrophysiology. multimodal imaging P3 pattern learning target detection F unctional MRI (fMRI) of the blood oxygenation level-dependent (BOLD) response (BOLD-fMRI) measures local changes in brain hemodynamics associated with a cognitive process noninvasively with a high spatial resolution. However, an unsolved issue in fMRI research is the insufficient temporal resolution of the BOLD response. In contrast to the spatial resolution of BOLD-fMRI, event-related potentials (ERP) access the current induced by synaptic activity instantaneously, with an effective temporal resolution on the order of tens to hundreds of milliseconds in case of long-latency cortical responses. However, the location of underlying generators cannot be inferred with certainty. In combination, these two complementary noninvasive methods would allow for joint high-resolution spatial and temporal mapping of the mental process under investigation and add to a more complete understanding of the neural correlates of perception and cognition (1-3). In humans, this integrated spatial and temporal precision could so far be obtained only in direct intracranial recordings, usually performed in patients receiving brain surgery for treatment of epilepsy (4-7). There are basically three approaches to multimodal integration: (i) through fusion, usually referring to the use of a common forward or generative model that can explain both the electroencephalo-gram (EEG) and fMRI data (8, 9); (ii) through constraints, where spatial information from the fMRI is used for a (spatiotemporal) source reconstruction of the EEG (10-12); and (iii) through prediction, where the fMRI signal is modeled as some measure of the EEG convolved with a hemodynamic response function, a principle used in our study. Invasive recordings in animals have shown that the BOLD response is approximately linearly related to local changes in the underlying neuronal activity. The relationship appears to be stronger for the afferent pre-and postsynaptic processing, which produces the local field potential (LFP), than it is for the output from the neuron, i.e., spike rate or multiunit activity (13-16). The LFP is the basis for the scalp EEG and ERP when coherent at a more macroscopic scale (17), implying that spatiotemporal data integration can be achieved by investigating correlations between BOLD and scalp EEGERP. This can be done either continuously over time, as in the study of background rhythms (18-20) and epileptic discharges (21, 22) in the EEG, or in the context of inducing variation in a given cognitive operation (23-25). When a consistent relationship is detected, one can infer that the corresponding fMRI activation either directly represents the electric source or modulates remote generators (18-25). However, the temporal evolution of neuronal activation has not been addressed. To resolve this issue, we used the trial-to-trial variability of single-trial ERPs (26, 27) recorded simultaneously with the fMRI as predictors for hemody-namic responses to a variant of an auditory target detection (oddball) paradigm. In this design, infrequent targets were interspersed with frequent standard stimuli at random or regular intervals in an alternating way (see also Fig. 5, which is published as supporting information on the PNAS web site). Sequences of regularly spaced targets, i.e., patterns embedded in this design, affect the subjective predictabilityexpectancy (28, 29), and pilot experiments indicated that several components, at different laten-cies in the ERP, are modulated according to a sigmoid function of the number of times an interval is repeated, and learned. These amplitude modulations (AMs) develop across trials, on a timescale slow enough to be sampled with fMRI, and should be consistently correlated with the BOLD response in discrete brain regions across the observation time, assuming temporally and spatially independent neuronal generators (Fig. 1). fMRI responses that can be predicted by AMs in the ERP can be tied to the processing engaged at the time of the AMs. The approach thus allows inferences about This work was presented in part in poster form at the},
   author = {Tom Eichele and Karsten Specht and Matthias Moosmann and Marijtje L A Jongsma and Rodrigo Quian Quiroga and Helge Nordby and Kenneth Hugdahl},
   issue = {49},
   keywords = {AM, amplitude modulation,Abbreviations: ERP, event-related potential,BOLD, blood oxygen-ation level-dependent,EEG, electroencephalogram,TTI,,fMRI, functional MRI},
   note = {Possibly not suitable},
   title = {Assessing the spatiotemporal evolution of neuronal activation with single-trial event-related potentials and functional MRI},
   volume = {6},
   url = {www.pnas.orgcgidoi10.1073pnas.0505508102},
}
@report{,
   abstract = {The topic of multiple forms of memory is considered from a biological point of view. Fact-and-event (declarative, explicit) memory is contrasted with a collection of nonconscious (non-declarative, implicit) memory abilities including skills and habits , priming, and simple conditioning. Recent evidence is reviewed indicating that declarative and nondeclarative forms of memory have different operating characteristics and depend on separate brain systems. A brain-systems framework for understanding memory phenomena is developed in light of lesion studies involving rats, monkeys, and humans, as well as recent studies with normal humans using the divided visual field technique, event-related potentials, and positron emission to-mography (PET). m INTRODUCTION},
   author = {Larry R Squire},
   title = {Declarative and Nondeclarative Memory: Multiple Brain Systems Supporting Learning and Memory},
   url = {http://direct.mit.edu/jocn/article-pdf/4/3/232/1754979/jocn.1992.4.3.232.pdf},
}
@report{Gabrieli1998,
   abstract = {Current knowledge is summarized about long-term memory systems of the human brain, with memory systems defined as specific neural networks that support specific mnemonic processes. The summary integrates convergent evidence from neuropsychological studies of patients with brain lesions and from functional neuroimaging studies using positron emission tomography (PET) or functional magnetic resonance imaging (fMRI). Evidence is reviewed about the specific roles of hippocampal and parahippocampal regions , the amygdala, the basal ganglia, and various neocortical areas in declarative memory. Evidence is also reviewed about which brain regions mediate specific kinds of procedural memory, including sensorimotor, perceptual , and cognitive skill learning; perceptual and conceptual repetition priming ; and several forms of conditioning. Findings are discussed in terms of the functional neural architecture of normal memory, age-related changes in memory performance, and neurological conditions that affect memory such as amnesia, Alzheimer's disease, Parkinson's disease, and Huntington's disease. CONTENTS},
   author = {J D E Gabrieli},
   journal = {Annu. Rev. Psychol},
   keywords = {conditioning,declarative memory,functional brain imaging,repetition priming,skill learning},
   pages = {87-115},
   title = {COGNITIVE NEUROSCIENCE OF HUMAN MEMORY},
   volume = {49},
   year = {1998},
}
@report{Milner1998,
   author = {Brenda Milner and Larry R Squire and Eric R Kandel},
   journal = {Neuron},
   pages = {445-468},
   title = {Cognitive Neuroscience Review and the Study of Memory},
   volume = {20},
   year = {1998},
}
@generic{LaBar2006,
   abstract = {Emotional events often attain a privileged status in memory. Cognitive neuroscientists have begun to elucidate the psychological and neural mechanisms underlying emotional retention advantages in the human brain. The amygdala is a brain structure that directly mediates aspects of emotional learning and facilitates memory operations in other regions, including the hippocampus and prefrontal cortex. Emotion-memory interactions occur at various stages of information processing, from the initial encoding and consolidation of memory traces to their long-term retrieval. Recent advances are revealing new insights into the reactivation of latent emotional associations and the recollection of personal episodes from the remote past. © 2006 Nature Publishing Group.},
   author = {Kevin S. LaBar and Roberto Cabeza},
   doi = {10.1038/nrn1825},
   issn = {1471003X},
   issue = {1},
   journal = {Nature Reviews Neuroscience},
   month = {1},
   pages = {54-64},
   pmid = {16371950},
   title = {Cognitive neuroscience of emotional memory},
   volume = {7},
   year = {2006},
}
@article{Murray2020,
   abstract = {The learning of motor skills unfolds over multiple timescales, with rapid initial gains in performance followed by a longer period in which the behavior becomes more refined, habitual, and automatized. While recent lesion and inactivation experiments have provided hints about how various brain areas might contribute to such learning, their precise roles and the neural mechanisms underlying them are not well understood. In this work, we propose neural- and circuit-level mechanisms by which motor cortex, thalamus, and striatum support motor learning. In this model, the combination of fast cortical learning and slow subcortical learning gives rise to a covert learning process through which control of behavior is gradually transferred from cortical to subcortical circuits, while protecting learned behaviors that are practiced repeatedly against overwriting by future learning. Together, these results point to a new computational role for thalamus in motor learning and, more broadly, provide a framework for understanding the neural basis of habit formation and the automatization of behavior through practice.},
   author = {James M. Murray and G. Sean Escola},
   doi = {10.1038/s41467-020-19788-5},
   issn = {20411723},
   issue = {1},
   journal = {Nature Communications},
   month = {12},
   pmid = {33361766},
   publisher = {Nature Research},
   title = {Remembrance of things practiced with fast and slow learning in cortical and subcortical pathways},
   volume = {11},
   year = {2020},
}
@article{Morgan2020,
   abstract = {The brain utilizes distinct neural mechanisms that ease the transition through different stages of learning. Furthermore, evidence from category learning has shown that dissociable memory systems are engaged, depending on the structure of a task. This can even hold true for tasks that are very similar to each other, which complicates the process of classifying brain activity as relating to changes that are associated with learning or reflecting the engagement of a memory system suited for the task. The primary goals of these studies were to characterize the mechanisms that are associated with category learning and understand the extent to which different memory systems are recruited within a single task. Two studies providing spatial and temporal distinctions between learning-related changes in the brain and category-dependent memory systems are presented. The results from these experiments support the notion that exemplar memorization, rule-based, and perceptual similarity-based categorization are flexibly recruited in order to optimize performance during a single task. We conclude that these three methods, along with the memory systems they rely on, aid in the development of expertise, but their engagement might depend on the level of familiarity with a category.},
   author = {Kyle K. Morgan and Dagmar Zeithamova and Phan Luu and Don Tucker},
   doi = {10.3390/brainsci10040224},
   issn = {20763425},
   issue = {4},
   journal = {Brain Sciences},
   keywords = {Category learning,Eeg,Erp,Learning,Machine learning,Memory,Multiple memory systems,P300},
   month = {4},
   publisher = {MDPI AG},
   title = {Spatiotemporal dynamics of multiple memory systems during category learning},
   volume = {10},
   year = {2020},
}
@book_section{,
   author = {Wulfram Gerstner and  Werner M. Kistler},
   doi = {10.1017/CBO9780511815706.002},
   journal = {Spiking Neuron Models},
   month = {8},
   note = {might fit better in coding section,it contains a lot of similar things with the neuronal dynamics book.Howeverin the beggining it talks a little about neurons and synapses<br/><br/>},
   pages = {1-28},
   publisher = {Cambridge University Press},
   title = {Introduction},
   url = {https://www.cambridge.org/core/product/identifier/CBO9780511815706A008/type/book_part},
   year = {2002},
}
@book{,
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-85148-3},
   editor = {G. Buzsáki and R. Llinás and W. Singer and A. Berthoz and Y. Christen},
   isbn = {978-3-642-85150-6},
   publisher = {Springer Berlin Heidelberg},
   title = {Temporal Coding in the Brain},
   url = {http://link.springer.com/10.1007/978-3-642-85148-3},
   year = {1994},
}
@report{Maass1997,
   abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology. 1. DEFINITIONS AND MOTIVATIONS If one classifies neural network models according to their computational units, one can distinguish three different generations. The first generation is based on McCulloch-Pitts neurons as computational units. These are also referred to as perceptrons or threshold gates. They give rise to a variety of neural network models such as multilayer perceptrons (also called threshold circuits), Hopfield nets, and Boltzmann machines. A characteristic feature of these models is that they can only give digital output. In fact they are universal for computations with digital input and output, and every boolean function can be computed by some multilayer perceptron with a single hidden layer. The second generation is based on computational units that apply an "activation function" with a continuous set of possible output values to a weighted sum (or polynomial) of the inputs. Common activation functions are the sigmoid function a(y) = 1/(1 + e-y) and the linear Acknowledgements: I would like to thank Eduardo Sontag and an anonymous referee for their helpful comments. Written under partial support by the Austrian Science Fund. Requests for reprints should be sent to W. Maass,},
   author = {Wolfgang Maass},
   issue = {9},
   keywords = {Computational complexity,Integrate-and-fire neutron,Lower bounds,Sigmoidal neural nets,Spiking neuron},
   pages = {1659-1671},
   title = {Networks of Spiking Neurons: The Third Generation of Neural Network Models},
   volume = {10},
   year = {1997},
}
@report{,
   abstract = {A frightening thought for a computer scientist is that there might be completely different ways of designing computing machinery, that we may miss by focusing on incremental improvements of current designs. In fact, we know that there exist much better design strategies, since the human brain has information processing capabilities that are in many aspects superior to our current computers. Furthermore the brain does not require expensive programming, debugging or replacement of failed parts, and it consumes just 10-20 Watts of energy. Unfortunately, most information processing strategies of the brain are still a mystery. In spite of many positive reports in the media, even the most basic questions concerning the organization of the computational units of the brain and the way in which the brain implements learning and memory, have not yet been answered. They are waiting to be unraveled by concerted efforts of scientists from many disciplines. Computer science is one of the disciplines from which substantial contributions are expected, and in fact other countries have established already hundreds of research facilities in the new hybrid discipline Computational Neuroscience 1 , which is dedicated to the investigation of computational principles in the brain. Computer scientists are contributing to these efforts through their experience in the evaluation of real and hypothetical systems that compute, as well as experience with robots and other machines that learn, move around, and explore their 1 http://home.earthlink.net/~perlewitz/},
   author = {Wolfgang Maass},
   title = {Computing with Spikes},
   url = {http://www.igi.TUGraz.at/maass},
}
@article{Esser2016,
   abstract = {Deep networks are now able to achieve human-level performance on a broad spectrum of recognition tasks. Independently, neuromorphic computing has now demonstrated unprecedented energy-efficiency through a new chip architecture based on spiking neurons, low precision synapses, and a scalable communication network. Here, we demonstrate that neuromorphic computing, despite its novel architectural primitives, can implement deep convolution networks that (/) approach state-of-the-art classification accuracy across eight standard datasets encompassing vision and speech, (ii) perform inference while preserving the hardware's underlying energy-efficiency and high throughput, running on the aforementioned datasets at between 1,200 and 2,600 frames/s and using between 25 and 275 mW (effectively >6,000 frames/s per Watt), and (iii') can be specified and trained using backpropagation with the same ease-of-use as contemporary deep learning. This approach allows the algorithmic power of deep learning to be merged with the efficiency of neuromorphic processors, bringing the promise of embedded, intelligent, brain-inspired computing one step closer.},
   author = {Steven K. Esser and Paul A. Merolla and John V. Arthur and Andrew S. Cassidy and Rathinakumar Appuswamy and Alexander Andreopoulos and David J. Berg and Jeffrey L. McKinstry and Timothy Melano and Davis R. Barch and Carmelo Di Nolfo and Pallab Datta and Arnon Amir and Brian Taba and Myron D. Flickner and Dharmendra S. Modha},
   doi = {10.1073/pnas.1604850113},
   issn = {10916490},
   issue = {41},
   journal = {Proceedings of the National Academy of Sciences of the United States of America},
   keywords = {Convolutional network,Neural network,Neuromorphic,Truenorth},
   month = {10},
   pages = {11441-11446},
   publisher = {National Academy of Sciences},
   title = {Convolutional networks for fast, energy-efficient neuromorphic computing},
   volume = {113},
   year = {2016},
}
@article{Zenke2018,
   abstract = {Avastmajority of computation in the brain is performed by spiking neural networks. Despite the ubiquity of such spiking, we currently lack an understanding of how biological spiking neural circuits learn and compute in vivo, as well as how we can instantiate such capabilities in artificial spiking circuits in silico. Here we revisit the problem of supervised learning in temporally coding multilayer spiking neural networks. First, by using a surrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based three-factor learning rule capable of training multilayer networks of deterministic integrate-and-fire neurons to perform nonlinear computations on spatiotemporal spike patterns. Second, inspired by recent results on feedback alignment, we compare the performance of our learning rule under different credit assignment strategies for propagating output errors to hidden units. Specifically, we test uniform, symmetric, and random feedback, finding that simpler tasks can be solved with any type of feedback, while more complex tasks require symmetric feedback. In summary, our results open the door to obtaining a better scientific understanding of learning and computation in spiking neural networks by advancing our ability to train them to solve nonlinear problems involving transformations between different spatiotemporal spike time patterns.},
   author = {Friedemann Zenke and Surya Ganguli},
   doi = {10.1162/neco_a_01086},
   issn = {1530888X},
   issue = {6},
   journal = {Neural Computation},
   month = {6},
   pages = {1514-1541},
   pmid = {29652587},
   publisher = {MIT Press Journals},
   title = {SuperSpike: Supervised learning in multilayer spiking neural networks},
   volume = {30},
   url = {https://arxiv.org/abs/1705.11146},
   year = {2018},
}
@article{Tavanaei2018,
   abstract = {In recent years, deep learning has been a revolution in the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained in a supervised manner using backpropagation. Huge amounts of labeled examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and arguably the only viable option if one wants to understand how the brain computes. SNNs are also more hardware friendly and energy-efficient than ANNs, and are thus appealing for technology, especially for portable devices. However, training deep SNNs remains a challenge. Spiking neurons' transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy, but also computational cost and hardware friendliness. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while the SNNs typically require much fewer operations.},
   author = {Amirhossein Tavanaei and Masoud Ghodrati and Saeed Reza Kheradpisheh and Timothee Masquelier and Anthony S. Maida},
   doi = {10.1016/j.neunet.2018.12.002},
   month = {4},
   title = {Deep Learning in Spiking Neural Networks},
   url = {http://arxiv.org/abs/1804.08150 http://dx.doi.org/10.1016/j.neunet.2018.12.002},
   year = {2018},
}
@article{Neftci2019,
   abstract = {Spiking neural networks are nature's versatile solution to fault-tolerant and energy efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking neural network processors attempt to emulate biological neural networks. These developments have created an imminent need for methods and tools to enable such systems to solve real-world signal processing problems. Like conventional neural networks, spiking neural networks can be trained on real, domain specific data. However, their training requires overcoming a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training spiking neural networks, and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. To that end, it gives an overview of existing approaches and provides an introduction to surrogate gradient methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.},
   author = {Emre O. Neftci and Hesham Mostafa and Friedemann Zenke},
   month = {1},
   title = {Surrogate Gradient Learning in Spiking Neural Networks},
   url = {http://arxiv.org/abs/1901.09948},
   year = {2019},
}
@article{Strubell2019,
   abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
   author = {Emma Strubell and Ananya Ganesh and Andrew McCallum},
   month = {6},
   title = {Energy and Policy Considerations for Deep Learning in NLP},
   url = {http://arxiv.org/abs/1906.02243},
   year = {2019},
}
@article{Pfeiffer2018,
   abstract = {Spiking neural networks (SNNs) are inspired by information processing in biology, where sparse and asynchronous binary signals are communicated and processed in a massively parallel fashion. SNNs on neuromorphic hardware exhibit favorable properties such as low power consumption, fast inference, and event-driven information processing. This makes them interesting candidates for the efficient implementation of deep neural networks, the method of choice for many machine learning tasks. In this review, we address the opportunities that deep spiking networks offer and investigate in detail the challenges associated with training SNNs in a way that makes them competitive with conventional deep learning, but simultaneously allows for efficient mapping to hardware. A wide range of training methods for SNNs is presented, ranging from the conversion of conventional deep networks into SNNs, constrained training before conversion, spiking variants of backpropagation, and biologically motivated variants of STDP. The goal of our review is to define a categorization of SNN training methods, and summarize their advantages and drawbacks. We further discuss relationships between SNNs and binary networks, which are becoming popular for efficient digital hardware implementation. Neuromorphic hardware platforms have great potential to enable deep spiking networks in real-world applications. We compare the suitability of various neuromorphic systems that have been developed over the past years, and investigate potential use cases. Neuromorphic approaches and conventional machine learning should not be considered simply two solutions to the same classes of problems, instead it is possible to identify and exploit their task-specific advantages. Deep SNNs offer great opportunities to work with new types of event-based sensors, exploit temporal codes and local on-chip learning, and we have so far just scratched the surface of realizing these advantages in practical applications.},
   author = {Michael Pfeiffer and Thomas Pfeil},
   doi = {10.3389/fnins.2018.00774},
   issn = {1662-4548},
   journal = {Frontiers in Neuroscience},
   month = {10},
   publisher = {Frontiers Media SA},
   title = {Deep Learning With Spiking Neurons: Opportunities and Challenges},
   volume = {12},
   year = {2018},
}
@report{Reinagel2000,
   abstract = {The amount of information a sensory neuron carries about a stimulus is directly related to response reliability. We recorded from individual neurons in the cat lateral geniculate nucleus (LGN) while presenting randomly modulated visual stimuli. The responses to repeated stimuli were reproducible, whereas the responses evoked by nonrepeated stimuli drawn from the same ensemble were variable. Stimulus-dependent information was quantified directly from the difference in entropy of these neural responses. We show that a single LGN cell can encode much more visual information than had been demonstrated previously, ranging from 15 to 102 bits/sec across our sample of cells. Information rate was correlated with the firing rate of the cell, for a consistent rate of 3.6 0.6 bits/spike (mean SD). This information can primarily be attributed to the high temporal precision with which firing probability is modulated; many individual spikes were timed with better than 1 msec precision. We introduce a way to estimate the amount of information encoded in temporal patterns of firing, as distinct from the information in the time varying firing rate at any temporal resolution. Using this method, we find that temporal patterns sometimes introduce redundancy but often encode visual information. The contribution of temporal patterns ranged from 3.4 to 25.5 bits/sec or from 9.4 to 24.9% of the total information content of the responses. Cells in the lateral geniculate nucleus of the thalamus (LGN) respond to spatial and temporal changes in light intensity within their receptive fields. The collective responses of many such cells constitute the input to visual cortex. All stimulus discrimination at the perceptual level must ultimately be supported by reliable differences in the neural response at the level of the LGN cell population. We are therefore interested in measuring the statistical discriminability of LGN responses elicited by different visual stimuli. It has been shown that the LGN can respond to visual stimuli with remarkable temporal precision (Reich et al., 1997). This implies that LGN neurons have the capability to signal information at high rates. Previous estimates of the information in LGN responses have used two general approaches. The first approach, stimulus reconstruction, relies on an explicit model of what the neuron is encoding, as well as an algorithm for decoding it (Bialek et al., 1991; Rieke et al., 1997). This method has been used to place lower bounds on the information encoded by single neurons (Rei-nagel et al., 1999) or pairs of neurons (Dan et al., 1998) in the LGN in response to dynamic visual stimuli. The second approach, the "direct" method, relies instead on statistical properties of the responses to different stimuli (the en-tropy of the responses). Because this involves only comparisons of spike trains, without reference to stimulus parameters, we need not know what features of the stimulus the cell encodes. Analysis of this type can be simplified by using a small set of stimuli and describing neural responses in terms of a few parameters, as has been done in previous studies of the LGN (Eckhorn and Pöpel, 1975; McClurkin et al., 1991). Recently, a version of the direct method has been developed that can be applied to the detailed firing patterns of neurons in response to arbitrarily complex stimuli (Strong et al., 1998). This method provides a direct measure of how much information is contained in a neural response, in the sense that the method is independent of any assumptions about what the neuron represents or how that information is represented. The information could be encoded at any temporal resolution and could involve any kind of temporal pattern. Here we apply this method to study the responses of individual LGN cells to a complex (high-entropy) temporal stimulus. Because the direct method does not constrain either the temporal resolution of the code or the role of temporal patterns, the result does not by itself tell us anything about how LGN cells encode stimuli. We therefore present two further analyses. First, we explore the temporal resolution of the neural code. Second, we introduce a measure of the contribution of temporal patterns. We distinguish three broad possibilities: (1) temporal patterns do not exist or are irrelevant to the neural code; (2) temporal patterns exist and make the neural code more redundant; or (3) temporal patterns exist and encode useful information. In our data, we find a range of results. Some cells encode information redundantly , whereas others use temporal patterns to encode visual information. In the latter case, to extract all the information from the spike trains, it would be necessary to consider temporal firing patterns; the time-varying instantaneous probability of firing would not be sufficient at any temporal resolution. MATERIALS AND METHODS Experimental Surger y and preparation. C ats were initially anesthetized with ketamine HC l (20 mg / kg, i.m.) followed by sodium thiopental (20 mg / kg, i.v., supplemented as needed and continued at 2-3 mg kg 1 hr 1 for the duration of the experiment). The animals were then ventilated through an endotracheal tube. Electrocardiograms, electroencephalograms, temperature , and expired C O 2 were monitored continuously. Animals were paralyzed with Norcuron (0.3 mg kg 1 hr 1 , i.v.). Eyes were refracted, fitted with appropriate contact lenses, and focused on a tangent screen. Electrodes were introduced through a 0.5 cm diameter craniotomy over the LGN. All surgical and experimental procedures were in accordance with National Institutes of Health and United States Department of Agriculture guidelines and were approved by the Harvard Medical Area Standing Committee on Animals. Electrical recording. Single LGN neurons in the A laminae of the LGN were recorded with plastic-coated tungsten electrodes (AM Systems, Ever-ett, WA). In some experiments, single units were recorded with electrodes of a multielectrode array (System Eckhorn Thomas Recording, Marburg, Germany). Recorded voltage signals were amplified, filtered, and passed to a personal computer running DataWave (L ongmont, C O) Discovery software , and spike times were determined to 0.1 msec resolution. Preliminary},
   author = {Pamela Reinagel and R Clay Reid},
   issue = {14},
   journal = {The Journal of Neuroscience},
   keywords = {LGN,entropy,information theory,neural coding,reliability,variability,white noise},
   note = {not related<br/><br/>},
   pages = {5392-5400},
   title = {Temporal Coding of Visual Information in the Thalamus},
   volume = {20},
   year = {2000},
}
@generic{Rucci2018,
   abstract = {Establishing a representation of space is a major goal of sensory systems. Spatial information, however, is not always explicit in the incoming sensory signals. In most modalities it needs to be actively extracted from cues embedded in the temporal flow of receptor activation. Vision, on the other hand, starts with a sophisticated optical imaging system that explicitly preserves spatial information on the retina. This may lead to the assumption that vision is predominantly a spatial process: all that is needed is to transmit the retinal image to the cortex, like uploading a digital photograph, to establish a spatial map of the world. However, this deceptively simple analogy is inconsistent with theoretical models and experiments that study visual processing in the context of normal motor behavior. We argue here that, as with other senses, vision relies heavily on temporal strategies and temporal neural codes to extract and represent spatial information.},
   author = {Michele Rucci and Ehud Ahissar and David Burr},
   doi = {10.1016/j.tics.2018.07.009},
   issn = {1879307X},
   issue = {10},
   journal = {Trends in Cognitive Sciences},
   keywords = {eye movements,microsaccade,ocular drift,retina,saccade,space perception,temporal processing,visual fixation,visual system},
   month = {10},
   pages = {883-895},
   pmid = {30266148},
   publisher = {Elsevier Ltd},
   title = {Temporal Coding of Visual Space},
   volume = {22},
   year = {2018},
}
@generic{Masquelier2011,
   abstract = {In this review, we describe our recent attempts to model the neural correlates of visual perception with biologically inspired networks of spiking neurons, emphasizing the dynamical aspects. Experimental evidence suggests distinct processing modes depending on the type of task the visual system is engaged in. A first mode, crucial for object recognition, deals with rapidly extracting the glimpse of a visual scene in the first 100 ms after its presentation. The promptness of this process points to mainly feedforward processing, which relies on latency coding, and may be shaped by spike timing-dependent plasticity (STDP). Our simulations confirm the plausibility and efficiency of such a scheme. A second mode can be engaged whenever one needs to perform finer perceptual discrimination through evidence accumulation on the order of 400 ms and above. Here, our simulations, together with theoretical considerations, show how predominantly local recurrent connections and long neural time-constants enable the integration and build-up of firing rates on this timescale. In particular, we review how a non-linear model with attractor states induced by strong recurrent connectivity provides straightforward explanations for several recent experimental observations. A third mode, involving additional top-down attentional signals, is relevant for more complex visual scene processing. In the model, as in the brain, these top-down attentional signals shape visual processing by biasing the competition between different pools of neurons. The winning pools may not only have a higher firing rate, but also more synchronous oscillatory activity. This fourth mode, oscillatory activity, leads to faster reaction times and enhanced information transfers in the model. This has indeed been observed experimentally. Moreover, oscillatory activity can format spike times and encode information in the spike phases with respect to the oscillatory cycle. This phenomenon is referred to as "phase-of-firing coding," and experimental evidence for it is accumulating in the visual system. Simulations show that this code can again be efficiently decoded by STDP. Future work should focus on continuous natural vision, bio-inspired hardware vision systems, and novel experimental paradigms to further distinguish current modeling approaches. © 2011 Masquelier, Albantakis and Deco.edu.},
   author = {Timothée Masquelier and Larissa Albantakis and Gustavo Deco},
   doi = {10.3389/fpsyg.2011.00151},
   issn = {16641078},
   issue = {JUN},
   journal = {Frontiers in Psychology},
   keywords = {Attention,Decision making,Neural coding,Neurodynamics,Oscillations,STDP,Spiking neurons,Vision},
   note = {Not bad ,it has some interesting ideas and links to some useful citations but I don't like their experiments},
   pmid = {21747774},
   title = {The timing of vision - how neural processing links to different temporal dynamics},
   volume = {2},
   year = {2011},
}
@report{,
   abstract = {The principle function of the central nervous system is to represent and transform information and thereby mediate appropriate decisions and behaviors. The cerebral cortex is one of the primary seats of the internal representations maintained and used in perception, memory, decision making, motor control, and subjective experience, but the basic coding scheme by which this information is carried and transformed by neurons is not yet fully understood. This article defines and reviews how information is represented in the firing rates and temporal patterns of populations of cortical neurons, with a particular emphasis on how this information mediates behavior and experience.},
   author = {R Christopher Decharms and Anthony Zador},
   journal = {Annu. Rev. Neurosci},
   keywords = {coding,cortex,neural signal,temporal coding},
   note = {lost the annotations I will add them again when I reread this},
   pages = {613-647},
   title = {NEURAL REPRESENTATION AND THE CORTICAL CODE},
   volume = {23},
   year = {2000},
}
@article{Falcone2019,
   abstract = {The rostromedioventral striatum is critical for behavior dependent on evaluating rewards. We asked what contribution tonically active neurons (TANs), the putative striatal cholinergic interneurons, make in coding reward value in this part of the striatum. Two female monkeys were given the option to accept or reject an offered reward in each trial, the value of which was signaled by a visual cue. Forty-five percent of the TANs use temporally modulated activity to encode information about discounted value. These responses were significantly better represented using principal component analysis than by just counting spikes. The temporal coding is straightforward: the spikes are distributed according to a sinusoidal envelope of activity that changes gain, ranging from positive to negative according to discounted value. Our results show that the information about the relative value of an offered reward is temporally encoded in neural spike trains of TANs. This temporal coding may allow well tuned, coordinated behavior to emerge.SIGNIFICANCE STATEMENT Ever since the discovery that neurons use trains of pulses to transmit information, it seemed self-evident that information would be encoded into the pattern of the spikes. However, there is not much evidence that spike patterns encode cognitive information. We find that a set of interneurons, the tonically active neurons (TANs) in monkeys' striatum, use temporal patterns of response to encode information about the discounted value of offered rewards. The code seems straightforward: a sinusoidal envelope that changes gain according to the discounted value of the offer, describes the rate of spiking across time. This temporal modulation may provide a means to synchronize these interneurons and the activity of other neural elements including principal output neurons.},
   author = {Rossella Falcone and David B. Weintraub and Tsuyoshi Setogawa and John H. Wittig and Gang Chen and Barry J. Richmond},
   doi = {10.1523/JNEUROSCI.0869-19.2019},
   issn = {15292401},
   issue = {38},
   journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
   keywords = {monkey,striatum,tonically active neurons},
   month = {9},
   pages = {7539-7550},
   pmid = {31363063},
   publisher = {NLM (Medline)},
   title = {Temporal Coding of Reward Value in Monkey Ventral Striatal Tonically Active Neurons},
   volume = {39},
   year = {2019},
}
@report{,
   author = {Nikola K Kasabov},
   title = {Springer Series on Bio-and Neurosystems 7 Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence},
   url = {http://www.springer.com/series/15821},
}
@report{,
   abstract = {Biological neurons use short and sudden increases in voltage to send information. These signals are more commonly known as action potentials, spikes or pulses. Recent neurological research has shown that neurons encode information in the timing of single spikes, and not only just in their average firing frequency. This paper gives an introduction to spiking neural networks, some biological background, and will present two models of spiking neurons that employ pulse coding. Networks of spiking neurons are more powerful than their non-spiking predecessors as they can encode temporal information in their signals, but therefore do also need different and biologically more plausible rules for synaptic plasticity. You constantly receive sensory input from your environment. You process this information, recognizing food or danger , and take appropriate actions. Not only you; anything that interacts with its environment needs to do so. Mimicking such a seemingly simple mechanism in a robot proofs to be insanely difficult. Nature must laugh at our feeble attempts; animals perform this behaviour with apparent ease. The reason for this mind-boggling performance lies in their neural structure or 'brain'. Millions and millions of neurons are interconnected with each other and cooperate to efficiently process incoming signals and decide on actions. A typical neuron sends its signals out to over 10.000 other neu-rons, making it clear to even to inexpert reader that the signal flow is rather complicated. To put it mildly: we do not understand the brain that well yet. In fact, we do not even completely understand the functioning of a single neuron. The chemical activity of the synapse already proves to be infinitely more complex than firstly assumed. However, the rough concept of how neurons work is understood: neurons send out short pulses of electrical energy as signals, if they have received enough of these themselves. This basically simple mechanism has been moulded into a mathematical model for computer use. Artificial as these computerised neurons are, we refer to them as networks of artificial neurons, or artificial neural networks. We will sketch a short history of these now; the biological background of the real neuron will be drawn in the next chapter. Generations of artificial neurons Artificial neural networks are already becoming a fairly old technique within computer science; the first ideas and models are over fifty years old. The first generation of artificial neural networks consisted of McCulloch-Pitts threshold neu-rons [15], a conceptually very simple model: a neuron sends a binary 'high' signal if the sum of its weighted incoming signals rises above a threshold value. Even though these neurons can only give digital output, they have been successfully applied in powerful artificial neural networks like multi-layer perceptrons and Hopfield nets. For example, any function with Boolean output can be computed by a multi-layer perceptron with a single hidden layer; these networks are called universal for digital computations. Neurons of the second generation do not use a step-or threshold function to compute their output signals, but a continuous activation function, making them suitable for analog in-and output. Commonly used examples of activation functions are the sigmoid and hyperbolic tangent. Typical examples of neural networks consisting of neurons of these types are feed-forward and recurrent neural networks. These are more powerful than their first generation predecessors: when equipped with a threshold function at the output layer of the network they are universal for digital computations , and do so with fewer neurons than a network of the first generation [14]. In addition they can approximate any analog function arbitrarily well, making these networks universal for analog computations. Neuron models of the first two generations do not employ individual pulses, but their output signals typically lie between 0 and 1. These signals can be seen as normalized firing rates (frequencies) of the neuron within a certain period of time. This is a so-called rate coding, where a higher rate of firing correlates with a higher output signal. Rate coding implies an averaging mechanism, as real spikes work binary: spike, or no spike, there is no intermediate. Due to such an averaging window mechanism the output value of a neuron can be calculated in iteration. After such a cycle for each neu-ron the 'answer' of the network to the input values is known. Real neurons have a base firing-rate (an intermediate frequency of pulsing) and continuous activation functions can model these intermediate output frequencies. Hence, neu-rons of the second generation are more biologically realistic and powerful than neurons of the first generation [3].},
   author = {Jilles Vreeken},
   title = {Spiking neural networks, an introduction},
}
@article{Jang2019,
   abstract = {Spiking neural networks (SNNs) are distributed trainable systems whose computing elements, or neurons, are characterized by internal analog dynamics and by digital and sparse synaptic communications. The sparsity of the synaptic spiking inputs and the corresponding event-driven nature of neural processing can be leveraged by energy-efficient hardware implementations, which can offer significant energy reductions as compared to conventional artificial neural networks (ANNs). The design of training algorithms lags behind the hardware implementations. Most existing training algorithms for SNNs have been designed either for biological plausibility or through conversion from pretrained ANNs via rate encoding. This article provides an introduction to SNNs by focusing on a probabilistic signal processing methodology that enables the direct derivation of learning rules by leveraging the unique time-encoding capabilities of SNNs. We adopt discrete-time probabilistic models for networked spiking neurons and derive supervised and unsupervised learning rules from first principles via variational inference. Examples and open research problems are also provided.},
   author = {Hyeryung Jang and Osvaldo Simeone and Brian Gardner and André Grüning},
   doi = {10.1109/MSP.2019.2935234},
   month = {10},
   title = {An Introduction to Probabilistic Spiking Neural Networks: Probabilistic Models, Learning Rules, and Applications},
   url = {http://arxiv.org/abs/1910.01059 http://dx.doi.org/10.1109/MSP.2019.2935234},
   year = {2019},
}
@report{Neftci2019,
   abstract = {Spiking neural networks are nature's versatile solution to fault-tolerant and energy efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking neural network processors attempt to emulate biological neural networks. These developments have created an imminent need for methods and tools to enable such systems to solve real-world signal processing problems. Like conventional neural networks, spiking neural networks can be trained on real, domain specific data. However, their training requires overcoming a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training spiking neural networks, and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. To that end, it gives an overview of existing approaches and provides an introduction to surrogate gradient methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.},
   author = {Emre O Neftci and Hesham Mostafa and Friedemann Zenke},
   isbn = {1901.09948v2},
   title = {Surrogate Gradient Learning in Spiking Neural Networks},
   year = {2019},
}
