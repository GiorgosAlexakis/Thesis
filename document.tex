\documentclass{report}
\usepackage[utf8]{inputenc}

\title{Spiking Neural Networks, classifying Dynamic Vision Sensor Data }
\author{Georgios Alexakis,Dimitrios Korakobounis}
\date{2021}
\usepackage{cite}
\usepackage{graphicx}
\graphicspath{{Images/}}
\begin{document}



\maketitle

\tableofcontents{}
\chapter{Introduction}
     
\section{Thesis description}

Machine learning, a subset of AI, has grown in popularity in recent years, owing largely to advancements in GPU hardware and the massive amounts of data generated by the digital age. The concepts and algorithms used in the field today have been floating around for decades, but we have not been able to use them to their full extent until now. The majority of machine learning algorithms employ a simple artificial neural network structure consisting of multiple layers of interconnected neurons. In deep learning, a term used when the number of layers is large, each level learns to transform its input data into a slightly more abstract and composite representation. In image processing, for example, lower layers may identify edges, while higher layers may identify structures relevant to humans, such as numbers, letters, or faces.

However, while deep learning networks have advanced to the point that they outperform human performance in multiple tasks, the efficiency of these networks is orders of magnitude lower compared to the human brain. Therefore it stands to reason to keep exploring the structure and inner workings of the human brain in order to increase the performance of machine learning algorithms and the hardware we use to implement them. 

This is where spiking neural networks come into play, neural networks that are far more inspired by information processing in biology than their predecessors(ANNs). The brain encodes information in sparse and asynchronous signals that are inherently processed in parallel. Deep learning neural networks process input layer by layer, and errors must be propagated backward in a non biologically plausible way. Processing information layer by layer indicates that information is not processed asynchronously. This limitation is imposed by the underlying hardware, synchronous circuits. A synchronous circuit is a digital circuit in digital electronics in which changes in the state of memory elements are synchronized by a clock signal. Learning methods in Spiking Neural Networks and a new type of computer hardware, neuromorphics, make an effort to utilize asynchronous processing.

For the reasons mentioned above, as well as the method by which we collect video data, using machine learning for video processing is one of the most computationally expensive tasks. Since standard cameras capture videos in image frames, the neural network must process all pixels each time a new frame is introduced.It appears to be much more efficient to be able to process the changing pixels asynchronously. This is why event cameras (Dynamic Vision Sensors) were developed. Event cameras are bio-inspired sensors that operate in a somewhat different way than conventional cameras. They calculate per-pixel brightness changes asynchronously rather than collecting images at a fixed time. As a result, a stream of events is produced that encodes the time, position, and sign of the brightness changes.

For the time being, these sensors are quite expensive, but thankfully, several DVS datasets have been recorded for researchers like us who want to test and develop spiking neural network machine learning algorithms in data obtained by these types of sensors.

The authors intend to introduce findings from neuroscience to readers with electrical and computer engineering backgrounds to inspire them to delve deeper into what the brain has to educate them and how this could lead to new and more sophisticated AI and why this is necessary considering the immense energy consumption of current methods. The authors first inform   the readers first about the energy requirements of today's neural networks. Then they compare the hardware that employs simple spiking neural networks,neuromorphics, with Von-Neumann computer architecture . The dissertation goes on to describe neuroscience research in great depth. The authors attempt to describe findings from neurons,synapses, dendrites, and how these form larger structures, then continue to information encoding, temporal coding of visual information, and how a brain vision system is interconnected to parts of the brain involved in learning and memory.The authors next explain methods to simulate neuron models and what learning algorithms can be used to test their efficiency and performance with software libraries based on existing machine learning libraries such as PyTorch and TensorFlow, taking into account the need to implement these findings in the industry. The authors present their experiments on Dynamic Vision Sensor Datasets in the final section.
\section{Energy requirements of current Machine Learning Models}
\section{Neuromorphics}
\chapter{Brain Inspiration}
\section{Neurons}
This chapter starts by defining the neuron, the most basic unit of the brain \cite{gerstner2014}. These cells are responsible for processing sensory feedback from the outside world, and transforming and processing electrical signals.\cite{balduzzi2013}. Ramón y Cajal´ was the first researcher to make drawings of neurons after observing them under a microscope\cite{garcialopezp2010}
.One example of these drawings is depicted in Figure.\ref{fig:neurons-ramoncajal} .For a better understanding of what a neuron is consisted of see Figure.\ref{fig:neurons-multipolar}.
\begin{figure}[htp]
    \centering
    \includegraphics[width=4cm]{Neurons-Synapses/Ramon y Cajal.jpeg}
    \caption{The rough surface of dendrites, which leave the cell laterally and upwardly, is what identifies them. The axons are small, straight lines that reach downwards with several branches to the left and right.Ramon y Cajal(1909).}
    \label{fig:neurons-ramoncajal}
\end{figure}
\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{Neurons-Synapses/Blausen_0657_MultipolarNeuron.png}
    \caption{Neuron illustration.The dendrites,axons and synaptic terminals are of interest.}
    \label{fig:neurons-multipolar}
\end{figure}
\section{Synapses}
The brain is made up of a vast network of neurons. Neurons communicate with one another through synapses, which are specialized cell junctions. Synapses are required not only for neuronal signaling and computation but also for long-term changes (synaptic plasticity) that underpin information storage, such as learning and memory, in the brain  \cite{li2003}. The contact region between two communicating neurons is defined by two distinct components: the pre-synaptic terminal and the post-synaptic target site, which are separated by a synaptic cleft .

Chemical and electrical synapses have been discovered in research, but the authors shall concentrate purely on electrical synapses. Electrical synapses cause a pre-synaptic impulse to be quickly converted into an electrical excitatory postsynaptic potential in the post-junctional cell. Activation of voltage-gated ion channels leads to the generation of action potentials if the current transmitted to the post-synaptic cell is sufficient to depolarize the membrane above a certain threshold \cite{Hormuzdi2004}. The amount of excitation in both cells, however, is not equal. A less depolarized paired partner can be excited by a more depolarized cell, and a more depolarized cell can be inhibited by a less depolarized cell.The synapse can also cause a rectifying behavior \cite{Furshpan1959}.Electrical synapses are very intriguing for researchers due to their unique ability of reciprocity as well as their ability to carry sub-threshold potentials allowing for synchronous activity of neurons.

\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{Neurons-Synapses/Neurites-and-Synapses-header.jpg}
    \caption{Synapse}
    \label{fig:synapse}
\end{figure}

It has been observed that gap junctions serve an important role in the development of the nervous system \cite{Fischbach1972}. They also produce large functional clusters of coupled neurons, which are usually organized in vertical columns spanning many cortical layers\cite{Peinado1993}\cite{Yuste1992}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=5cm]{Neurons-Synapses/angry-y-u-no.jpg}
    \caption{Synapse}
    \label{fig:spines}
\end{figure}
Since the authors are interested in how findings from brain research can be applied in computer vision machine learning applications it is necessary to understand how synapses aid in processing vision input in the mammalian brain. Inputs from pre-synaptic neurons are transmitted through tiny protrusions called spines on the post synaptic dendrites(see Fig.\ref{fig:spines}) \cite{tobias2017}.However, where an input is located on the dendritic tree, as well as whether it is triggered by similar stimuli to those that trigger its neighbors, matters, allowing for simultaneously active inputs \cite{London2005}.

For a long time, it was unclear what information each neuron receives from various parts of the visual field, and how this information relates to the visual features encoded by the neuron's spatial receptive field until recent research has provided insight. Inputs representing similar visual features from the same location in visual space were more likely to cluster on neighbouring spines.Higher-order dendritic branches often synapse inputs from visual field regions beyond the postsynaptic neuron's receptive field. When the input's receptive field is spatially displaced along the axis of the postsynaptic neuron's receptive field direction, these putative long-range inputs are more frequent and more likely to share the postsynaptic neuron's preference for oriented edges. As a result, neurons with displaced receptive fields bind preferentially when their receptive fields are co-oriented and co-axially aligned. This synaptic connectivity organization is well suited for amplifying elongated edges and thus serves as a possible "framework" for contour integration and object grouping \cite{Iacaruso2017} and provides evidence for the idea that visual space is mapped onto the dendrites in a specific manner.
\section{Information Representation and Processing in the Brain}
After reviewing the functional elements of the brain, it's time to look at how information is represented in the cerebral cortex with spike trains of neuronal populations and also how can this research provide a better methodology for machine learning research.
\subsection{Neural representation}
A message that uses the rules and structures by which a signal carries information (neural code) to serve a function is called a representation. Its content and function make up the representation. The signal that carries a sensory input is the representation's content, while its function is the processing of the input sensory signal (cognitive process) and the cognitive process' outcome.

Organisms produce an internal mirror that correctly reflects their environment \cite{Koch1994}. The input signal must then have projections that enable it to play a role in the organism's activities to adapt to its surroundings. The mechanism of transforming representations is called representation. Computation is a complement to representation. The knowledge transformations that representations serve would be impossible without computational processes. As neuronal representations are projected from one cortical area to another, they are often transformed. Neural circuits represent information as they operate, which they then transform using computational processes \cite{decharms2000}. 

\subsection{Neural Code}
How this information is exactly represented and processed in the brain is still an open question to neuroscience but several hypotheses exist. The rate-coding( average number of spikes over some time interval) hypothesis \cite{ Salzman1992} \cite{Tovee1993} contends that the information is carried by the mean firing rate, while the temporal-coding   hypothesis \cite{Bair1996} \cite{buracas1998} \cite{Rucci2018} claims that the exact timing of the spikes is what represents the information carried.
\bibliographystyle{IEEEtran}
\bibliography{library}
\end{document}
