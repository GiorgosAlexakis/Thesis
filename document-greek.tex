\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[english,greek]{babel}
\usepackage[LGR,T1]{fontenc}
\usepackage[font=medium]{caption}
\usepackage{amsmath}
\usepackage{extsizes}
\usepackage[export]{adjustbox}


\title{Spiking Neural Networks, classifying Dynamic Vision Sensor Data }
\author{Georgios Alexakis,Dimitrios Korakobounis}
\date{2021}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{wrapfig}
\graphicspath{{Images/}}
\begin{document}



\maketitle
\selectlanguage{greek}
\tableofcontents{}
\chapter{Εισαγωγή}

\section{Περιγραφή Διπλωματικής}
Η μηχανική μάθηση, ένα υποσύνολο της τεχνητής νοημοσύνης, έχει αυξηθεί σε δημοτικότητα τα τελευταία χρόνια, εξαιτίας κυρίως των εξελίξεων στο υλικό υπολογιστών που σχετίζονται με γραφικά (κάρτες γραφικών) και των τεράστιων όγκων δεδομένων που δημιουργούνται από την ψηφιακή εποχή. Οι έννοιες και οι αλγόριθμοι που χρησιμοποιούνται στον τομέα σήμερα κυκλοφορούν εδώ και δεκαετίες, αλλά δεν είχαμε τη δυνατότητα να τις χρησιμοποιήσουμε μέχρι τώρα. Η πλειοψηφία των αλγορίθμων μηχανικής μάθησης χρησιμοποιεί μια απλή δομή τεχνητού νευρωνικού δικτύου που αποτελείται από πολλαπλά στρώματα διασυνδεδεμένων νευρώνων. Στη βαθιά μάθηση, ένας όρος που χρησιμοποιείται όταν ο αριθμός των επιπέδων είναι μεγάλος, κάθε επίπεδο μαθαίνει να μετατρέπει τα δεδομένα εισόδου του σε μια ελαφρώς πιο αφηρημένη και σύνθετη αναπαράσταση. Στην επεξεργασία εικόνας, για παράδειγμα, τα χαμηλότερα στρώματα μπορεί να εντοπίζουν ακμές, ενώ τα υψηλότερα στρώματα μπορεί να προσδιορίζουν δομές σχετικές με τον άνθρωπο, όπως αριθμούς, γράμματα ή πρόσωπα.  

Ωστόσο, ενώ τα δίκτυα βαθιάς μάθησης έχουν προχωρήσει σε σημείο που υπερτερούν της ανθρώπινης απόδοσης σε πολλαπλές δοκιμασίες, η αποδοτικότητα αυτών των δικτύων είναι τάξεις μεγέθους χαμηλότερη σε σύγκριση με τον ανθρώπινο εγκέφαλο. Συνεπώς, είναι λογικό να συνεχίσουμε να διερευνάμε τη δομή και την εσωτερική λειτουργία του ανθρώπινου εγκεφάλου, προκειμένου να αυξήσουμε την απόδοση των αλγορίθμων μηχανικής μάθησης και του υλικού που χρησιμοποιούμε για την εφαρμογή τους.

Τα νευρωνικά δίκτυα αιχμών \textlatin{(Spiking Neural Networks)} που εμπνέονται πολύ περισσότερο από την επεξεργασία πληροφοριών στη βιολογία από τα προηγούμενα (ANN) μπορεί να αποτελέσουν μια αποδοτικότερη λύση. Ο εγκέφαλος κωδικοποιεί πληροφορίες σε αραιά και ασύγχρονα σήματα που είναι εγγενώς επεξεργασμένα παράλληλα. Τα νευρωνικά δίκτυα βαθιάς εκμάθησης επεξεργάζονται την είσοδο στρώμα προς στρώμα και τα λάθη πρέπει να διαδίδονται προς τα πίσω με μη βιολογικά εύλογο τρόπο. Η επεξεργασία πληροφοριών στρώμα προς στρώμα υποδηλώνει ότι οι πληροφορίες δεν υποβάλλονται σε ασύγχρονη επεξεργασία. Αυτός ο περιορισμός επιβάλλεται από το υποκείμενο υλικό, τα σύγχρονα κυκλώματα. Ένα σύγχρονο κύκλωμα είναι ένα ψηφιακό κύκλωμα στα ψηφιακά ηλεκτρονικά μέσα στο οποίο οι αλλαγές στην κατάσταση των στοιχείων μνήμης συγχρονίζονται βάση ενός ρολογιού. Οι μέθοδοι εκμάθησης στα  νευρωνικά δίκτυα αιχμών και ένας νέος τύπος υλικού υπολογιστών, τα νευρομορφικά υπολογιστικά συστήματα, κάνουν μια προσπάθεια να χρησιμοποιήσουν ασύγχρονη επεξεργασία.
Για τους λόγους που αναφέρθηκαν παραπάνω, καθώς και τη μέθοδο με την οποία συλλέγουμε δεδομένα βίντεο, η χρήση μηχανικής μάθησης για επεξεργασία βίντεο είναι μία από τις πιο υπολογιστικά δαπανηρές δοκιμασίες. Δεδομένου ότι οι τυπικές κάμερες καταγράφουν βίντεο σε πλαίσια εικόνας, το νευρωνικό δίκτυο πρέπει να επεξεργάζεται όλα τα εικονοστοιχεία κάθε φορά που εισάγεται ένα νέο πλαίσιο.Θα ήταν πιο οικονομικό αν μπορούσαμε να επεξεργαζόμαστε τα μεταβαλλόμενα εικονοστοιχεία ασύγχρονα . Αυτός είναι ο λόγος για τον οποίο αναπτύχθηκαν σένσορες δυναμικής όρασης \textlatin{(Dynamic Vision Sensors)} ονομάζονται και κάμερες συμβάντων \textlatin{(Event Cameras)}. Οι κάμερες συμβάντων είναι αισθητήρες βιολογικής έμπνευσης που λειτουργούν με κάπως διαφορετικό τρόπο από τις συμβατικές κάμερες. Υπολογίζουν τις αλλαγές φωτεινότητας ανά pixel ασύγχρονα αντί να συλλέγουν εικόνες ανα συγκεκριμένες χρονικές στιγμές. Ως αποτέλεσμα, παράγεται μια ροή γεγονότων που κωδικοποιεί την χρονική στιγμή, τη θέση και το πρόσημο των αλλαγών φωτεινότητας.

Προς το παρόν, αυτοί οι αισθητήρες είναι αρκετά ακριβοί, αλλά , έχουν καταγραφεί αρκετά σύνολα δεδομένων \textlatin{DVS} για ερευνητές όπως εμάς που θέλουν να δοκιμάσουν και να αναπτύξουν αλγόριθμους μηχανικής μάθησης νευρωνικών δικτύων σε δεδομένα που λαμβάνονται από αυτούς τους τύπους αισθητήρων.

Σκοπεύουμε να εισαγάγουμε ευρήματα από τη νευροεπιστήμη σε αναγνώστες με ηλεκτρολογικό και μηχανολογικό υπόβαθρο για να τους εμπνεύσουμε να εμβαθύνουν σε πράγματα που ο εγκέφαλος έχει να τους μάθει και πώς αυτό θα μπορούσε να οδηγήσει σε νέα και πιο εξελιγμένη τεχνητή νοημοσύνη και γιατί αυτό είναι απαραίτητο λαμβάνοντας υπόψη την τεράστια κατανάλωση ενέργειας των τρεχουσών μεθόδων. Ενημερώνουμε πρώτα τους αναγνώστες για τις ενεργειακές απαιτήσεις των σημερινών νευρωνικών δικτύων. Η διατριβή περιγράφει την έρευνα της νευροεπιστήμης με απλό τρόπο. Προσπαθούμε να περιγράψουμε ευρήματα από νευρώνες, συνάψεις, δενδρίτες και πώς αυτά σχηματίζουν μεγαλύτερες δομές, στη συνέχεια συνεχίζουμε με την κωδικοποίηση πληροφοριών, τη χρονική κωδικοποίηση των οπτικών πληροφοριών και πώς το οπτικό σύστημα του εγκεφάλου είναι διασυνδεδεμένο. Στη συνέχεια συγκρίνουμε το υλικό που χρησιμοποιεί απλά νευρικά δίκτυα, νευρομορφικά, με αρχιτεκτονική υπολογιστή  \textlatin{Von-Neumann}. Έπειτα εξηγούμε τις μεθόδους και τις εξισώσεις που χρησιμοποιούνται για την προσομοίωση μοντέλων νευρώνων και ποιοι αλγόριθμοι μάθησης μπορούν να χρησιμοποιηθούν για να ελέγξουν την αποτελεσματικότητα και την απόδοσή τους με βιβλιοθήκες λογισμικού που βασίζονται σε υπάρχουσες βιβλιοθήκες μηχανικής εκμάθησης όπως οι \textlatin{PyTorch} και \textlatin{TensorFlow}, λαμβάνοντας υπόψη την ανάγκη εφαρμογής αυτών των ευρημάτων στη βιομηχανία. Στην τελευταία ενότητα παρουσιάζουμε τα πειράματά μας σχετικά με τα σύνολα δεδομένων αισθητήρα δυναμικής όρασης και εξηγούμε λεπτομερώς τα αποτελέσματα.

\section{Τρέχοντα ενεργειακά προβλήματα μηχανικής μάθησης}
Οι τεράστιοι υπολογιστικοί πόροι που απαιτούνται για την εκπαίδευση των μοντέλων βαθειών νευρωνικών δικτύων καταναλωνούν σημαντική ποσότητα ηλεκτρικής ενέργειας. Η εκπαίδευση συνήθως διαρκεί μερικές ημέρες, αν όχι εβδομάδες ανάλογα με το μέγεθος των δεδομένων εκπαίδευσης. Ως αποτέλεσμα, λόγω του κόστους υλικού, ηλεκτρικής ενέργειας καί χρόνου εκπαίδευσης, αυτά τα μοντέλα είναι οικονομικά και περιβαλλοντικά δαπανηρά για εκπαίδευση και κατασκευή, αφήνοντας υψηλό αποτύπωμα άνθρακα. Επιπλέον, η εκπαίδευση είναι ακόμα αργή σε σύγκριση με αυτό που είναι ικανός ένας ανθρώπινος εγκέφαλος. Οι ερευνητές σε μια έρευνα \cite{Strubell2019},υπολόγισαν την κατανάλωση ενέργειας για να εκπαιδεύσουν ένα μεγάλα νευρωνικά μοντέλα επεξεργασία φυσικής γλώσσας και έκαναν συγκρίσεις . Ένα μοντέλο μπορεί να καταναλώσει περισσότερα από μισό εκατομμύριο λίβρες \textlatin{CO}\textsubscript{2}, που είναι σχεδόν 5 φορές περισσότερο \textlatin{CO}\textsubscript{2} το οποίο παράγει ενα αυτοκίνητο στην διάρκεια ζωής του (see Fig \ref{fig:energy-requirements}). Η ενέργεια που χρειάζεται για να εκπαιδεύσουμε το νευρωνικό μοντέλο \textlatin{BERT} είναι περίπου ο ίδιος με μια δια-αμερικανική πτήση. Ένας άνθρωπος, από την άλλη πλευρά, παράγει περίπου 11.000 κιλά \textlatin{CO}\textsubscript{2} κάθε χρόνο. Οι ερευνητές πρέπει να δώσουν έμφαση στο σχεδιασμό αποδοτικότερων μοντέλων και υλικού υπό το φως αυτών των πραγματικοτήτων. 

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{Introduction/energy-requirements.PNG}
    \caption{}
    \label{fig:energy-requirements}
\end{figure}
\begin{figure}[htp]
    \centering
    \includegraphics[width=14cm]{Introduction/costs.PNG}
    \caption{}
    \label{fig:costs}
\end{figure}

Η μεταφορά ενός υπάρχοντος μοντέλου σε μια νέα δοκιμασία ή η δημιουργία νέων μοντέλων από την αρχή θα απαιτήσει πρόσθετους πόρους. Το κόστος υπολογισμού νέφους για μερικά από τα πιο ακριβή μοντέλα γλωσσών καθώς και οι χρόνοι εκπαίδευσης φαίνονται στο Σχήμα \ref{fig:costs}. Ένα άλλο ζήτημα είναι ότι τα περισσότερα από τα μοντέλα που συζητήθηκαν σε αυτήν τη έρευνα \cite{Strubell2019} δημιουργήθηκαν εκτός ακαδημαϊκού χώρου. Οι πρόσφατες εξελίξεις στην κατέστησαν δυνατές χάρη στην εμπορική πρόσβαση σε υπολογισμούς μεγάλης κλίμακας. Λόγω του υψηλού κόστους εκπαίδευσης, ωστόσο, οι υπολογιστικοί πόροι δεν είναι προσβάσιμοι σε όλους, εμποδίζοντας φτωχότερες χώρες να αναπτύξουν δικά τους νευρωνικά μοντέλα. 


\chapter{Έπνευση απο τον βιολογικό εγκέφαλο}
Η κλίμακα του εγκεφάλου των θηλαστικών είναι τεράστια. Κάθε ανθρώπινος εγκέφαλος που περιλαμβάνει περίπου 25 χιλιάδες νευρώνες και 10 επί 10\textsuperscript{8} συνάψεις ανά κυβικό εκατοστό \cite{nguyen2013} στο νεοφλοιό. Ο νεοφλοιός είναι το εξωτερικό στρώμα του εγκεφάλου. Υπολογίζεται σε ηλικία 20 ετών, ο νεοφλοιός συνολικά, περιέχει περίπου 25 επί 10\textsuperscript{9} νευρώνες και σχεδόν 180 τρισεκατομμύρια συνάψεις! Θα διερευνήσουμε κάποια ευρήματα από έρευνες και πειράματα του εγκεφάλου και θα δούμε πώς αυτά μπορούν να μας βοηθήσουν να αναπτύξουμε αποτελεσματικότερες  μεθόδους μηχανικής μάθησης.

\section{Δομικά στοιχεία εγκεφάλου}
\subsection{Νευρώνες}
Αυτό το κεφάλαιο ξεκινά καθορίζοντας τον νευρώνα, την βασικότερο κύτταρο του εγκεφάλου \cite{gerstner2014}. Αυτά τα κύτταρα είναι υπεύθυνα για την επεξεργασία αισθητηριακών ανατροφοδοτήσεων από τον έξω κόσμο και τον μετασχηματισμό και επεξεργασία ηλεκτρικών σημάτων. \cite{balduzzi2013}. Ο \textlatin{Ramón y Cajal´} ήταν ο πρώτος ερευνητής που έκανε σχέδια νευρώνων αφού τα παρατήρησε κάτω από μικροσκόπιο \cite{garcialopezp2010}
.Ένα παράδειγμα αυτών των σχεδίων απεικονίζεται στο σχήμα \ref{fig:neurons-ramoncajal}. Για καλύτερη κατανόηση του τι αποτελείται από έναν νευρώνα, δείτε το σχήμα. \ref{fig:neurons-multipolar}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=4cm]{Neurons-Synapses/Ramon y Cajal.jpeg}
    \caption{Η τραχιά επιφάνεια των δενδριτών, που φεύγουν από το κύτταρο πλάγια και προς τα πάνω, είναι αυτό που τα προσδιορίζει. Οι νευράξονες είναι μικρές, ευθείες που φτάνουν προς τα κάτω με αρκετούς κλάδους αριστερά και δεξιά.\textlatin{Ramon y Cajal},1909.}
    \label{fig:neurons-ramoncajal}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{Neurons-Synapses/Blausen_0657_MultipolarNeuron.png}
    \caption{Εικονογράφηση νευρώνων. Παρατηρήστε τους δενδρίτες,  νευράξονες και τα συναπτικά τερματικά (\textlatin{synaptic terminals}) }
    \label{fig:neurons-multipolar}
\end{figure}
\subsection{Συνάψεις}
Ο εγκέφαλος αποτελείται από ένα τεράστιο δίκτυο νευρώνων. Οι νευρώνες επικοινωνούν μεταξύ τους μέσω συνάψεων, οι οποίες είναι εξειδικευμένες κυτταρικές συνδέσεις. Οι συνάψεις απαιτούνται όχι μόνο για νευρωνική σηματοδότηση και υπολογισμό αλλά και για μακροπρόθεσμες αλλαγές (συναπτική πλαστικότητα) που υποστηρίζουν την αποθήκευση πληροφοριών, όπως η μάθηση και η μνήμη, στον εγκέφαλο \cite{li2003}. Η περιοχή επαφής μεταξύ δύο νευρώνων που επικοινωνούν ορίζεται από δύο περιοχές: τον προ-συναπτικό τερματικό (\textlatin{pre-synaptic terminal}) και τη μετα-συναπτική θέση στόχου (\textlatin{post-synaptic target site}), τα οποία χωρίζονται από μια συναπτική σχισμή (\textlatin{synaptic cleft}) .

Χημικές και ηλεκτρικές συνάψεις έχουν ανακαλυφθεί στην νευροεπιστήμη, αλλά θα επικεντρωθούμε στις ηλεκτρικές συνάψεις. Οι ηλεκτρικές συνάψεις προκαλούν μια προ-συναπτική ώθηση (\textlatin{pre-synaptic impulse}) να μετατραπεί γρήγορα σε ηλεκτρικό διεγερτικό μετασυναπτικό δυναμικό (\textlatin{excitatory postsynaptic potential}) στο μετα-διασταυρούμενο κύτταρο (\textlatin{post-junctional cell}). Η ενεργοποίηση των καναλιών ιόντων με τάση οδηγεί στη δημιουργία δυναμικών δράσης (\textlatin{action potentials})  εάν το ρεύμα που μεταδίδεται στο μετασυναπτικό κύτταρο είναι αρκετό για να αποπολώσει τη μεμβράνη πάνω από ένα ορισμένο όριο \cite{Hormuzdi2004}. Ωστόσο, η ποσότητα διέγερσης και στα δύο κύτταρα δεν είναι ίση. Ένας λιγότερο αποπολωμένος συζευγμένος σύντροφος μπορεί να διεγερθεί από ένα πιο εκπολωμένο κύτταρο (\textlatin{depolarized cell}) και ένα πιο εκπολωμένο κύτταρο μπορεί να ανασταλεί από ένα λιγότερο εκπολωμένο κύτταρο. Η σύναψη μπορεί επίσης να προκαλέσει μια συμπεριφορά διόρθωσης (\textlatin{rectifying behavior}) \cite{Furshpan1959}. Οι ηλεκτρικές συνάψεις είναι πολύ ενδιαφέρουσες για τους ερευνητές στη μοναδική τους ικανότητα αμοιβαιότητας καθώς και στην ικανότητά τους να μεταφέρουν ηλεκτρικά δυναμικά κάτω από το κατώφλι που επιτρέπουν τη συγχρονισμένη δραστηριότητα των νευρώνων.

\begin{figure}[htp]
    \centering
    \includegraphics[width=18cm]{Neurons-Synapses/Neurites-and-Synapses-header.jpg}
    \caption{Synapse}
    \label{fig:synapse}
\end{figure}

Έχει παρατηρηθεί ότι οι διασταυρώσεις διαστήματος (\textlatin{gap junctions}) εξυπηρετούν σημαντικό ρόλο στην ανάπτυξη του νευρικού συστήματος \cite{Fischbach1972}. Παράγουν επίσης μεγάλες λειτουργικές ομάδες συζευγμένων νευρώνων, οι οποίες συνήθως οργανώνονται σε κάθετες στήλες που εκτείνονται σε πολλά φλοιώδη στρώματα \cite{Peinado1993} \cite{Yuste1992} \ref{fig:verticalcolumns} .

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Neurons-Synapses/verticalcolumns.jpg}
    \caption{Κάθετες στήλες νευρώνων \cite{molnar2020}}
    \label{fig:verticalcolumns}
\end{figure}
\begin{figure}[htp]
    \centering
    \includegraphics[width=5cm]{Neurons-Synapses/angry-y-u-no.jpg}
    \caption{Σύναψη}
    \label{fig:spines}
\end{figure}

Δεδομένου ότι ενδιαφερόμαστε για το πώς τα ευρήματα από την έρευνα του εγκεφάλου μπορούν να εφαρμοστούν σε εφαρμογές μηχανικής μάθησης υπολογιστικής όρασης, είναι απαραίτητο να κατανοήσουμε πώς οι συνάψεις βοηθούν στην επεξεργασία των οπτικών ερεθισμάτων στον εγκέφαλο. Οι εισροές προ-συναπτικών νευρώνων μεταδίδονται μέσω μικροσκοπικών προεξοχών που ονομάζονται αγκάθια (\textlatin{spines}) στους μετασυναπτικούς δενδρίτες (βλ. Εικ. \ref{fig:spines}) \cite{tobias2017}. Ωστόσο, όπου μια είσοδος βρίσκεται και στο δενδριτικό δέντρο το αν ενεργοποιείται από παρόμοια ερεθίσματα με εκείνα που πυροδοτούν τους γείτονές του, έχει σημασία, επιτρέποντας ταυτόχρονα ενεργές εισροές \cite{London2005}.

Για μεγάλο χρονικό διάστημα, ήταν ασαφές ποιες πληροφορίες λαμβάνει κάθε νευρώνας από διάφορα μέρη του οπτικού πεδίου και πώς αυτές οι πληροφορίες σχετίζονται με τα οπτικά χαρακτηριστικά που κωδικοποιούνται από το χωρικό δεκτικό πεδίο του νευρώνα έως ότου πρόσφατη έρευνα κατέδειξε περισσοτέρα για τον μηχανισμό αυτό. Οι είσοδοι που αντιπροσωπεύουν παρόμοια οπτικά χαρακτηριστικά από την ίδια θέση στον οπτικό χώρο ήταν πιο πιθανό να συγκεντρωθούν σε γειτονικά αγκάθια. Οι δενδριτικοί κλάδοι υψηλότερης τάξης συχνά συνάπτουν εισόδους από περιοχές οπτικού πεδίου πέρα από το δεκτικό πεδίο του μετασυναπτικού νευρώνα. Όταν το δεκτικό πεδίο της εισόδου μετατοπίζεται χωρικά κατά μήκος του άξονα της κατεύθυνσης του δέκτη του μετασυναπτικού νευρώνα (\textlatin{receptive field}), αυτές οι φερόμενοι είσοδοι μεγάλης εμβέλειας είναι πιο συχνές και πιο πιθανό να μοιραστούν την προτίμηση του μετασυναπτικού νευρώνα για προσανατολισμένες ακμές. Ως αποτέλεσμα, οι νευρώνες με εκτοπισμένα δεκτικά πεδία συνδέονται κατά προτίμηση όταν τα δεκτικά πεδία τους είναι προσανατολισμένα και ομοαξονικά ευθυγραμμισμένα. Αυτή η οργάνωση συναπτικής συνδεσιμότητας είναι κατάλληλη για την "ενίσχυση" επιμήκων ακμών,δηλαδή τονίζονται περισσότερο τα χαρακτηριστικά του αντικειμενού που παρατηρείται , και χρησιμεύει ως ένα πιθανό "πλαίσιο" για ολοκλήρωση περιγράμματος (αναφερόμαστε στην ικανότητα του οπτικού συστήματος να συνδέει ασύνδετα τοπικά στοιχεία σε συνεκτικά σχήματα \cite{persike2016}) και στην ομαδοποίηση αντικειμένων του οπτικού συστήματος \cite{Iacaruso2017} και παρέχει στοιχεία για την ιδέα ότι ο οπτικός χώρος "αποτυπώνεται" στους δενδρίτες με συγκεκριμένο τρόπο και όχι τυχαία.


\section{Αναπαράσταση και επεξεργασία πληροφοριών στον εγκέφαλο}
Μετά την ανασκόπηση των λειτουργικών στοιχείων του εγκεφάλου, ήρθε η ώρα να δούμε πώς οι πληροφορίες αντιπροσωπεύονται στον εγκεφαλικό φλοιό με τρένα ακίδας νευρωνικών πληθυσμών επιγραμματικά και επίσης πώς μπορεί αυτή η έρευνα να προσφέρει μια καλύτερη μεθοδολογία για την έρευνα μηχανικής μάθησης. 
\subsection{Νευρωνική αναπάρασταση}

Ένα μήνυμα που χρησιμοποιεί τους κανόνες και τις δομές με τις οποίες ένα σήμα μεταφέρει πληροφορίες (νευρωνικός κώδικας) για να εξυπηρετήσει μια συνάρτηση μπορούμε να την αποκαλέσουμε αναπαράσταση.Η αναπαράσταση αποτελείται απο περιεχόμενο και η λειτουργία του . Το σήμα που μεταφέρει μια αισθητηριακή είσοδο είναι το περιεχόμενο της αναπαράστασης, ενώ η λειτουργία του είναι η επεξεργασία του αισθητηριακού σήματος εισόδου (γνωστική διαδικασία) και το αποτέλεσμα της γνωστικής διαδικασίας αυτής.

Οι οργανισμοί παράγουν έναν εσωτερικό αντίκτυπο που αναπαριστά τα ερεθίσματα του περιβάλλοντος τους \cite{Koch1994}. Το σήμα εισόδου πρέπει στη συνέχεια να έχει προβολές που του επιτρέπουν να παίζει ρόλο στις δραστηριότητες του οργανισμού για να προσαρμοστεί στο περιβάλλον του. Ο μηχανισμός μετασχηματισμού των παραστάσεων ονομάζεται αναπαράσταση. Ο υπολογισμός είναι συμπλήρωμα της αναπαράστασης. Οι μετασχηματισμοί γνώσης που εξυπηρετούν οι αναπαραστάσεις θα ήταν αδύνατοι χωρίς υπολογιστικές διαδικασίες. Καθώς οι νευρωνικές αναπαραστάσεις προβάλλονται από τη μια φλοιώδη περιοχή στην άλλη, συχνά μεταμορφώνονται. Τα νευρωνικά κυκλώματα περιέχουν πληροφορίες στην μορφή αιχμών , τις οποίες στη συνέχεια μετατρέπουν χρησιμοποιώντας υπολογιστικές διαδικασίες \cite{decharms2000}. 

\subsection{Nευρωνική Κωδικοποίηση}

Ο τρόπος με τον οποίο οι πληροφορίες αντιπροσωπεύονται και επεξεργάζονται  ακριβώς στον εγκέφαλο εξακολουθεί να είναι ένα ανοιχτό πρόβλημα για τη νευροεπιστήμη, αλλά υπάρχουν αρκετές υποθέσεις. Η υπόθεση κωδικοποίησης ρυθμού (μέσος αριθμός αιχμών σε κάποιο χρονικό διάστημα) \cite{Salzman1992} \cite{Tovee1993} υποστηρίζει ότι οι πληροφορίες μεταφέρονται από τον μέσο ρυθμό πυροδότησης, ενώ η υπόθεση χρονικής κωδικοποίησης \cite{Bair1996} \cite{buracas1998} \cite{Rucci2018} ισχυρίζεται ότι ο ακριβής χρόνος των αιχμών είναι αυτός που κωδικοποιεί τις πληροφορίες που μεταφέρονται.
Η διάκριση μεταξύ αυτών των δύο υποθέσεων είναι το διάστημα που χρησιμοποιείται για τον υπολογισμό των αιχμών, που σημαίνει ότι στην χρονική κωδικοποίηση το διάστημα είναι τόσο μικρό που μετράται μόνο μία ακίδα. Το διάστημα για ένα πείραμα που χρησιμοποιεί κωδικοποίηση ρυθμού αποφασίζεται με βάση τα χρονικά περιθώρια που θεωρούνται σημαντικά για μια δεδομένη κατάσταση, όπως το πόσο γρήγορα μετατοπίζεται το ερέθισμα, ο χρόνος ολοκλήρωσης μιας νευρικής μεταβλητής, η διαδικασία κωδικοποίησης ή η σχετική χρονική κλίμακα συμπεριφοράς.

Μια σειρά απο αιχμές (\textlatin{spike train}) είναι ένα σύνθετο σήμα που μεταβάλλεται στο χρόνο και αποτελείται από πολλαπλές αιχμές που παράγονται από τον νευρώνα σε συγκεκριμένες χρονικές στιγμές. Η υπόθεση κωδικοποίησης ρυθμού θεωρεί σημαντικό μόνο έναν αριθμό, το μέσο ποσοστό αυτών των αιχμών. Παρόλο που η κωδικοποίηση και η αποκωδικοποίηση (η νευρωνική απόκριση αποκωδικοποιείται με την καταμέτρηση των αιχμών και το ερέθισμα κωδικοποιείται ρυθμίζοντας τον ρυθμό πυροδότησης ανάλογο με την τιμή κάποιας παραμέτρου ερεθίσματος) είναι απλές, αυτή η υπόθεση φαίνεται να είναι μια υπεραπλούστευση. Σε πειράματα συμπεριφοράς, οι χρόνοι απόκρισης είναι συχνά πολύ σύντομοι για να επιτρέψουν αργό χρονικό μέσο όρο \cite{thorpe1996}. Σε ένα άλλο πείραμα σε έναν οπτικό νευρώνα μύγας, το ερέθισμα που εξαρτάται από το χρόνο ανακατασκευάστηκε με επιτυχία από τους χρόνους πυροδότησης νευρώνων \cite{Bialek1991}. Υπάρχουν επίσης ενδείξεις συγχρονισμένων χρονικών συσχετίσεων μεταξύ παλμών διαφορετικών νευρώνων \cite{Lestienne1996}. 

\subsubsection{Νευρωνικές Συνθέσεις(\textlatin{Neuron Assemblies})}

Ο \textlatin{Yoshio Sakurai} στην έρευνά του \cite{sakurai1999} αναφέρει ότι οι μεμονωμένοι νευρώνες είναι ανεπαρκείς ως βασικός μηχανισμός κωδικοποίησης. Όπως αναφέραμε νωρίτερα, ο εγκέφαλος περιέχει έναν αμέτρητο αριθμό συνάψεων, που σημαίνει ότι κάθε νευρώνας λαμβάνει σήματα από χιλιάδες άλλους νευρώνες. Αυτό κάνει έναν μόνο νευρώνα να έχει μια ασταθή συμπεριφορά πυροδότησης καθώς το δυναμικό της μεμβράνης του υφίσταται μεγάλες διακυμάνσεις.

Επιπλέον, μεμονωμένοι νευρώνες έχουν ελάχιστη μόνο επιρροή σε άλλους νευρώνες και δεν μπορούν να παράγουν αρκετά ισχυρή μετάδοση για να προκαλέσουν αιχμές στους επόμενους νευρώνες όταν πρόκειται για λειτουργικές μεταδόσεις μεταξύ νευρώνων. Σύμφωνα με θεωρητικά επιχειρήματα, ο αριθμός των νευρώνων του εγκεφάλου είναι ανεπαρκής για να συλλάβει τον τεράστιο όγκο δεδομένων που επεξεργάζεται ένα ζώο κατά τη διάρκεια της ζωής του. Επειδή οι συνδυασμοί και οι διαμορφώσεις των στοιχείων παράγουν νέα στοιχεία, υπάρχει μια συνδυαστική έκρηξη, ο αριθμός των στοιχείων πληροφοριών είναι σχεδόν απεριόριστος. Η αναπαράσταση ενιαίων νευρώνων αυτών των στοιχείων είναι επίσης άβολη για τη συσχέτιση και τη διάκριση μεταξύ στοιχείων πληροφοριών, υποδεικνύοντας τον βαθμό ομοιότητας ή διαφοράς μεταξύ των στοιχείων ή τη δημιουργία νέων εννοιών και ιδεών από διαφορετικά στοιχεία. Έτσι, η υπόθεση του ενιαίου νευρώνα φαίνεται απίθανο να εξηγήσει την κωδικοποίηση και επεξεργασία του εγκεφάλου. Η δραστηριότητα του συνόλου ενός πληθυσμού νευρώνων φαίνεται πιο ικανή να κωδικοποιεί πληροφορίες στον εγκέφαλο. Αυτή η υπόθεση ονομάζεται κωδικοποίηση συνόλου.

Για να σας δώσω έναν πιο ακριβή ορισμό: Ένα σύνολο νευρώνων σχηματίζει μια λειτουργική ομάδα εάν οι παρορμήσεις ... είναι συντονισμένες, στο βαθμό που οι χρονικές τους σχέσεις είναι διατεταγμένες, τουλάχιστον πιθανολογικά, σε χαρακτηριστικά πρότυπα είναι αυτό που ορίζει μια παλαιότερη ερευνητική εργασία μια νευρωνικές σύνθεση \cite{gerstein1978}. Ένας νευρώνας δεν χρειάζεται να συμμετέχει σε μία μόνο λειτουργική ομάδα. μπορεί να είναι μέρος πολλαπλών νευρωνικών συνόλων σε διαφορετικούς χρόνους. Επιπλέον, οι νευρώνες που αποτελούν μια λειτουργική ομάδα δεν χρειάζεται να βρίσκονται σε άμεση συναπτική επαφή ή σε κοντινή απόσταση ανατομικά. Οι νευρωνικές συνθέσεις μπορούν επίσης να σχηματιστούν όταν οι νευρώνες μοιράζονται εξωτερική δραστηριότητα εισόδου. Επειδή θα εστιάσουμε στην οπτική επεξεργασία σε αυτή τη διατριβή, ας εξετάσουμε πώς οι νευρωνικές συνθέσεις μπορούν να παίξουν ρόλο στα διάφορα στάδια της οπτικής επεξεργασίας.

Ορισμένες διαδικασίες που λαμβάνουν χώρα κατά τη διάρκεια μιας περιόδου μεταγεννητικής ανάπτυξης έχουν ως αποτέλεσμα την επιλεκτική σταθεροποίηση των συνδέσεων μεταξύ των νευρωνικών στοιχείων που συχνά έχουν συνδεδεμένη δραστηριότητα, επιτρέποντας την τροποποίηση της συνδεσιμότητας με βάση τις λειτουργικές παραμέτρους \cite{singer1998}. Στο στάδιο εισόδου των νευρωνικών συνόλων του ραβδωτού φλοιού μπορούν να συμβάλλουν στη βελτιστοποίηση της αντιστοίχισης μεταξύ των παραστάσεων των δύο ματιών. Βοηθά επίσης στην κατασκευή νευρωνικών αναπαραστάσεων για συχνά επαναλαμβανόμενες διαμορφώσεις χαρακτηριστικών (\textlatin{feature configurations}) σε μεταγενέστερο επίπεδο επεξεργασίας συμμετέχοντας στο σχηματισμό επιλεκτικών συνδέσεων μεταξύ φλοιών στηλών (βλ. Επίσης ενότητα 2.1.2 για τις φλοιώδεις στήλες).

Η ενισχυμένη συσχετισμένη δραστηριότητα μπορεί να προκύψει από την επιλεκτική ενίσχυση των συναπτικών συνδέσεων μεταξύ των νευρώνων. Μια τέτοια επιλεκτική ενίσχυση αναφέρεται στην προσωρινή δυναμική που διατηρείται κατά τη διάρκεια μιας νευρικής δραστηριότητας, παρά στη μόνιμη αλλαγή της συναπτικής αποτελεσματικότητας μεταξύ των νευρώνων. Ως αποτέλεσμα, η προαναφερθείσα δυναμική διαμόρφωση σχετιζόμενων με γεγονότα και συμπεριφορές συσχετισμένων δραστηριοτήτων σε νευρώνες υποστηρίζει την ιδέα ότι οι νευρώνες μπορούν να συνδεθούν γρήγορα σε μια λειτουργική ομάδα προκειμένου να επεξεργαστούν τις απαιτούμενες πληροφορίες ενώ παραμένουν αποσυνδεδεμένοι από ταυτόχρονα ενεργοποιημένες ανταγωνιστικές ομάδες. Τέλος, ας εξετάσουμε σημαντικές ιδιότητες της κωδικοποίησης αυτής:

\begin{enumerate}
    \item Επικαλυπτόμενη κωδικοποίηση στοιχείων πληροφοριών. Ο ίδιος νευρώνας είναι μέρος πολλών διαφορετικών νευρωνικών συνθέσεων. 
    \item Αραιή κωδικοποίηση πληροφοριών. Οποιαδήποτε μεμονωμένη νευρωνική σύνθεση περιέχει ένα μικρό υποσύνολο όλων των νευρώνων στον φλοιό.  
    \item Δυναμική κατασκευή και ανακατασκευή. Οι νευρωνικές συνθέσεις είναι χρονικά σύνολα νευρώνων που συνδέονται μεταξύ τους με ευέλικτες λειτουργικές συνάψεις.
    \item Δυναμική διατήρηση. Η ενεργοποίηση μιας νευρωνικής σύνθεσης θα επιμείνει για ένα διάστημα μέσω ανατροφοδότησης λόγω των διεγερτικών συνάψεων μεταξύ των νευρώνων.
    \item Δυναμική ολοκλήρωση. Η ενεργοποίηση ενός αρκετά μεγάλου υποσυνόλου μιας νευρωνικής σύνθεσης έχει ως αποτέλεσμα την ενεργοποίηση ολόκληρης της νευρωνικής σύνθεσης.
\end{enumerate}
Η υπόθεση κωδικοποίησης νευρωνικής σύνθεσης είναι συναρπαστική, καθώς μπορεί να επιβεβαιωθεί από πολλές ερευνητικές μελέτες στον ανθρώπινο εγκέφαλο.Για παράδειγμα, διαπιστώθηκε λειτουργική αλληλεπικάλυψη μεμονωμένων νευρώνων και δυναμική συσχέτισης μεταξύ πολλών νευρώνων στη λειτουργική μνήμη και τη μνήμη αναφοράς.
Οι νευρωνικές συνθέσεις αποτελούνται από μεμονωμένους νευρώνες που σχετίζονται με μια λειτουργία,αυτο μπορεί να περιγράψει την δυνατότητα διπλής κωδικοποίησης από νευρωνικές συνθέσεις και τον ενιαίο νευρώνα τους. Τα πειραματικά δεδομένα για αυτά τα πειράματα μπορούν να βρεθούν στο \cite{sakurai1999}. Η κατανόηση των σημαντικών αποτελεσμάτων πίσω από αυτά τα πειράματα μπορεί να επιτρέψει πιο αποτελεσματικές και μεθόδους μηχανικής μάθησης που προσομοιάζουν περισσότερο τον εγκέφαλο.
\begin{figure}[htp]
    \includegraphics[width=10cm,right]{Brain Inspiration/neuron assemblies/assemblies.PNG}
    \caption{Οπτικοποίηση ορισμένων ιδιοτήτων των νευρωνικών συνθέσεων.}
    \label{fig:assemblies}
\end{figure}
\subsection{Ταλαντώσεις}
Οι ταλαντώσεις που συμβαίνουν σε διαφορετικές συχνότητες θεωρούνται ως λειτουργικά συναφή σήματα του εγκεφάλου και η έρευνα υποστηρίζει ότι οι ταλαντώσεις που σχετίζονται με γεγονότα γεφυρώνουν το χάσμα μεταξύ μεμονωμένων νευρώνων και νευρωνικών συνθέσεων \cite{Basar2000}. Η έρευνα υποθέτει επίσης ότι τα επιλεκτικά κατανεμημένα συστήματα ταλάντωσης δέλτα, θήτα, άλφα και γάμμα λειτουργούν ως ηχηρά δίκτυα επικοινωνίας μέσω μεγάλου πληθυσμού νευρώνων. Αντανακλούν τον χρονικό συγχρονισμό της συμπεριφοράς των νευρωνικών πληθυσμών και έχουν εμπλακεί ως ένας μηχανισμός που επιλέγει υποσύνολα νευρώνων για περαιτέρω συλλογική επεξεργασία και ενδεχομένως ως ένας μηχανισμός αναπαράστασης των ερεθισμάτων, επειδή μπορούν να εμφανίσουν εξάρτηση από μια εργασία ή απο ένα ερέθισμα \cite{Singer1995} \cite{Singer1999}.

\section{Συναπτική Πλαστικότητα}
Η συναπτική πλαστικότητα είναι μια διαδικασία που τροποποιεί τη συνδεσιμότητα των νευρώνων που επηρεάζεται από την ποσότητα διέγερσης μεταξύ τους (Ενότητα 2.2 για να δείτε πώς το επιτρέπει αυτό μια σύναψη). Ο νευροεπιστήμονας \textlatin{Shatz} το περιέγραψε ως "κύτταρα που πυροδοτούνται μαζί, συνδέονται μαζί"  (\textlatin{"cells that fire together, wire together"}) \cite{shatz1992}. Πιο συγκεκριμένα, εάν ένα από τα κύτταρα είναι συστηματικά ενεργό λίγο πριν από το άλλο, η πυροδότηση του πρώτου μπορεί να έχει αιτιώδη σχέση με τη πυροδότηση ενός δεύτερου, το οποίο μπορεί αργότερα να ανακαλέσει με την ενίσχυση των συνδέσεων, έτσι ορίζουν οι νευροεπιστήμονες τη συναπτική πλαστικότητα. Ο συγχρονισμός είναι σημαντικός αφού μπορεί να αποκαλύψει την αιτιότητα και μπορεί επίσης να χρησιμεύσει ως ένα χρονικό σχήμα κωδικοποίησης σε χρονική κλίμακα χιλιοστών του δευτερολέπτου.

Στα μέσα της δεκαετίας του 1990,κυριαρχούσε η ιδέα του \textlatin{Hebb} ότι η συμπτωματική δραστηριότητα σε συνδεδεμένους νευρώνες είναι αυτό που έχει σημασία στην στην πλαστικότητα . Συνειδητοποιήθηκε ότι οι συναπτικές συνδέσεις του εγκεφάλου έχουν μηχανισμούς που θα τους έκαναν ιδιαίτερα ευαίσθητους στο χρονισμό και επιβεβαιώθηκε σε πολυάριθμες μελέτες  \cite{markram1995}\cite{markram1997}\cite{Gerstner1996} ,οι νευρωεπιστήμονες ονόμασαν τα ευρήματα τους ως Εκμάθηση Πλαστικότητας που εξαρτάται από το χρόνο (\textlatin{Spike Time Dependent Plasticity -  STDP}). Με την ενίσχυση εκείνων των εισροών που προέβλεψαν τη δική τους δραστηριότητα ακίδας, ένας νευρώνας που περιέχεται σε ένα νευρωνικό δίκτυο μπορεί να επιλέξει τους γειτονικούς νευρώνες που "αξίζει να ακούσει" με το \textlatin{STDP}. Ο επίμαχος νευρώνας, από την άλλη πλευρά, δίνει λιγότερη προσοχή στους γύρω νευρώνες που δεν το κάνουν. Ως αποτέλεσμα, ένας νευρώνας μπορεί να ενσωματώσει εισόδους με προγνωστική δύναμη και να τις μεταφράσει σε μια σημαντική πρόβλεψη εξόδου, ακόμη και αν η έννοια δεν είναι πλήρως κατανοητή από τον νευρώνα. Ως αποτέλεσμα, το \textlatin{STDP} παρέχει έναν πολύ βασικό και κομψό μηχανισμό για τη σωστή "σύνδεση" των νευρώνων στον εγκέφαλο \cite{Markram2012}.


\chapter{Νευρομορφικά Υπολογιστικά Συστήματα}

Ο νόμος του \textlatin{Moore}, ο οποίος προέβλεψε εκθετική αύξηση του αριθμού των τρανζίστορ που θα μπορούσαν να γίνουν σε ένα μόνο μικροτσίπ, οδήγησε τις εξελίξεις στην τεχνολογία μικροτσίπ. Η εκθετική χρονική σταθερά είναι μικρή, διπλασιάζεται κάθε 18 μήνες. Ο νόμος του \textlatin{Moore} "εφαρμόστηκε" κυρίως μέσω της μείωσης του μεγέθους των τρανζίστορ, καθώς οσο τα τρανζίστορ CMOS γίνονται μικρότερα, γίνονται φθηνότερα, γρηγορότερα και πιο ενεργειακά αποδοτικά.
Τα Νευρομορφικά Υπολογιστικά Συστήματα περιλαμβάνουν ένα ευρύ φάσμα τεχνικών επεξεργασίας πληροφοριών, όλες διακριτές από τα συμβατικά συμβατικά συστήματα υπολογιστών λόγω κάποιου βαθμού νευροβιολογικής έμπνευσης. Η θεωρία στην οποία στηρίζονται τα Νευρομορφικά μπορεί να εντοπιστεί στο θεμελιώδες έργο του \textlatin{Carver Mead} στο \textlatin{Caltech} στα τέλη της δεκαετίας του 1980. Αυτό το πρώιμο έργο ενέπνευσε άλλους να συνεχίσουν να αναπτύσσουν νευρομορφικές συσκευές και οι προαναφερθείσες εξελίξεις στην τεχνολογία \textlatin{VLSI} βοήθησαν τη συνεχή επέκταση στο μέγεθος και τη λειτουργικότητα των νευρομορφικών συσκευών \cite{furber2016}.

Τα τρέχοντα ψηφιακά σχέδια υπολογιστών γενικής χρήσης παρέχουν την "ανοσία" θορύβου και την προβλέψιμη συμπεριφορά για την οποία είναι γνωστή η μηχανή \textlatin{Turing}. Η βιολογία παραιτείται από τον ντετερμινισμό υπέρ της αποδοτικότητας, ο οποίος θα μπορούσε να ενδιαφέρει μελλοντικούς μηχανικούς υπολογιστών που εργάζονται σε συστήματα όπως συστήματα όρασης ρομπότ, όπου η απόλυτη ακρίβεια είναι αδύνατο να επιτευχθεί και η ενεργειακή απόδοση είναι κορυφαία προτεραιότητα. Τα Νευρομορφικά Υπολογιστικά Συστήματα στοχεύουν στην εξαγωγή ή μίμηση της πολυπλοκότητας του ανθρώπινου εγκεφάλου και των αρχών λειτουργίας του σε πιο αφηρημένες μεθόδους που μπορούν να εφαρμοστούν σε ένα υπολογιστικό σύστημα αυτού του τύπου. Τα νευρομορφικά υπολογιστικά συστήματα τηρούν την αρχή του κατανεμημένου υπολογισμού, δηλαδή έχουν μεγάλη ποσότητα μικρών υπολογιστικών «πυρήνων» ανάλογων με νευρώνες συνδεδεμένους σε δίκτυα με κάποιο βαθμό επιτρεπόμενης ευελιξίας συνδεσιμότητας. 

Τα τελευταία χρόνια, εμφανίστηκε ένας αριθμός νευρομορφικών συστημάτων μεγάλης κλίμακας, χρησιμοποιώντας τον τεράστιο πόρο τρανζίστορ που είναι τώρα διαθέσιμος σε ένα μόνο μικροτσίπ και, σε μια περίπτωση, ολόκληρο δίσκος πυριτίου. Οι δυνατότητες της τεχνολογίας συνδυάζονται με κλιμακούμενες αρχιτεκτονικές για να επιτρέψουν την αύξηση των νευρομορφικών δυνατοτήτων για την υποστήριξη νευρωνικών δικτύων με εκατομμύρια νευρώνες και δισεκατομμύρια συνάψεις. Οι υπολογιστικοί νευροεπιστήμονες μπορούν τώρα να εξετάσουν την ανάπτυξη μοντέλων ολόκληρου του εγκεφάλου των πλασμάτων που κυμαίνονται από έντομα έως μικροσκοπικά θηλαστικά ή μεγάλες υπο-περιοχές του ανθρώπινου εγκεφάλου. Τα ίδια συστήματα παρέχουν επίσης πλατφόρμες ικανές να υποστηρίξουν νέες κλίμακες γνωστικής αρχιτεκτονικής. Μερικά από τα πιο αξιοσημείωτα παραδείγματα είναι τα ακόλουθα.
\subsubsection{\textlatin{IBM TrueNorth}}
Το τσιπ \textlatin{IBM TrueNorth}  βασίζεται σε κατανεμημένα ψηφιακά νευρωνικά μοντέλα που στοχεύουν σε γνωστικές εφαρμογές σε πραγματικό χρόνο. Το τσιπ είναι ένα πολύ μεγάλο, \textlatin{CMOS} τσιπ 28 νανόμετρων 5,4 εκατομμυρίων τρανζίστορ που ενσωματώνει 4096 νευροσυναπτικούς πυρήνες όπου κάθε πυρήνας περιλαμβάνει 256 νευρώνες ο καθένας με 256 συναπτικές εισόδους. Η διασταυρούμενη έξοδος συνδέει το μοντέλο του ψηφιακού νευρώνα, το οποίο εφαρμόζει μια μορφή αλγορίθμου \textlatin{integrate-and-fire} με 23 παραμετροποιήσιμες παραμέτρους που μπορούν να προσαρμοστούν για να αποδώσουν μια ποικιλία διαφορετικών συμπεριφορών και ψηφιακές ψευδοτυχαίες πηγές χρησιμοποιούνται για τη δημιουργία στοχαστικών συμπεριφορών, διαμορφώνοντας συναπτικές συνδέσεις, κατώφλι νευρώνων και διαρροή νευρώνων \cite{cassidy2013}. Οι έξοδοι των γεγονότων ακίδας του νευρώνα κάθε πυρήνα ακολουθούν ατομικά διαμορφώσιμες διαδρομές από σημείο σε σημείο στην είσοδο ενός άλλου πυρήνα, οι οποίες μπορεί να είναι στον ίδιο ή ένα διαφορετικό τσιπ \textlatin{TrueNorth}. Όταν η έξοδος ενός νευρώνα πρέπει να συνδέεται με δύο ή περισσότερους νευροσυναπτικούς πυρήνες, ο νευρώνας διπλασιάζεται στον ίδιο πυρήνα (βλ. Σχήμα \ref{fig:truenorth}) . Ο ντετερμινιστικός χαρακτήρας του ψηφιακού μοντέλου διασφαλίζει ότι όλα τα αντίγραφα παράγουν πανομοιότυπα τρένα ακίδας (\textlatin{spike trains}) \cite{furber2016}.
\begin{figure}[htp]
    \centering
    \includegraphics[width=14cm]{Neuromorphics/truenorth.PNG}
    \caption{Οι επικοινωνίες του \textlatin{TrueNorth}  βασίζονται σε συνδέσμους από σημείο σε σημείο που μεταφέρουν αιχμές από έναν μόνο νευρώνα σε έναν μόνο νευροσυναπτικό πυρήνα, όπου οι αιχμές μπορούν να συνδεθούν με οποιονδήποτε ή όλους τους 256 νευρώνες του πυρήνα. Για παράδειγμα εδώ, ο αριστερότερος νευρώνας του πυρήνα 1 συνδέεται με τον πυρήνα 3. Ο 2ος και ο 3ος νευρώνας του πυρήνα 1 αντιγράφουν ο ένας τον άλλο για να συνδεθούν με τους πυρήνες 2 και 3 και ο καθένας κάνει μία σύνδεση. }
    \label{fig:truenorth}
\end{figure}

\subsubsection{\textlatin{SpiNNaker}}
Το έργο \textlatin{SpiNNaker} \cite{furber2014} έχει αναπτύξει έναν μαζικά παράλληλο ψηφιακό υπολογιστή του οποίου η υποδομή επικοινωνίας είναι εμπνευσμένη από το στόχο της μοντελοποίησης μεγάλης κλίμακας νευρωνικών δικτύων σε βιολογικό πραγματικό χρόνο με συνδέσεις παρόμοιες με αυτές του εγκεφάλου. Το μεγαλύτερο σύστημα \textlatin{SpiNNaker} που χρησιμοποιείται αυτή τη στιγμή έχει 1.000.000 πυρήνες. Ωστόσο, το \textlatin{SpiNNaker} δεν μοιάζει με άλλα νευρομορφικά συστήματα. Χρησιμοποιεί μικρούς ακέραιους πυρήνες (προσαρμοσμένα τσιπ) που προορίζονται για ενσωματωμένες εφαρμογές για κινητά. 
\begin{figure}[htp]
    \centering
    \includegraphics[width=14cm]{Neuromorphics/spinnaker.PNG}
    \caption{Συστήματα \textlatin{SpiNNaker} σε μικρή κλίμακα. Το σύστημα 4 κόμβων (72 πυρήνων) (αριστερά) τροφοδοτείται από υποδοχή USB και είναι ιδανικό για μάθηση και μικρές εργασίες, όπως εργασίες σε ρομπότ. Το σύστημα 48 κόμβων (864 πυρήνων) (δεξιά) είναι το βασικό δομικό στοιχείο των μεγαλύτερων υπολογιστών και μπορεί να χρησιμοποιηθεί για μεγαλύτερες εφαρμογές}
    \label{fig:spinnaker}
\end{figure}

Το ύφασμα επικοινωνίας του \textlatin{SpiNNaker} έχει σχεδιαστεί για την αποστολή μεγάλου όγκου μικρών πακέτων δεδομένων (δηλαδή αιχμές νευρώνων) σε πολλούς προορισμούς σύμφωνα με στατιστικά διαμορφωμένες διαδρομές πολλαπλής διανομής \cite{plana2011}.
Ο σχεδιασμός του \textlatin{SpiNNaker} βασίζεται σε ένα μικρό πλαστικό πακέτο 300 \textlatin{bga} (συστοιχία πλέγματος) το οποίο ενσωματώνει ένα προσαρμοσμένο τσιπ επεξεργασίας και ένα τυπικό τσιπ μνήμης 128 \textlatin{Mbyte SDRAM}. Το τσιπ επεξεργασίας, σχεδιασμένο σε τεχνολογία \textlatin{CMOS 130 nm}, περιέχει 18 πυρήνες επεξεργαστή \textlatin{ARM968}, ο καθένας με 32 \textlatin{Kbytes} μνήμης εντολών και 64 \textlatin{Kbytes} μνήμης δεδομένων, δρομολογητή πακέτων πολλαπλών εκπομπών και διάφορα στοιχεία υποστήριξης \cite{painkras2013} \cite{furber2016 }.

*****
Κάθε νευρώνας σε ένα κοινό \textlatin{ANN}, όπως τα \textlatin{DNN} και τα \textlatin{CNN}, μπορεί να αναπαρασταθεί ως ένας αριθμός που υπολογίζεται όταν ο νευρώνας λαμβάνει μια μήτρα εισόδου. Ο νευρώνας ακίδας ενός \textlatin{SNN} είναι, για άλλη μια φορά, μια αριθμητική τιμή που υπολογίζεται όταν οι αιχμές εισόδου έρχονται σε ένα χρονικό παράθυρο. Δεν έχουμε σύγχρονη είσοδο και, ως εκ τούτου, σύγχρονη έξοδο σε αυτήν την περίπτωση. Στις περισσότερες περιπτώσεις (μοντέλα), η έξοδος του νευρώνα είναι μια αύξηση του δυναμικού στο ακόλουθο στρώμα νευρώνων. Αυτή η άνοδος συμβαίνει όταν πληρούνται συγκεκριμένα κριτήρια στον νευρώνα και είναι η κύρια αιτία του ευρέος φάσματος μοντέλων νευρώνων.
****
\begin{figure}[htp]
    \centering
    \includegraphics[width=14cm]{Neuromorphics/overview.PNG}
    \caption{Επισκόπηση των νευρομορφικών τσιπς και σύγκριση με τον ανθρώπινο εγκέφαλο. Έχουμε ακόμα πολύ δρόμο για να φτάσουμε την αποδοτικότητα του ανθρώπινου εγκεφάλου.}
    \label{fig:overview}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=14cm]{Neuromorphics/photonics.PNG}
    \caption{Σύγκριση νευρομορφικών πλατφορμών υλικού. Η υπεροχή των νευρομορφικών υπολογιστικών συστημάτων ως προς την απόδοση είναι εμφανής. Τα μελλοντικά φωτονικά νευρομορφικά συστήματα θα μπορούσαν να προσφέρουν οαόμη καλύτερες επιδόσεις, αλλά τα φωτονικά δεν αποτελούν μέρος αυτής της διπλωματικής εργασίας.\cite{shastri2018} }
    \label{fig:overview}
\end{figure}
\selectlanguage{english}
\chapter{Spiking Neural Networks}
In this chapter, we will shift our attention from the biological brain to the artificial one. Spiking Neural Networks differ from the common Artificial Neural Networks that have been developed throughout the last century, as stated before, mainly in the temporal dimension of the input data. This difference creates the need for the development of neuron models such that they can decode and extract information from the temporal data. Moreover, most of the datasets for training and testing ANNs that currently exist, consist of non-temporal data. Such datasets can't be given as input in an asynchronous model as a SNN. To overcome this problem, many algorithms of conversion have been developed, some of them will be presented in this chapter.
\section{Neuron Models}
\subsection{The Hodgkin - Huxley model}
In 1952, Hodgkin and Huxley published 4 papers regarding how the neurons work\cite{Johnson2017}. They performed experiments on a giant axon of a squid and developed the following model. Through their tests, they found that the ionic movement from tree ions, Sodium (Na\textsuperscript{+}), Potassium (K\textsuperscript{+}) and a leak current mainly consisting of Chlorine ions (Cl\textsuperscript{-}), controlled by two voltage-dependent channels (Sodium and Potassium channels) are responsible for the current flow of the neuron. While the neuron is at rest, inside the neuron, a high concentration of negatively charged ions creates a voltage difference with the exterior of the neuron, which is positively charged. When the voltage reaches a certain threshold, Sodium and Potassium pumps open, moving ions in and out of the cell respectively. This ionic movement results in a current that moves as a spike through the axon to the next neuron.

The Hodgkin-Huxley model is represented as an electrical circuit shown in figure 3.1. The voltage denoted as V\textsubscript{m} represents the voltage across the cell membrane. We can see that an input current I\textsubscript{m} can charge the capacitor C\textsubscript{m} or it can leak through the rest of the channels. The potential of each of the ions is different so it is used one different battery for each one of the ions. It is worth noting that the Sodium battery E\textsubscript{Na\textsuperscript{+}} is oriented in reverse compared to the rest of the ion batteries and this is justified as the Sodium ions move out of the membrane. The arrows in the resistors of the Sodium and Potassium channel denote that these resistors are though as non static values and describe the pumps of the biological model.

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{Neurons-Synapses/HH model circuit.png}
    \caption{The Hodgkin-Huxley model represented as an electrical circuit.}
    \label{fig:neurons-multipolar}
\end{figure}

In order to analyze the circuit, the static values of the Chlorine channel and the leak channel can be calculated with Kirchhoff's law as one channel, named Leak channel, with a steady valued battery and resistance. Applying the Kirchhoff's current law in the circuit we get the following equation: \[I(t) = \sum_{k}I_k\]
Replacing each current equation with its voltage equivalent, arises a first order differential equation due to the capacitor C\textsubscript{m}. For each of the Sodium, Potassium and Leak channels, the corresponding conductance has to be evaluated. Hodgkin and Huxley found that two types of pumps are responsible for the movement of Sodium ions and one pump type for the Potassium. For this reason, they added the parameters m, h and n to describe the control of the pumps in the ion movement. Through numerical experiments they arrived at the following result describing the potential of the cell membrane

\begin{equation}
C_m\frac{dV_m}{dt} = I_m - (g_{Na^+}m^3h(V_m+E_{Na^+}) + g_{K^+}n^4(V_m-E_{K^+}) + g_L(V_m-E_L))
\end{equation}


where the values m, h and n are voltage dependent and are described by the following differential equations:
\begin{equation}
\frac{dm}{dt}=\alpha_m(V_m)(1-m)+\beta_m(V_m)m
\end{equation}
\begin{equation}
\frac{dh}{dt}=\alpha_h(V_m)(1-h)+\beta_h(V_m)h
\end{equation}
\begin{equation}
\frac{dn}{dt}=\alpha_n(V_m)(1-n)+\beta_n(V_m)n
\end{equation}
The \(\alpha_m(V_m)\), \(\alpha_h(V_m)\), \(\alpha_n(V_m)\), \(\beta_m(V_m)\), \(\beta_h(V_m)\), \(\beta_n(V_m)\) are voltage dependent functions that define the behavior of the m, h and n variable and, in total, the cell membrane potential. Solving each equation, we get an exponential solution with an exponent constant \(\tau_m\), \(\tau_h\) and \(\tau_n\) respectively. In figures 3.2a and 3.2b is shown the membrane voltage that occurs through the above set of equations and the values of the time constants as a function of potential. This difference indicates the existence of fast and slow ion gates.

\begin{figure}[htp]
    \centering
    \subfloat[Hodgkin-Huxley model membrane's voltage trajectory]
    {\includegraphics[width=5cm]{Neurons-Synapses/HodgkinHuxley_output.png}\label{fig:f1}}
    \hfill
    \subfloat[time constants as a function of potential]
    {\includegraphics[width=5cm]{Neurons-Synapses/time-constants-Hodgkin-Huxley-model.png}\label{fig:f2}}
    \caption{fig:Hodgkin-Huxley model dependencies}
\end{figure}

Given exact values at the voltage-dependent variables of them, n and h functions, we can simulate the voltage of the cell membrane and study its behavior. In their work \cite{NelsonM} Nelson M. and Rinzel J. used a GENESIS tutorial squid \cite{squid} to generate multiple forms of a spike. The Hodgkin Huxley model has been developed since 1952 to adapt to the findings of the behavior of neurons and this book \cite{gerstner2014} presents extended information about the biological model of the neuron and the mathematical adaptation of the Hodgkin Huxley model.

The Hodgkin Huxley model pioneered the field of neuronal dynamics in the aspect of neuroscience but it also initiated the research and development of Spiking Neural Networks. From an engineering perspective, there have been many successful attempts to implement a neuron using the aforementioned model in hardware with FPGA as in \cite{Levi2018} which results in a possible communication between electrical signals from a living organism to an artificial structure and, with a development of a SNN, cooperation between them. There have also been developed multilayered Spiking Neural Networks with the Hodgkin Huxley model of the neurons and been trained for tasks like edge detection and pattern classification as shown in this \cite{Yedjour2017} and this \cite{pattern2016} research work respectively with promising results.

Despite its innovative description of a neuron, the model has been criticized for its efficiency in describing the complex behavior of the many different types of neurons. These criticisms arise from weaknesses of the model such as its inability to account for events that can affect the neuron's state\cite{limit1993}. From engineering perspective, its main drawback for the development of complex SNNs is its high computational complexity. It has been proven \cite{reduction1997} that the Hodgkin Huxley model can be described, with sufficient accuracy, as a single-variable threshold model. Engineers shifted their attention to developing simpler threshold models so as to be able to create more complex structures which led to the development of neuron models such as the ones that follow.

\medskip

\subsection{Leaky Integrate-and-Fire Model: LIF}

The LIF model was first proposed in 1907 \cite{Brunel2007} and presents the neuron as a leaky integrating unit. The LIF model, in contrast with the HH, is a threshold model, that is, it emits an output spike when the input voltage reaches a predefined threshold. The equivalent electrical circuit of the model is a RC circuit with an input current I\textsubscript{inject}. The input currents are typically spikes (\(\delta\)-Dirac like functions) that, due to the capacitor C, increase the voltage of the unit by the same value. The resistor, or as shown in Figure 3.3 the conductor g\textsubscript{leak}, is responsible for the neuron's voltage leakage, while the source E\textsubscript{m} accounts for the neuron's voltage in idle state. When the threshold is reach, the unit emits a spike and the voltage returns to the initial(idle) state.

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{Neurons-Synapses/Simplied-circuit-of-the-LIF-neuron-model.png}
    \caption{The Leaky Integrate-and-Fire model as an electrical circuit}
    \label{fig:lif-circuit}
\end{figure}

\vspace{5mm}

Analyzing the circuit the following equation is derived, describing the potential of the unit through time. The solution of the homogeneous equation (i\textsubscript{inject}=0) is an exponential decay. The constant \(\tau\textsubscript{m}=C/g\textsubscript{leak}\) is the time constant of the exponential decay and is determined by the aforementioned factors, the conductor and the capacitor of the circuit. Figure 3.4 shows the response of the model in a spike train input.

\vspace{5mm}

\begin{equation}
\tau\textsubscript{m}\frac{dV\textsubscript{m}(t)}{dt}=-(V\textsubscript{m}-E\textsubscript{m})+\frac{i\textsubscript{inject}}{g\textsubscript{leak}}
\end{equation}

\vspace{5mm}

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Neurons-Synapses/The-illustration-of-Leaky-Integrate-and-Fire-LIF-neuron-dynamics-The-pre-spikes-are.png}
    \caption{Voltage of neuron's potential and its response. Each of the input signal is multiplied by its weight and then summed to act on the neuron's potential. When the neuron's membrane potential reaches the threshold, it produces a spike and the voltage is reset to its idle state.}
    \label{fig:lif-neuron}
\end{figure}

\medskip

The simplicity of the model has drawn the attention of researchers both for software and hardware implementation of the neuron. It has been used for a wide variety of usages. In 2003 the leaky integrate-and-fire model was used to model the cochlea and detect sound features\cite{sound2003}, while more common, for ANNs, tasks, such as pattern recognition and image segmentation, have been tackled using either single LIF model neurons (for pattern recognition)\cite{pattern2007} or SNNs that consists of such neuron models (image segmentation)\cite{Chaturvedi2012}. Doutsi E. et al. in their work\cite{Doutsi2021} used the LIF model to transform the input signal to spike trains. This extended research over this model led researchers to view the model itself as a subject for study and much work has been done in improving the model itself, like in\cite{Mullowney2008}. From the point of view of the hardware implementation, SNNs based on the model mentioned above have already been proposed or developed and trained for various purposes such as\cite{Liu2019} and \cite{Chu2015}, exploiting the advantages of SNNs. The increasing research development of electronic components, like memristors, that favor the development of such networks\cite{Yang2020}, has created a explode in the creation of neuromorphic, and even photonic architectures\cite{Nahmias2013}, that promise low power consumption\cite{Liu2019},\cite{Chatterjee2019}, and high compacted architectures\cite{Rozenberg2019} for developing LIF model-based SNNs. 

The drawback of the model emanates from its simplicity. It has been proposed that, compared to the HH model, the LIF model could be less tolerant to noise while resembles less the biological neuron\cite{subthreshold2005}. Furthermore, compared to the results of the state-of-the-art ANNs and machine learning algorithms, SNNs do not always present satisfactory results, such as in this study\cite{SVM2014} where a Leaky Integrate-and-Fire SNN model is compared with an SVM.

\medskip

\subsection{Izhikevich Model}

In 2003 Eugene M. Izhikevich published an article proposing a new model of neurons\cite{Izhikevich2003}. The goal was to provide a model with mathematical simplicity and biological plausibility, combining features from the Leaky Integrate-and-Fire model and the Hodgkin-Huxley model. The resulting model would be useful to model biological neurons while could be easily implemented, from an engineering perspective. The mathematical description is presented in the following equations. Like the LIF model, it is a threshold model, where different values of the parameters could be inputted to simulate a variety of neurons.

\begin{equation}
    \frac{dv}{dt}(t) = 0.04v^2+5v+140-u+I(t)
\end{equation}
\begin{equation}
\frac{du}{dt}(t) = a(bv-u)
\end{equation}
\begin{equation}
if \:u\geq30mV \: then \: \left\{
\begin{array}{ll}
     v = c &  \\
     u = u+d & 
\end{array}
\right.
\end{equation}

The \(v\) value represents the membrane potential of the neuron while the \(u\) value represents a membrane recovery variable. In his work, Izhikevich showed that manipulating the a, b, c and d parameters, can produce a wide variety of neuronal responses. Later, in 2004 he stated\cite{Izhikevich2004} that the Izhikevich model ranks high in combining high biological plausibility and low implementation cost, amongst the existing models of its era. 

Indeed, in the last decade, the Izhikevich model has been implemented in integrated circuits exploiting recent developments of the field. Multiple techniques that compete with each other such as MNIN\cite{Haghiri2018} and CORDIC\cite{Elnabawy2018} promise lower computational cost and error performance. The model has been tested in classical Artificial Intelligence tasks, either implemented in its own hardware, such as in this paper\cite{Rice2009}, where an FPGA was developed to represent an Izhikevich model based Spiking Neural Network and was tested in character recognition, or in software that simulates the response of the model. Single neurons have been tested in non-linear pattern recognition problems\cite{Antonio2010} showing the ability of an individual neuron in classifying multiple patterns. One clever exploitation of this ability is presented here\cite{luna-a2019} where the multilayer perceptron network used for the classification of a typical CNN is replaced by an Izhikevich neuron. As a result, this network achieves similar scores with typical CNNs while reducing the training time.

Despite the promising statements from the creator of the model, in recent years, it has been criticized. Reviews and comparisons with other models (such as LIF and HH) show minuscule, if any, advantages. In 2014, Michael J. Skocik and Lyle N. Long compared the computational cost of the Izhikevich, LIF and Hodgkin-Huxley model using multiple arithmetic methods (so as to find the best implementation of each model)\cite{Skocik2014}. Their results showed that the Izhikevich model is comparable with the HH model in terms of computational cost, while the LIF model is, as expected the best out of the three. In 2017, Sergio Valadez-Godínez et al.\cite{Godinez2017} compared the same neuron models in terms of accuracy and cost if different fire rates. They also concluded, that in most cases, the Izhikevich model gave bad results, being less efficient than the HH model, while being more computationally expensive than the LIF model. A more recent study of 2020 from the same researchers\cite{Valadez-Godinez2020} presents and sums up the problems of the Izhikevich model that been proposed in the bibliography.

\medskip

\subsection{Spike Response Model - SRM}
The Spike Response Model was developed, as an idea, in a series of papers in the last decade of the 20th century, but the name was first introduced in this paper \cite{Gerstner1993} of 1993. In this paper, Wulfram Gerstner et.al. present the mathematical model of the SRM neuron along with simulation results. The model resembles the Integrate-and-Fire model (IF model), with the difference that the membrane potential is dependent on various linear kernels that act on the incoming spikes. Furthermore, the threshold in this model is time-dependent in contrast to the Integrate-and-Fire model. In this book \cite{gerstner2014}, the authors dive into an extensive description and explanation of the SRM and its similarity with the IF model. The following set of equations (3.9), (3.10) describe the membrane potential and the threshold value of the model, respectively. The aforementioned kernels \(\eta(), \kappa()\) and \(\theta_1()\) can be interpreted as linear response filters acting on the incoming or the postsynaptic spikes. In more detail, the \(\kappa\) kernel is the linear response of the membrane for the incoming spike and it integrates the input current through time. The \(\eta\) kernel corresponds to the action potential of the neuron. This is better understood by the fact that the kernel affects the membrane potential and it is dependent on the \(t^f\) variable, which resembles the firing time of the neuron. The kernel has to contain even the negative overshoot of the neuron's potential. The \(\theta_1\) kernel describes the neuron's short-term plasticity and changes the threshold value of the neuron (in most cases) after the neuron fires.

\begin{equation}
u(t) = \sum_{t^f}\eta(t-t^f)+\int_0^\infty\kappa(s)I(t-s)ds+u_rest
\end{equation}
\begin{equation}
\theta(t) = \theta_0 + \sum_{t^f}\theta_1(t-t^f)
\end{equation}

The model can also be interpreted as closed loop system where each kernel is a finite impulse response filter (FIR filter), the input of the system is the input current of the neuron, the output is the spike train that the neuron produces due to its input and the system's state itself is the membrane potential. The following image, taken from the book \cite{gerstner2014} gives an optical representation of the system mentioned above.

\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{Neurons-Synapses/SRM.jpg}
    \caption{SRM diagram. Each filter corresponds to a kernel that describes the mathematical model. The system represents the membrane potential and the output, the spike train of the neuron}
    \label{fig:lif-neuron}
\end{figure}

The abstract nature of the model due to the non-strictly defined kernel functions, gives the ability to implement a variety of neuron types and action potential responses and fine-tune the variables to create models that mimic the functionality of real neurons. In 2003 Reanut Jolivet et.al. showed that the Spike Response Model could predict with high accuracy the response of real neurons, given the same input \cite{Jolivet2003}. Improvements of the model have been proposed such as in \cite{Bohte2012} which make the model more biologically plausible

The Spike Response Model is said to be a generalization of the Integrate and Fire model. That said, it may have been expected that 
neural networks based on this neuron model would draw the attention of Artificial Intelligence engineers. Although some works have used such networks to tackle tasks like Breast Cancer Diagnostic, as shown in \cite{Ourdighi2016}, with impressive results, the SRM does not attract the interest of the engineering community. As a result only a few hardware implementations exist, such as \cite{Clayton2011}, while the model itself stands as a tool mainly from biological perspective.

\subsection{Stochastic and Probabilistic Neuron model}

All neuron models that have been presented were deterministic systems, described by fixed valued equations and deterministic inputs. As a result, by definition, knowing the initial state and the input signal, the response of the model can be deduced. This fact can be useful as it helps understanding important features as the stability of the model. However, it has been suggested that adding stochasticity in such biological systems can be useful since it gives the ability for weak signals to be detected. This idea is called stochastic resonance and was presented in 1981. Stochastic resonance is said to be present when the following three conditions exist in the system. First, there has to be an activation barrier, such as the threshold value presented in the previous models. Moreover, weak and periodic signals, such as the input spike-train must be applied. Finally, an input noise has to be applied in the input signal. Stochastic resonance is extensively analyzed in this work \cite{Gammaitoni1998}. This phenomenon is not only a useful tool from an engineering perspective, but is also observed in biological systems\cite{Honggi2002}. Consequently, there has been an extensive research in intentionally adding stochasticity (such as by applying a noisy input) or probability in neuron models, so as to exploit the effects of stochastic resonance.

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Neurons-Synapses/Stochastic-resonance-occurs-when-an-optimal-level-of-noise-is-added-to-a-subthreshold.png}
    \caption{Stochastic Resonance. An input noise is added to the weak signal. As a result, the new noisy input can exceed the threshold and the activation occurs. The intensity of the added noise plays an important role in the output and has to be handled carefully. A low intensity noise will not cause the input to exceed the threshold, while a high intensity one will dominate the input signal and the information will be lost.}
    \label{fig:lif-neuron}
\end{figure}

Stochasticity, as mentioned before, does show up in biological models. Stochasticity of the system can be caused by a noisy input, as presented above, with the existance of stochastic resonance in deterministic models, such as in \cite{Clayton2011}. Nonetheless, the model itself can be guided by stochastic processes and probability embedded in its parameters. It is known that the opening and closing of ion pumps in the neurons is guided by some amount of stochasticity embedded in the system. To tackle this behavior, stochastic models of the Hodgkin-Huxley model have been proposed \cite{Fox1997} that treat the opening and closing of the ion pumps as non-deterministic events. While such models can be useful from biological perspective, they do not serve a remarkable purpose in developing trainable large-scale spiking neural networks due to their complexity. For such usage, non-deterministic model of other, simple models have been developed such as the LIF model and the Spike Response Model. The non-deterministic behaviour is usually added in parameters that correspond to the three phases of the neuron: the presynaptic, the membrane and the postsynaptic. In his work\cite{Kasabov2010}, N.Kasabov defines a probabilistic neuron model where each of the three aforementioned stages contribute with a probability for the spike arrival, the spike contribution to the neuron membrane and the spike generation. Developed network with stochastic neurons for pattern recognition tasks show features that deterministic SNNs lack of. A common conclusion that is typically mention is the robustness of the network compared with the deterministic one, as stated in this paper\cite{Dhoble2011} where the probabilistic LIF neuron model was used, and in this one\cite{Sinyavskiy2010} where this time a stochastic SRM with a spike generation probability was used. In this paper \cite{Wu2012} multiple non-deterministic models are compared to the corresponding deterministic, showing robustness and explicit behaviour.

Another approach in adding stochasticity to the model is the infusion of stochastic diffusion. The most common model that uses the diffusion process as a stochastic neuron model is the Ornstein-Uhlenbeck model \cite{Lansky1995}. Another model that uses the same idea is the Feller model. As a result of their unique and complex nature, these types of models have attracted the interest and parameter estimation has been extensively studied over them, as presented in \cite{Ditlevsen2006} and in \cite{Lansky2008}. However, it should be mentioned that only the surface has been scratched of the stochastic neuron model family. There have been developed a wide variety of neuron models that exploit the stochasticity in different ways and have it act on different aspects of the model. Two more such models are presented here, the Galves–Löcherbach model \cite{Galves2013} and the stochastic Fitzhugh-Nagumo model \cite{Tuckwell1998}, left for the reader to dive into.

As presented above, stochasticity in neuronal networks could be a useful tool to explore unnoticed behaviors of the systems, but one could argue that coding and implementing such systems in hardware is inefficient and probably computationally expensive. However, stochastic resonance has to do with adding noise to the input. As a result, by carefully manipulating the electronic components of the hardware, the naturally created noise can be added to the input and give the desired noisy input spike-train. Such devices, like stochastic memristors, have been tested in their ability to control the noise of the device and create fully stochastic switching devices \cite{Gaba2013}, and such devices have been used in developing hardware components with embedded stochasticity for spiking neural networks \cite{Maruan2015}. Moreover, memristors are not an one-way street in developing stochastic SNNs. Different components, such as the avalanche diode \cite{Clayton2011}, can be manipulated to produce the desired noisy input, or even to exploit smart architecture in FPGAs and develop stochastic SNNs with fully deterministic components, where stochasticity is digitally added \cite{Josep2012}. 

One topic that stochastic SNNs (and ANNs in general) show potential is finding fast approximate solutions to NP-complete problems. Biological systems show a high ability in finding good enough solutions in constraint satisfaction problems. Knowing this, stochastic SNNs have been developed to mimic such behavior with satisfying results\cite{Fonseca2017}. Another promising use of stochastic spiking neural networks is their use as a reservoir computer or, in the case of spiking neural networks, liquid state machines. Liquid state machines(LSM) consist of a large collection of neurons that are randomly connected to each other and each one receives input spikes through time from an external source and other neurons in the LSM. Due to their architecture LSM have high capabilities in computing a large variety of non-linear functions and the added stochasticity can even further improve the framework. Such stochastic LSMs (sLSM) have already been tested against their deterministic LSM \cite{IEEE2011} and it has been deduced that they can perform better than the deterministic ones. This framework has already been used successfully for developing a SNN for EEG classification \cite{Nuntalid2011}.

\bigskip
\section{Spike Information Processing}
\subsection{Information Representation}

The added temporal dimension that is embedded in SNNs, gives rise to the need of encoding the input data in such ways that the Network can process. There are two main encoding schemes to represent the given information into spiketrain, rate coding and pulse or temporal coding. The choice of data encoding constitutes an important decision in the design of the Neural Model, as it gives the ability to develop models with different learning rules, compatible with the encoding scheme that was chosen, and form models in a wide spectrum of biological plausibility and engineering computability.

\subsubsection{Rate Code}

Rate coding is the encoding scheme where the information is encoded to the number of spikes emitted within a time window. The firing rate of a neuron is calculated as the number of spikes the neuron produces within a given time window. There are many encoding algorithms that are based on the rate coding scheme that count the firing rate of single neurons or population of neurons during one or multiple time windows. However, calculating the average firing rate of a population of neurons with similar properties over a time window may not be very useful in representing or extracting information. This method of representing the information is widely used due to its simplicity and low complexity. However, focusing of the number of spikes in a time window and not the exact time that each spike occurs, information that is encoded in the exact timing of the spike has little contribution or is even lost. Furthermore, while there are some cases that the firing rate of neurons actually represent information in biological systems, as shown here \cite{Huxter2003}, it is commonly accepted that rate coding shows little biological plausability. In this work \cite{Richmond1987}, Richmond and Optican showed that initial layers of neurons may be highly correlated with firing rate, but for "deeper" layers, there was little correlation between spike count and information representation. 

\subsubsection{Temporal Code}

Temporal coding is based on the idea that information is encoded in the exact time of the spike being emitted, or the time difference between spikes. Temporal coding is shown to carry more information as the timing of the spikes carry information on its own, increasing the efficiency of neural connections \cite{Mainen2009}. Temporal coding provides another very important feature that its possible absence makes the study of learning algorithms in Spiking Neural Networks difficult, differentiability. In this research \cite{Mostafa2018}, the authors show that the input-output relation of s SNN with temporal encoding scheme is differentiable almost everywhere. 

\subsection{Encoding and Decoding Spikes}
\subsubsection{Threshold-based encoding (or Temporal Contrast)}
\subsubsection{Rank Order Coding (ROC)}
\subsubsection{Population Rang Coding (POC)}
\subsubsection{Ben’s Spike Encoding Algorithm (BSA)}
\subsubsection{Step Forward (SF) Encoding algorithm}
\subsubsection{Moving-Window (MW) Spike Encoding Algorithm}
\selectlanguage{greek}
\section{Μέθοδοι Μάθησης}
Έχοντας περιγράψει προηγουμένως τις μεθόδους μοντελοποίησης νευρώνων, αυτή η ενότητα αναλύει τις μεθόδους μάθησης που χρησιμοποιούνται στη μηχανική μάθηση με νευρωνικά δίκτυα.
\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Learning Methods/spikeprop/spikeprop-net-architecture.PNG}
    \caption{(A)\textlatin{Feedforward Spiking Neural Network}  (B) Σύνδεση μεταξύ νευρώνων που αποτελείται από πολλαπλά συναπτικά τερματικά (\textlatin{synaptic terminals})}
    \label{fig:spikeprop-net-architecture}
\end{figure}
\subsection{\textlatin{SpikeProp}}

Το \textlatin{SpikeProp}, μια μέθοδος που βασίζεται σε \textlatin{backpropagation} για εποπτευόμενη μάθηση, ήταν μία από τις πρώτες μεθόδους που χρησιμοποιήθηκαν για την εκπαίδευση των Νευρωνικών Δικτύων Αιχμών \cite{bohte2002}. Ο χρονισμός των αιχμών ενός νευρώνα κωδικοποιεί τις πληροφορίες κατά τη διάρκεια της εκπαίδευσης. Οι ερευνητές αποδεικνύουν επίσης εμπειρικά ότι δίκτυα βιολογικά εύλογων νευρώνων με αιχμές μπορούν να εκτελέσουν σύνθετη μη γραμμική ταξινόμηση σε μια γρήγορη χρονική κωδικοποίηση, καθώς και δίκτυα που κωδικοποιούνται βάση του ρυθμού των αιχμών.
\subsubsection{Αρχιτεκτονική Δικτύου}
Η αρχιτεκτονική αποτελείται από ένα προωθητικό δίκτυο νευρώνων με αιχμές (\textlatin{ feedforward network of spiking neurons}) , αντί για τεχνητούς νευρώνες, ακολουθούμενο από συναπτικούς τερματικούς σταθμούς με καθυστέρηση, όπως περιγράφεται στο \cite{ruf1998}, βλέπε Εικ. \ref{fig:spikeprop-net-architecture}. Οι νευρώνες ακίδας συνήθως δημιουργούν δυνατότητες δράσης (αιχμές) όταν το δυναμικό της μεμβράνης ξεπεράσει ένα όριο. Οι ερευνητές θεωρούν το δυναμικό της μεμβράνης ως εσωτερική μεταβλητή νευρωνικής κατάστασης. Η σχέση μεταξύ των αιχμών και της μεταβλητής εσωτερικής κατάστασης του νευρώνα μπορεί να περιγραφεί από οποιοδήποτε μοντέλο νευρώνα, αλλά στην εφαρμογή του \textlatin{SpikeProp} σε πειράματα, χρησιμοποιείσαν το μοντέλο απόκρισης αιχμών (\textlatin{SRM}).

Ένας νευρώνας \textit{\textlatin{j}} στο δίκτυο, λαμβάνει αιχμές εισόδου από ένα σύνολο προ-συναπτικών νευρώνων \(\Gamma\textsubscript{\textlatin{j}}\) με χρόνους πυροδότησης \(t\textsubscript{\textlatin{j}},\textlatin{i}\in\Gamma\textsubscript{\textlatin{j}}\). Η δυναμική της μεταβλητής εσωτερικής κατάστασης του νευρώνα \(x\textsubscript{\textlatin{j}}\) επηρεάζεται από τους προ-συναπτικούς νευρώνες σύμφωνα με μια συνάρτηση απόκρισης ακμής \(\varepsilon(t)\) και ένα συναπτικό βάρος \(\textlatin{w}\textsubscript{\textlatin{j}}\).
\begin{equation}
    x\textsubscript{\textlatin{j}}(t) = \sum_{\textlatin{i}\in\Gamma\textsubscript{\textlatin{j}}} w\textsubscript{\textlatin{ij}}\varepsilon(t-t\textsubscript{i})
\end{equation}

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Learning Methods/spikeprop/spikeprop-connections-network.PNG}
    \caption{Συνδέσεις ενός \textlatin{Feedforward Spiking Neural Network} : (A) ενα συναπτικο τερματικό: το αργοπορημένο
προ-συναπτικό δυναμικό σταθμίζεται και παράγει ένα μετα-συναπτικό δυναμικό, (Β)
δύο συνδέσεις πολλαπλών συνάψεων με τη σταθμισμένη είσοδο να αθροίζεται στον επόμενο νευρώνα}
    \label{fig:spikeprop-connections-network}
\end{figure}

Η συνάρτηση απόκρισης είναι υπεύθυνη για το μετασυναπτικό δυναμικό, την ακίδα που παράγεται και το συναπτικό βάρος ρυθμίζει το ύψος αυτού του δυναμικού. Κάθε σύνδεση,σχήμα \ref{fig:spikeprop-net-architecture} (B), μπορεί να αναλυθεί περαιτέρω σε \textit{\textlatin{m}} σταθερά συναπτικά τερματικά όπου κάθε συναπτικό τερματικό έχει το δικό του βάρος και καθυστέρηση. Αυτή η καθυστέρηση ορίζεται ως η διαφορά μεταξύ του χρόνου πυροδότησης του προσυναπτικού νευρώνα, και την χρονική στιγμή που το μετασυναπτικό δυναμικό αρχίζει να αυξάνεται, fig\ref{fig:spikeprop-connections-network} (A). Μια προ-συναπτική άνοδος σε ένα συναπτικό τερματικό \textit{k} ορίζεται ως μετασυναπτικό δυναμικό τυπικού ύψους με καθυστέρηση \(d^k\). Ο μη σταθμισμένος όρος ενός μόνο συναπτικού τερματικού που συμβάλλει στη μεταβλητή κατάστασης μπορεί να οριστεί ως:

\begin{equation}
    y^k\textsubscript{\textlatin{i}}(t) = \varepsilon(t-t\textsubscript{\textlatin{i}}-d^k) 
\end{equation}

Η συνάρτηση απόκρισης ακίδας  \(\varepsilon(t)\) έχει αρχική τιμή μηδέν. Ο χρόνος \(t\textsubscript{\textlatin{i}}\) είναι ο χρόνος πυροδότησης του προ-συναπτικού νευρώνα \textit{\textlatin{i}} . Η συνάρτηση απόκρισης δίνεται από: 
\begin{equation}
    \varepsilon(t)=\frac{t}{\tau}e^{1-\frac{t}{\tau}}
\end{equation}
\(\tau\) μοντελοποιεί τη σταθερά χρόνου του δυναμικού της μεμβράνης  που καθορίζει τον χρόνο ανόδου και αποσύνθεσης του μετασυναπτικού δυναμικού δυναμικού. Το σταθμισμένο άθροισμα των προ-συναπτικών εισφορών είναι τώρα:
\[ \sum_{k=1}^{m}w\textsubscript{\textlatin{ij}}^ky\textsubscript{\textlatin{i}}^k(t) \]
H μεταβλητή \(w\textsubscript{\textlatin{ij}}^ky\) δηλώνει το σχετικό βάρος του συναπτικού τερματικού k ,\textlatin{fig} \ref{fig:spikeprop-connections-network} . Ο χρόνος πυροδότησης \textit{\textlatin{t}}\textsubscript{\textlatin{j}} του νευρώνα \textit{\textlatin{j}} είναι ίσος με την πρώτη φορά που η μεταβλητή κατάστασης υπερβαίνει το κατώφλι \(theta\) .Το κατώφλι είναι σταθερή τιμή και παραμένει ίσο μεταξύ όλων των νευρώνων.

\subsubsection{\textlatin{BackPropagation} με \textlatin{SpikeProp}}

Το δίκτυο έχει τρία επίπεδα όπως φαίνεται στο Σχήμα \ref{fig:spikeprop-net-architecture},\textlatin{H}(είσοδος) , \textlatin{I}(κρυμένο) ,\textlatin{J}(έξοδος), αν και μπορούν να χρησιμοποιηθούν περισσότερα κρυμμένα επίπεδα.

Ο στόχος αυτού του αλγορίθμου είναι να μάθει ένα σύνολο χρόνων πυροδότησης,{\(t\textsubscript{\textlatin{j}}^d\)} στους νευρώνες εξόδου, {\(\textlatin{j}\in \textlatin{J}\)} για ένα δεδομένο σύνολο μοτίβων εισόδου {\(P[t\textsubscript{1}...t\textsubscript{\textlatin{h}}]\)}. Το σύνολο των μοτίβων εισόδου ορίζει ένα ενιαίο μοτίβο εισόδου που περιγράφεται με μοναδικούς χρόνους ακίδας για κάθε νευρώνα {\(h\in H\)}.

Η συνάρτηση σφάλματος που χρησιμοποιήθηκε στα πειράματα των ερευνητών ήταν η ρίζα του μέσου τετραγωνικού σφάλματος, αλλά και πάλι μπορούν να γίνουν άλλες επιλογές γι 'αυτήν. Δεδομένης επιθυμητών χρόνων αιχμών
{\(t\textsubscript{\textlatin{j}}^d\)} και τους πραγματικούς χρόνους πυροδότησης {\(t\textsubscript{\textlatin{j}}^a\)}, η συνάρτηση σφάλματος ορίζεται ως:

\selectlanguage{english}
\begin{equation}
    E=1/2\sum_{j\in J}(t\textsubscript{j}^a-t\textsubscript{j}^d)^2
\end{equation}
\selectlanguage{greek}
Κάθε συναπτικό τερματικό \textit{\textlatin{k}} θεωρείται ως ξεχωριστή σύνδεση με το βάρος \(w\textsubscript{\textlatin{ij}}^k\). Οι εξισώσεις του \textlatin{backpropagation} για τους νευρώνες εξόδου είναι:
\selectlanguage{english}
\begin{equation}
    \Delta w\textsubscript{ij}^k=-\eta y\textsubscript{i}^k (t\textsubscript{j}^a) \delta \textsubscript{j}
\end{equation}
\begin{equation}
\delta \textsubscript{j} = \frac { t\textsubscript{j}^d-t\textsubscript{j}^a}  {\sum_{i \in \Gamma \textsubscript{j} } \sum_{l} w \textsubscript{ij}^l(\partial y \textsubscript{i}^l(t \textsubscript{j}^a) / \partial t \textsubscript{j}^a)}
\end{equation}
\selectlanguage{greek}
Τώρα, πρέπει να οριστούν οι εξισώσεις για τα κρυμμένα επίπεδα.
\selectlanguage{english}
\begin{equation}
    \Delta w\textsubscript{hi}^k=-\eta y\textsubscript{h}^k (t\textsubscript{i}^a) \delta \textsubscript{i}
\end{equation}
\begin{equation}
\delta \textsubscript{i} = \frac { \sum_{j \in \Gamma^i  }\delta\textsubscript{j}\sum_{k} w \textsubscript{ij}^k(\partial y \textsubscript{i}^k(t \textsubscript{j}^a) / \partial t \textsubscript{i}^a)}
{ \sum_{h \in \Gamma \textsubscript{i} }\sum_{l} w \textsubscript{hi}^l(\partial y \textsubscript{h}^l(t \textsubscript{i}^a) / \partial t \textsubscript{i}^a)}
\end{equation}
\selectlanguage{greek}
Η ταξινόμηση εξόδου κωδικοποιήθηκε με τρόπο που ο νευρώνας που κωδικοποιούσε για την αντίστοιχη κατηγορία είχε εκχωρηθεί σε ένα χρόνο πρώτης πυροδότησης.

\subsubsection{Σημαντικές παρατηρήσεις και περιορισμοί}

Κατά τον πειραματισμό, ο αλγόριθμος δεν συγκλίνει εάν υπάρχουν αρνητικά και θετικά βάρη. Επιπλέον, για τη σύγκλιση της μάθησης ήταν απαραίτητο να ενσωματωθούν τόσο διεγερτικοί όσο και ανασταλτικοί νευρώνες. Για την κωδικοποίηση της εισόδου, χρησιμοποιείται μια μέθοδος για την κωδικοποίηση μεταβλητών εισόδου σε χρονικά μοτίβα ακίδας με κωδικοποίηση πληθυσμού. Ωστόσο, η κωδικοποίηση των δεδομένων εισόδου εξαρτάται από την επιλογή του πειραματιστή και δεν περιορίζεται σε μία συγκεκριμένη επιλογή από τον αλγόριθμο, οπότε η απόδοση μπορεί να ποικίλει. Παρόλο που ο αλγόριθμος φαίνεται πολλά υποσχόμενος, απαιτεί τον ίδιο αριθμό επαναλήψεων σε σύγκριση με ένα τυπικό \textlatin{MLP} . Ωστόσο, δεδομένης της ρητής χρήσης του χρονικού πεδίου για υπολογισμούς, υποστηρίζεται ότι ένα δίκτυο νευρώνων ακίδας είναι εγγενώς πιο κατάλληλο για μάθηση και αξιολόγηση χρονικών προτύπων από τα σιγμοειδή δίκτυα.
\subsection{Πλαστικότητα εξαρτώμενη από τον χρόνο ακίδας (\textlatin{STDP})}

Η πλαστικότητα που εξαρτάται από τον χρόνο ακίδας είναι μια βιολογική διαδικασία, ένας τύπος πλαστικότητας, που προσαρμόζει τη δύναμη των νευρωνικών συνδέσεων στον εγκέφαλο, όπως περιγράφεται στην υποενότητα 2.3.4. Πιο συγκεκριμένα, το \textlatin{STDP} μπορεί να οριστεί ως η διαδικασία που τροποποιεί τη δύναμη των συνδέσεων με βάση τον σχετικό χρονισμό των δυνατοτήτων εξόδου και εισόδου ενός νευρώνα (ή αιχμών). Από τη σκοπιά ενός μαθηματικού μπορεί να θεωρηθεί ως μια χρονικά ασύμμετρη μορφή \textlatin{hebbian} μάθησης που εξαρτάται από τους χρονικούς συσχετισμούς των νευρώνων που προκαλούνται από στενούς χρονικούς συσχετισμούς μεταξύ των αιχμών των προσυναπτικών και μετασυναπτικών νευρώνων \cite{stdp2010}.

\textlatin{STDP} με νευρωνικά δίκτυα αιχμών είναι μια μορφή μάθησης χωρίς επίβλεψη . Κατά τη διάρκεια της μάθησης, η αλλαγή \(\Delta w\textsubscript{\textlatin{j}}\) μιας σύναψης από έναν προσυναπτικό νευρώνα \textit{\textlatin{j}} εξαρτάται από τη σχετική χρονική στιγμή μεταξύ εισόδων προσυναπτικής ακίδας και μετασυναπτικών αιχμών. Το σύνολο προσυναπτικών ωρών άφιξης στη σύναψη \textit{\textlatin{j}},με αριθμό καταμέτρησης \textit{\textlatin{f}} συμβολίζεται με \(t\textsubscript{\textlatin{j}}^f\) . 
Ομοίως, οι χρόνοι πυροδότησης του μετασυναπτικού νευρώνα ονομάζονται \(t\textsubscript{\textlatin{i}}^n\). Το συνολικό βάρος αλλάζει τότε \cite{Gerstner1996} σύμφωνα με τον τύπο: 
\selectlanguage{english}
\begin{equation}
\Delta w\textsubscript{j}=\sum_{f=1}^N\sum_{n=1}^N W(t\textsubscript{i}^n-t\textsubscript{j}^f)
\end{equation}
\selectlanguage{greek}
\(W(x)\) δηλώνει το παράθυρο εκμάθησης, μια συνήθης επιλογή είναι η ακόλουθη:
\selectlanguage{english}
$$
W(x)=\begin{cases}
			A\textsubscript{+}exp(-x/\tau\textsubscript{+}), & \text{for $x$\textgreater0}\\
            -A\textsubscript{-}exp(x/\tau\textsubscript{-}), & \text{for $x$\textless0}
		 \end{cases}
$$
\selectlanguage{greek}
Οι τιμές A\textsubscript{+} και -A\textsubscript{-} μπορεί να εξαρτώνται από την τρέχουσα τιμή του \textlatin{W}(\textlatin{x}) και οι σταθερές χρόνου \(\tau\) είναι της τάξης των 10 \textlatin{ms}. Έχει αποδειχθεί ότι το \textlatin{STDP} μαθαίνει «πρότυπα πρώιμης ακίδας» όταν ένας νευρώνας παρουσιάζεται επανειλημμένα με διακριτές βολές εισόδου( \textlatin{discrete volleys of input spikes}) , συγκεντρώνοντας συναπτικά βάρη σε εισόδους που πυροδοτούνται συνεχώς νωρίς, με αποτέλεσμα η μετασυναπτική καθυστέρηση ακίδας να μειώνεται μέχρι να φτάσει σε μια ελάχιστη και σταθερή τιμή. Αυτά τα ευρήματα ισχύουν υπό ένα συνεχές καθεστώς στο οποίο οι εισροές πυροδοτούν με σταθερό ρυθμό πληθυσμού. Κατά συνέπεια, το \textlatin{STDP} μπορεί να χειριστεί ένα δύσκολο υπολογιστικό πρόβλημα:τον εντοπισμό επαναλαμβανόμενου χωροχρονικού μοτίβου ακίδας που περιέχεται σε εξίσου πυκνούς συρμούς «περισπαστών» ακίδων(\textlatin{‘distractor' spike trains}) \cite{Masquelier2008}. Το \textlatin{STDP} επιτρέπει έτσι κάποια μορφή χρονικής κωδικοποίησης, ακόμη και ελλείψει ρητής χρονικής αναφοράς. Έχει αναφερθεί ότι επαναλαμβανόμενα χωροχρονικά μοτίβα ακίδας με ακρίβεια χιλιοστού του δευτερολέπτου υπάρχουν σε ηλεκτροφυσιολογικά πειράματα \cite{Fellous2004}. Εδώ \cite{Masquelier2008} δείχνουν πώς το \textlatin{STDP} μπορεί να μάθει το δύσκολο έργο της ανίχνευσης αυτών των επαναλαμβανόμενων μοτίβων, καταδεικνύοντας για άλλη μια φορά πώς τα νευρωνικά δίκτυα ακίδων μπορούν να λύσουν δύσκολα προβλήματα αυτού του τύπου με τις κατάλληλες μεθόδους.
\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Learning Methods/stdp/1.png}
    \caption{Με κόκκινο χρώμα υποδεικνύεται ένα επαναλαμβανόμενο μοτίβο διάρκειας 50 \textlatin{ms} που αφορά 50 προσαγωγούς νευρώνες μεταξύ 100. Το κάτω πλαίσιο απεικονίζει τους μέσους ρυθμούς πυροδότησης του πληθυσμού σε "κάδους χρόνου" (\textlatin{time bins}) των 10 \textlatin{ms}. Το δεξιό πλαίσιο απεικονίζει τα μεμονωμένα ποσοστά πυροδότησης κατά μέσο όρο σε όλη την περίοδο}
    \label{fig:stdp-1}
\end{figure}
\subsubsection{\textlatin{STDP}: ανίχνευση επαναλαμβανόμενων μοτίβων}
Ένα παράδειγμα ενός τέτοιου επαναλαμβανόμενου μοτίβου φαίνεται στο σχήμα \ref{fig:stdp-1}. Το πρόβλημα περιπλέκεται από το γεγονός ότι ούτε ο ρυθμός πυροδότησης του πληθυσμού ούτε οι ρυθμοί πυροδότησης των νευρώνων που συμμετέχουν στο μοτίβο είναι μοναδικοί κατά τη διάρκεια των περιόδων όταν υπάρχει το μοτίβο. Αυτό το είδος της κατάστασης απαιτεί να ληφθούν υπόψη οι χρόνοι/χρονικές στιγμές των ακίδων. Οι ερευνητές δείχνουν πώς ένας νευρώνας που χρησιμοποιεί \textlatin{STDP} μπορεί να λύσει αυτό το πρόβλημα αξιοποιώντας το γεγονός ότι ένα μοτίβο είναι μια σειρά από συμπτώσεις ακίδων. Το \textlatin{STDP} είναι γνωστό ότι έχει την επίδραση της συγκέντρωσης συναπτικών βαρών σε επαναλαμβανόμενες πρώιμες εισόδους πυροδότησης, με αποτέλεσμα τη μείωση της λανθάνουσας μετασυναπτικής αιχμής όταν ένας νευρώνας παρουσιάζεται επανειλημμένα με παρόμοια βολή εισόδων. Με άλλα λόγια, αυτές οι εισροές οδηγούν συστηματικά στη διαμόρφωση της επιλεκτικότητας του νευρώνα. Αυτή η διαμόρφωση επιτεύχθηκε σε μια ποικιλία συνθηκών θορύβου στο παρασκήνιο, καθώς και σε καταστάσεις όπου οι καθυστερήσεις και τα ποσοστά πυροδότησης ή ο συγχρονισμός έδωσαν αντιφατικές πληροφορίες \cite {rossum2000} \cite {guyonneau2005} \cite {masquelier2007}. Οι ερευνητές υποδεικνύουν τον περιορισμό αυτών των μελετών που απαιτούν ρητή χρονική αναφορά και αναρωτιούνται εάν το \textlatin{STDP} μπορεί να αναγνωρίσει το επαναλαμβανόμενο μοτίβο ελλείψει χρονικής αναφοράς.

Για να δοκιμαστεί αυτό, εισήχθη ένα αυθαίρετο μοτίβο όπως στο Σχήμα \ref{fig:stdp-1} και ερευνήθηκε εάν ένα μόνο \textlatin{STDP} που λάμβανε ήταν σε θέση να το μάθει χωρίς επίβλεψη. Οι αιχμές εισόδου προσομοιώθηκαν σύμφωνα με μια διαδικασία \textlatin{Poisson} όπου οι νευρώνες πυροδοτούν στοχαστικά και ανεξάρτητα. Το αυθαίρετο μοτίβο ήταν σκόπιμα κρυμμένο από τον ρυθμό πυροδότησης των νευρώνων, οπότε θα ήταν αδύνατο να λυθεί χρησιμοποιώντας μόνο τους ρυθμούς πυροδότησης. Ενισχύοντας τις συναπτικές συνδέσεις με τους προσαγωγούς νευρώνες που συμμετείχαν στην πυροδότηση του νευρώνα (διαμορφώνοντας την εκλεκτικότητα του νευρώνα). Όταν το μοτίβο εμφανιστεί ξανά, αυξάνεται η πιθανότητα ο νευρώνας να πυροδοτηθεί ξανά. Εκτός από τη διαμόρφωση της επιλεκτικότητας του νευρώνα, επιτρέπει επίσης τη σύγκλιση με κορεσμό, όταν όλες οι ακίδες στο μοτίβο που προηγούνται της μετασυναπτικής ακίδας αντιστοιχούν ήδη σε συνάψεις με το μέγιστο επιτροπόμενο δυναμικό και όλες είναι απαραίτητες για να φτάσουμε στο κατώφλι. Οι ακίδες έξω από το μοτίβο έχουν κατασταλλεί, ώστε να μην συμβάλλουν στο δυναμικό της μεμβράνης. Αυτό οδηγεί σε απουσία ψευδών "συναγερμών" που σημαίνει ότι μετά την εκμάθηση,ο νευρώνας πυροδοτείται  μόνο όταν υπάρχει το μοτίβο (σχήμα \ref{fig:stdp-converged}). Εάν αυτό συμβαίνει στον εγκέφαλο, οι πληροφορίες σχετικά με ένα ερέθισμα μπορούν να είναι άμεσα διαθέσιμες, αφού οι νευρώνες θα πυροδοτήσουν κατά την έναρξη του ερεθίσματος, όπως υποστηρίζεται εδώ \cite{thorpe2001}.

\subsubsection{\textlatin{STDP} και ταλαντώσεις}
Το \textlatin{STDP} επιτρέπει επίσης την ενσωμάτωση ταλαντώσεων καθώς φάνηκε ότι είναι σε θέση να επιλέξει μόνο εισόδους κλειδωμένες σε μια συγκεκριμένη φάση (\textlatin{phase-locked inputs}) μεταξύ ενός ευρέος πληθυσμού με τυχαίες φάσεις, μετατρέποντας τον μετασυναπτικό νευρώνα σε έναν ανιχνευτή συμπτώσεων. Μαθαίντας στο δίκτυο να ανταποκρίνεται σε ορισμένες εισόδους κλειδωμένες σε φάσεις, μπορεί να συντονίσει και να συγχρονίσει τη νευρωνική δραστηριότητα \cite{Gerstner1996}. Ένα παράδειγμα που αποδεικνύεται στην προαναφερθείσα έρευνα είναι πώς μια κουκουβάγια είναι σε θέση να συντονίσει τη νευρωνική δραστηριότητα μεταξύ των δύο αυτιών της αρκετά γρήγορα με αποτέλεσμα ο χρόνο αντίδρασης της πριν γυρίσει το κεφάλι της να είναι περίπου 100 \textlatin{ms}. Η εκμάθηση με το \textlatin{STDP} επιλέγει συνδέσεις συνάψεως με τέτοιο τρόπο ώστε οι ακίδες να φτάνουν με συνέπεια. Ένα άλλο παράδειγμα τέτοιας ανάγκης για συντονισμό με ταλαντώσεις είναι κατά τη διάρκεια της σακκαδικής κίνησης των ματιών. Για να αξιοποιηθούν αποτελεσματικά οι χωρικές πληροφορίες που περιέχονται στις διαμορφώσεις φωτεινότητας που προκύπτουν από τις κινήσεις των ματιών, η ανάλυση της νευρικής δραστηριότητας πρέπει να χρονομετρηθεί σε σχέση με την εμφάνιση των σακκαδικών κινήσεων. Για παράδειγμα, μια ακίδα από τον ίδιο νευρώνα φέρει διαφορετική πληροφοριακή τιμή εάν συμβαίνει κατά τη διάρκεια της πρώιμης εστίασης (\textlatin{early fixation}), όταν η αλλαγή εισόδου που προκαλείται από το προηγούμενη σακκαδική κίνηση ασκεί ακόμη την επιρροή του ή αργότερα, όταν οι χρονικές αλλαγές προκαλούνται μόνο από την οφθαλμική μετατόπιση (\textlatin{ocular drift})  \cite{Rucci2018 },δείτε και το σχήμα \ref{fig:eyes_saccades}.Οι συχνότητες των οπτικών ταλαντώσεων του οπτικού φλοιού μπορούν να ελεγχθούν τοπικά και μπορούν να ρυθμιστούν από και να κλειδωθούν σε εξωτερικά ερεθίσματα \cite{Ahissar2012}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Learning Methods/stdp/stdp-converged.png}
    \caption{(α) Το μοτίβο παριστάνεται ως γκρι ορθογώνιο. Σημειώστε την ομάδα των λευκών αιχμών στην αρχή: Οι περισσότερες από τις συνάψεις που αντιστοιχούν στις πρώτες αιχμές του μοτίβου έχουν ενισχυθεί από το STDP. Αξίζει να σημειωθεί ότι σχεδόν όλες οι συναπτικές συνδέσεις με προσαγωγούς που δεν ασχολούνται με το μοτίβο έχουν καταργηθεί πλήρως. (β) Στο ίδιο εύρος με το παραπάνω, το δυναμικό της μεμβράνης εμφανίζεται ως συνάρτηση του χρόνου. Η απότομη ακίδα που αντιστοιχεί στην προαναφερθείσα ομάδα είναι άμεσα ορατή\cite{Masquelier2008}.} 
    \label{fig:stdp-converged}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Learning Methods/stdp/eyes_saccades.png}
    \caption{Σακκαδικές κινήσεις:\textlatin{microsaccades or saccades}. Οφθαλμική μετατόπιση:\textlatin{ocular drift}.Φωτοϋποδοχείς στον αμφιβληστροειδή χιτώνα: \textlatin{photoreceptor cells}.}
    \label{fig:eyes_saccades}
\end{figure}

\subsection{Μάθηση με υποκαταστάτη παραγώγου σε Νευρωνικά Δίκτα Ακίδων}

Σε αυτή την υποενότητα παρουσιάζουμε τις μεθόδους υποκατάστατης παραγώγου, καθώς οι υποκατάστατες παράγωγοι χρησιμοποιούνται ευρέως σε άλλες μεθόδους εκμάθησης τις οποίες θα δούμε αργότερα. Επιπλέον, οι ερευνητές που δημοσιεύσαν την σχετική έρευνα χαρτογραφούν επίσημα \textlatin{SNN} σε \textlatin{RNN} \cite{neftci2019}. Η διατύπωση των \textlatin{SNN} ως \textlatin{RNN} επιτρέπει τη μεταφορά και εφαρμογή υφιστάμενων μεθόδων εκπαίδευσης για \textlatin{RNN}. Αυτή η χαρτογράφηση είναι σημαντική εάν θέλουμε να εκπαιδεύσουμε μεγαλύτερο αριθμό επιπέδων . Παροθσιάζεται επίσης ο τρόπο επίλυσης του προβλήματος εκχώρησης πίστωσης (\textlatin{credit assignment problem}) με πολλά επίπεδα και επίσης τα κύρια ζητήματα με τα πολλαπλά στρώματα νευρωνικών δικτύων ακίδων που κάνουν την εκπαίδευση πολύ πιο δύσκολη από τα "παραδοσιακά" νευρωνικά δίκτυα. 

\subsubsection{Χαρτογραφόντας \textlatin{RNN} σε \textlatin{SNN}}

Με την ευρύτερη έννοια, τα \textlatin{RNN} είναι δίκτυα των οποίων η κατάσταση εξελίσσεται με την πάροδο του χρόνου σύμφωνα με ένα σύνολο επαναλαμβανόμενων δυναμικών εξισώσεων. Μια τέτοια δυναμική επανάληψη μπορεί να συμβεί με την παρουσία επαναλαμβανόμενων συναπτικών συνδέσεων μεταξύ των νευρώνων στο δίκτυο. Στη σημερινή έρευνα μηχανικής μάθησης αυτός είναι ο πιο συνηθισμένος ορισμός του τι είναι το \textlatin{RNN}. Αυτό συμβαίνει, για παράδειγμα, όταν χρησιμοποιούνται μοντέλα νευρώνων ή συνάψεων με εσωτερική δυναμική. Αυτές οι δυναμικές είναι εγγενώς επαναλαμβανόμενες αφού η κατάσταση του δικτύου σε ένα δεδομένο χρονικό βήμα εξαρτάται επαναλαμβανόμενα από την κατάστασή του σε προηγούμενα χρονικά βήματα. Εδώ ως συνήθως χρησιμοποιείται το μοντέλο νευρώνων \textlatin{LIF} ( για την τάση της μεμβράνης). Ωστόσο, μπορεί να προστεθεί ένας επιπλέον όρος για επαναλαμβανόμενες συνδέσεις στην εξίσωση συναπτικού ρεύματος:
\selectlanguage{english}
\begin{equation}
\label{eqn:surrogate_current_dynamics}
\frac{\mathrm{d} I_{i}^{(l)}}{\mathrm{d} t}=-\underbrace{\frac{I_{i}^{(l)}(t)}{\tau_{\mathrm{syn}}}}_{\text {exp. decay }}+\underbrace{\sum_{j} W_{i j}^{(l)} S_{j}^{(l-1)}(t)}_{\text {feed-forward }}+\underbrace{\sum_{j} V_{i j}^{(l)} S_{j}^{(l)}(t)}_{\text {recurrent }}
\end{equation}
\selectlanguage{greek}

Tο άθροισμα περνάει από όλους τους προσυναπτικούς νευρώνες $j$ και $W_{i j}^{(l)}$ είναι τα αντίστοιχα προσαγωγικά βάρη από το παρακάτω επίπεδο. Το $V_{i j}^{(l)}$ αντιστοιχεί σε ρητές επαναλαμβανόμενες συνδέσεις σε κάθε επίπεδο, αυτό αντιστοιχεί στον επιπλέον όρο για τον οποίο μιλάμε. Οι ερευνητές δηλώνουν επίσης ότι με επαναλαμβανόμενες συνδέσεις ένας μόνο νευρώνας \textlatin{LIF} μπορεί να προσομοιωθεί με δύο γραμμικές διαφορικές εξισώσεις των οποίων οι αρχικές συνθήκες αλλάζουν αμέσως κάθε φορά που εμφανίζεται μια ακίδα. Έτσι, ο όρος επαναφοράς μπορεί να εισαχθεί στην εξίσωση δυναμικού της μεμβράνης ως ένας επιπλέον όρος που μειώνει ακαριαία το δυναμικό της μεμβράνης κατά $\left(\vartheta-U_{\latintext {rest }}\right)$ κάθε φορά που ο νευρώνας εκπέμπει μια ακίδα:
\selectlanguage{english}
\begin{equation}
\label{eqn:surrogate_membrane_dynamics}
\frac{\mathrm{d} U_{i}^{(l)}}{\mathrm{d} t}=-\frac{1}{\tau_{\mathrm{mem}}}\left(\left(U_{i}^{(l)}-U_{\text {rest }}\right)+R I_{i}^{(l)}\right)+S_{i}^{(l)}(t)\left(U_{\text {rest }}-\vartheta\right)
\end{equation}
\selectlanguage{greek}
Οι παραπάνω εξισώσεις πρέπει να προσεγγίζονται διακριτά για προσομοίωση σε υπολογιστή. Επίσης, ο συρμός εξόδου $S_{i}^{(l)}[n]$ του νευρώνα $i$ στο επίπεδο $l$ στο χρονικό βήμα $n$ πρέπει να εκφραστεί ως μη γραμμική συνάρτηση της τάσης μεμβράνης $S_{i}^{(l)}[n] \equiv \Theta\left(U_{i}^{(l)}[n]-\vartheta\right)$ όπου $\Theta$ υποδηλώνει τη βηματική συνάρτηση \textlatin{Heaviside} και $\vartheta$ αντιστοιχεί στην οριακή τιμή πυροδότησης. Το χρονικό βήμα προσομοίωσης πρέπει επίσης να είναι μικρό για να λειτουργήσει σωστά η προσέγγιση. Η εξίσωση \ref{eqn:surrogate_current_dynamics} γίνεται:
\selectlanguage{english}
\begin{equation}
I_{i}^{(l)}[n+1]=\alpha I_{i}^{(l)}[n]+\sum_{j} W_{i j}^{(l)} S_{j}^{(l)}[n]+\sum_{j} V_{i j}^{(l)} S_{j}^{(l)}[n]
\end{equation}
\selectlanguage{greek}
Ρυθμός κορεσμού: $\alpha \equiv \exp \left(-\frac{\Delta_{t}}{\tau_{\mathrm{syn}}}\right) .$  Για πεπερασμένα και θετικά $\tau_{\mathrm{syn}}$:$0<\alpha<1$. Επίσης, $S_{j}^{(l)}[n] \in\{0,1\} .$ Η μεταβλητή $n$ χρησιμοποιείται για να δηλώσει το χρονικό βήμα. Η εξίσωση \ref{eqn:surrogate_membrane_dynamics} είναι τώρα:
\begin{equation}
U_{i}^{(l)}[n+1]=\beta U_{i}^{(l)}[n]+I_{i}^{(l)}[n]-S_{i}^{(l)}[n]
\end{equation}
$\beta \equiv \exp \left(-\frac{\Delta_{t}}{\tau_{\text {mem }}}\right)$
Με αυτές τις δύο εξισώσεις, η κατάσταση του νευρώνα $i$ μπορεί να βρεθεί από τα στιγμιαία συναπτικά ρεύματα $I_{i}$ και την τάση της μεμβράνης $U_{i}$ (Πλαίσιο. 1 $]. $ Οι υπολογισμοί που απαιτούνται για την ενημέρωση η κατάσταση του κελιού μπορεί να ξετυλιχτεί στο χρόνο όπως φαίνεται στο υπολογιστικό γράφημα στο σχήμα \ref{fig:computationgraph}.
\begin{figure}[htp]
    \centering
    \includegraphics[width=6cm]{Learning Methods/surrogate/computationgraph.jpg}
    \caption{}
    \label{fig:computationgraph}
\end{figure}
Αυτή είναι μια συνηθισμένη μέθοδος απεικόνισης που χρησιμοποιείται για επαναλαμβανόμενα νευρωνικά δίκτυα, οπότε η χαρτογράφηση των \textlatin{SNN} σε \textlatin{RNN} είναι πολύ βολική εδώ. Τα βήματα χρόνου ρέουν από αριστερά προς τα δεξιά.
\begin{itemize}
    \item  Οι αιχμές εισόδου $\mathbf{S}^{(0)}$ εισάγονται στο κάτω μέρος και διαδίδονται προς τα πάνω σε υψηλότερα επίπεδα.
    \item  Τα συναπτικά ρεύματα $\mathbf{I}$ δέχονται κορεσμό $\alpha$ σε κάθε βήμα και τροφοδοτούνται στα δυναμικά της μεμβράνης  $\mathbf{U}$. Ο κορεσμός των $\mathbf{U}$ με την πάροδο του χρόνου χαρακτηρίζονται από $\beta .$
    \item Οι συρμοί αιχμής $S$ δημιουργούνται εφαρμόζοντας μια μη γραμμικότητα ορίου στα δυναμικά της μεμβράνης $\mathrm{U}$ σε κάθε χρονικό βήμα.
    \item  Οι ακίδες επηρεάζουν αιτιακά την κατάσταση του δικτύου (κόκκινες συνδέσεις).
    
\end{itemize}
Οι ερευνητές μας ενημερώνουν επίσης για το πώς μπορούν να επικοινωνηθούν οι αιχμές μέσα στο δίκτυο.
Πρώτον, κάθε ακίδα προκαλεί επαναφορά του δυναμικού μεμβράνης του νευρώνα που εκπέμπει την ακίδα. Δεύτερον, κάθε ακίδα μπορεί να τροφοδοτηθεί στον ίδιο νευρωνικό πληθυσμό μέσω επαναλαμβανόμενων συνδέσεων $\mathbf{V}^{(1)}$. Μπορεί επίσης να τροφοδοτηθεί $\mathbf{W}^{(2)}$ σε ένα επίπεδο δικτύου πιο κάτω ή, να τροφοδοτηθεί σε ένα επίπεδο ανάγνωσης στο οποίο ορίζεται μια συνάρτηση κόστους.
\subsubsection{Πρόβλημα εκχώρησης πίστωσης}
Όπως σε κάθε κανόνα εκμάθησης, το πρώτο βήμα είναι να επιλέξουμε μια συνάρτηση κόστους/απώλειας που μειώνεται όταν το δίκτυο αρχίσει να μαθαίνει αυτό που θέλουμε να μάθει. Το δεύτερο βήμα είναι να επιλέξουμε πώς τα βάρη των συνδέσεων ενημερώνονται κατά τη διάρκεια της εκπαίδευσης για να μειώσουμε τη τιμή της συνάρτησης κόστους/απώλειας. Αυτό ονομάζεται πρόβλημα εκχώρησης πίστωσης. Ένας τρόπος για να λυθεί αυτό το πρόβλημα είναι να χρησιμοποιηθεί η ανάθεση χωρικής πίστωσης με \textlatin{back-propagation} η οποία έχει σημαντικό κόστος μνήμης καθώς οι παράγωγοι πρέπει να επικοινωνηθούν ξανά στο δίκτυο αφού αποθηκεύσουμε όλες τις καταστάσεις νευρώνων. Ας δούμε πώς λειτουργεί το \textlatin{back-propagation} σε επαναλαμβανόμενα νευρωνικά δίκτυα (\textlatin{RNN}):
Το \textlatin{back-propagation} στα \textlatin{RNN} μπορεί να εφαρμοστεί με "ξετύλιγμα": δημιουργείται ένα βοηθητικό δίκτυο κάνοντας αντίγραφα του δικτύου για κάθε χρονικό βήμα. Ο ίδιος κανόνας μπορεί να εφαρμοστεί στα \textlatin{SNN}. Σε αυτήν την περίπτωση η επανάληψη είναι "ξετυλιγμένη" (σχήμα \ref{fig:unroll} ) που σημαίνει ότι δημιουργείται ένα βοηθητικό δίκτυο δημιουργώντας αντίγραφα του δικτύου για κάθε χρονικό βήμα.
\begin{figure}[htp]
    \centering
    \includegraphics[width=6cm]{Learning Methods/surrogate/unroll.PNG}
    \caption{}
    \label{fig:unroll}
\end{figure}


Το ξετυλιγμένο δίκτυο είναι απλά ένα βαθύ δίκτυο με κοινά βάρη τροφοδοσίας $\mathbf{W}^{(l)}$ και επαναλαμβανόμενα βάρη $\mathbf{V}^{(l)}$, στα οποία ισχύει το τυπικό \textlatin{BP}:
\selectlanguage{english}
\begin{equation}
\Delta W_{i j}^{(l)} \propto \frac{\partial}{\partial W_{i j}^{(l)}} \mathcal{L}[n]=\sum_{m=0}^{t} \delta_{i}^{(l)}[m] y_{j}^{(l-1)}[m]
\end{equation}
\begin{equation}
\Delta V_{i j}^{(l)} \propto \frac{\partial}{\partial V_{i j}^{(l)}} \mathcal{L}[n]=\sum_{m=1}^{t} \delta_{i}^{(l)}[s] y_{j}^{(l)}[m-1]
\end{equation}
\begin{equation}
\delta_{i}^{(l)}[n]=\sigma^{\prime}\left(a_{i}^{(l)}[n]\right)\left(\sum_{k} \delta_{k}^{(l+1)}[n] W_{i k}^{\top, l}+\sum_{k} \delta_{k}^{(l)}[n+1] V_{i k}^{\top, l}\right)
\end{equation}
\selectlanguage{greek}
Οι υπεργραφές $l=0, \ldots, L$ δηλώνουν το επίπεδο $(0$ είναι είσοδος, $L$ είναι έξοδος). Με $\alpha_{i}^{(l)}[n]=\sum_{j} W_{i j} x_{j}$ δηλώνουμε τη συνολική είσοδο στον νευρώνα \textlatin{i}, $y_{j}$ είναι η έξοδος του νευρώνα \textlatin{j} και $\eta$ ένας μικρός ρυθμός εκπαίδευσης. Επίσης, $\sigma^{\prime}$ είναι η παράγωγος της συνάρτησης ενεργοποίησης και το $\delta_{i}^{(l)}$ είναι το σφάλμα του νευρώνα εξόδου $i$ και $T$ δηλώνει τον ανάστροφο πίνακα. Η εφαρμογή BP σε ένα ξετυλιγμένο δίκτυο ονομάζεται \textlatin{BackPropagation Through Time} (\textlatin{BPTT}).

Ο δεύτερος τρόπος επίλυσης αυτού του προβλήματος είναι η χρήση μιας χρονικής ανάθεσης πίστωσης (σχήμα \ref{fig:computationgraph} θεωρείται προσωρινό πρόβλημα εκχώρησης πιστώσεων) . Αυτός ο τύπος χρονικής ανάθεσης μπορεί να υποδιαιρεθεί περαιτέρω στις μεθόδους "προς τα πίσω" και "προς τα εμπρός".
\begin{enumerate}
  
    \item προς τα πίσω: Μπορούμε να χρησιμοποιήσουμε το \textlatin{BPTT} όπως στην ανάθεση χωρικής πίστωσης, διαδίδουμε τα σφάλματα αντίστροφα στον χρόνο μετά από μια προώθηση.
    \item προς τα εμπρός: Όλες οι απαραίτητες πληροφορίες για τον υπολογισμό των παράγωγων διαδίδονται προς τα εμπρός \cite{williams1989}. Για παράδειγμα, η "παράγωγος προς τα εμπρός" του βάρους τροφοδοσίας $\mathbf{W}$ γίνεται:
\end{enumerate}
\selectlanguage{english}
\begin{equation*}
\Delta W_{i j}^{m} \propto \frac{\partial \mathcal{L}[n]}{\partial W_{i j}^{m}}=\sum_{k} \frac{\partial \mathcal{L}[n]}{\partial y_{k}^{(L)}[n]} P_{i j k}^{L, m}[n]
P_{i j k}^{(l, m)}[n]=\frac{\partial}{\partial W_{i j}^{m}} y_{k}^{(l)}[n]
\end{equation*}
\begin{equation*}
P_{i j k}^{(l, m)}[n]=\sigma^{\prime}\left(a_{k}^{(l)}[n]\right)\left(\sum_{j^{\prime}} V_{i j^{\prime}}^{(l)} P_{i j j^{\prime}}^{(l, m)}[n-1]+\sum_{j^{\prime}} W_{i j^{\prime}}^{(l)} P_{i j j^{\prime}}^{(l-1, m)}[n-1]+\delta_{l m} y_{i}^{(l-1)}[n-1]\right)
\end{equation*}
\selectlanguage{greek}

Οι παράγωγοι σε σχέση με τα επαναλαμβανόμενα βάρη $V_{i j}^{(l)}$ μπορούν επίσης να υπολογιστούν παρομοίως. Η μέθοδος βελτιστοποίησης προς τα πίσω είναι πιο αποτελεσματική από άποψη υπολογισμού, αλλά απαιτεί τη διατήρηση όλων των εισόδων και των ενεργοποιήσεων για κάθε χρονικό βήμα. Έτσι, η πολυπλοκότητα χώρου για κάθε στρώμα είναι $O(N T)$, όπου $N$ είναι ο αριθμός των νευρώνων ανά επίπεδο και $T$ είναι ο αριθμός των χρονικών βημάτων. Η μέθοδος προώθησης προς τα μπροστά απαιτεί τη διατήρηση των μεταβλητών $P_{i j k}^{(l, m)}$, με αποτέλεσμα την πολυπλοκότητα $O\left(N^{3}\right)$ ανά επίπεδο. Ωστόσο, εάν εφαρμοστούν απλοποιήσεις, αυτή η πολυπλοκότητα μπορεί να μειωθεί στο O (N), όπως στον κανόνα μάθησης \textlatin{Decolle} που περιγράφουμε αργότερα. Οι απλουστεύσεις μπορούν επίσης να μειώσουν την υπολογιστική πολυπλοκότητα.
Ωστόσο, οι παραπάνω αλγόριθμοι δεν μπορούν να εφαρμοστούν άμεσα. Ένα ζήτημα είναι η μη-παραγωγισιμή μη γραμμικότητα της ακίδας. Η παράγωγος της συνάρτησης νευρικής ενεργοποίησης $\sigma^{\prime} \equiv \frac{\partial y_{i}^{(l)}}{\partial a_{i}^{(l)}}$ είναι  πρόβλημα γιατί για έναν νευρώνα ακίδας, έχουμε $S(U(t))=\Theta(U(t)-\vartheta)$, του οποίου η παράγωγος είναι μηδενική παντού εκτός από το $U=\vartheta$, όπου είναι ασαφές . Το τυπικό \textlatin{BP} εκτός από τον ακριβό υπολογισμό, τις απαιτήσεις επικοινωνίας μνήμης, δεν είναι κατάλληλη για νευρομορφικό υλικό ούτε είναι βιολογικά ρεαλιστική. Αυτός ο τύπος υλικού έχει απαιτήσεις τοπικότητας που απαγορεύουν τo \textlatin{BP}. Η μέθοδος προς τα εμπρός μπορεί να είναι πιο εφαρμόσιμη, ωστόσο η κλιμάκωση των παραπάνω μεθόδων δεν είναι κατάλληλη για πολλά μοντέλα \textlatin{SNN} .
Η επίλυση του πρώτου έχει πολλές προσεγγίσεις:

\begin{enumerate}
\item Χρησιμοποιώντας βιολογικά εμπνευσμένους κανόνες τοπικής μάθησης για τις κρυφές ενότητες
\item Μετατροπή συμβατικά εκπαιδευμένων νευρωνικών δικτύων "βάσει ποσοστών" σε SNN.
\item Εξομάλυνση του μοντέλου δικτύου ώστε να είναι συνεχώς διαφορικό.
\item Ορισμός υποκατάστατης παραγώγου (\textlatin{Surrogate Gradient}-\textlatin{SG}) ως συνεχής χαλάρωση
\end{enumerate}

Η κύρια συμβολή αυτής της έρευνας είναι για την τελευταία προσέγγιση, χρησιμοποιώντας υποκατάστατες παράγωγων αλλά επίσης μας ενημερώνει για τις μεθόδους εξομάλυνσης. Ωστόσο, μας ενδιαφέρει μόνο ο ορισμός της προσέγγισης υποκατάστατης παραγώγου.
\subsubsection{Προσέγγιση υποκατάστατων παράγωγων}
Η προσέγγιση υποκατάστατων παράγωγων μπορεί να χωριστεί περαιτέρω σε δύο προσεγγίσεις:
\begin{enumerate}
    \item \textlatin{SG} που συνθέτουν μια συνεχή χαλάρωση
της μη ομαλής μη γραμμικότητας της συνάρτησης ακίδας. Αυτό δεν επηρεάζει τον αλγόριθμο βελτιστοποίησης.\ref{fig:sg}
    \item \textlatin{SG} που επηρεάζουν το "\textlatin{locality}" των ίδιων των υποκείμενων αλγορίθμων βελτιστοποίησης για τη βελτίωση της υπολογιστικής ή/και πρόσβασης στη μνήμη των γενικών διαδικασιών μάθησης.
\end{enumerate}
Ένα σημαντικό πλεονέκτημα με το Surrogate Gradients είναι οτι δεν χρειάζεται να καθορίσουμε ποια μέθοδος κωδικοποίησης θα χρησιμοποιηθεί στα κρυμμένα επίπεδα. Στην πρώτη προσέγγιση οι ερευνητές  αντικαθιστούν την παράγωγο της συνάρτησης ακίδας με την παράγωγο μιας ομαλής συνάρτησης. Αυτή η προσέγγιση χρησιμοποιείται στο \textlatin{Decolle} και είναι αρκετά απλή στην εφαρμογή και εύχρηστη για εφαρμογή, καθώς μπορεί να συνδυαστεί με εργαλεία αυτόματης διαφοροποίησης. Ο αλγόριθμος \textlatin{SuperSpike} χρησιμοποιεί έναν κανόνα μάθησης τριών παραγόντων σε πραγματικό χρόνο (\textlatin{online}) χρησιμοποιώντας ένα γρήγορο σιγμοειδές για την κατασκευή ενός \textlatin{SG}. Τα Surrogate Gradients χρησιμοποιούνται επίσης στο \textlatin{e-Prop} το οποίο θα εξετάσουμε μετά το \textlatin{Decolle}.
\subsubsection{Προβλήματα των υποκατάστατων παράγωγων}

Καθώς η χρήση των \textlatin{SG} για την εκπαίδευση των \textlatin{SNN} προχωρά σε βαθύτερες αρχιτεκτονικές, πιθανότατα να προκύψουν περισσότερα θέματα, παρόμοια με αυτά που εμφανίζονται στα \textlatin{ANN}. SGs που σχηματίζονται από συναρτήσεις σιγμοειδούς ενεργοποίησης, οι οποίες έχουν προβλήματα "εξαφάνισης παραγώγων" (\textlatin{vanishing gradients}). Ένα άλλο ζήτημα είναι η πιθανή προκατάληψη που εισάγουν τα \textlatin{SG} στη δυναμική της μάθησης.

\begin{figure}[htp]
    \centering
    \includegraphics[width=6cm]{Learning Methods/surrogate/SG.jpg}
    \caption{Για περισσότερες λεπτομέρειες, ανατρέξτε στο πρωτότυπο χαρτί Surrogate Gradient \cite{Neftci2019}. Αυτό που είναι σημαντικό εδώ είναι να δούμε πώς τα \textlatin{SG} μας επιτρέπουν να έχουμε μια συνεχή συνάρτηση.}
    \label{fig:sg}
\end{figure}
\subsubsection{Συμπεράσματα}
Σε αυτό το κεφάλαιο εισαγάγαμε τον αναγνώστη στα κύρια ζητήματα στην εκπαίδευση των νευρωνικών δικτύων και πώς αυτά εμποδίζουν τα \textlatin{SNN} να φτάσουν την ικανότητα των ANN. Η αντιστοίχιση σε \textlatin{RNN} επιτρέπει στα \textlatin{SNN} να είναι πιο προσιτά σε μηχανικούς μηχανικής μάθησης που δεν έχουν νευροεπιστημονικό υπόβαθρο και καθιστά ευκολότερη την κατανόηση των προβλημάτων κλιμάκωσης σε μεγάλους όγκους δεδομένων . Αυτό το κεφάλαιο δεν εισάγει κάποιο νέο αλγόριθμο εκμάθησης , αλλά μας επιτρέπει να εφαρμόσουμε \textlatin{BPTT} με τη χρήση \textlatin{SG}. Τα \textlatin{SG} είναι μια πολύ επιτυχημένη προσέγγιση στην επίλυση του ζητήματος μη διαφοροποιήσης, όπως φαίνεται από τον αριθμό των νέων αλγορίθμων SNN που τα περιλαμβάνουν. Περιλάβαμε αυτήν την υποενότητα διότι όλοι οι ακόλουθοι αλγόριθμοι στη διατριβή μας περιλαμβάνουν \textlatin{SG} και γιατί θα εκπαιδεύσουμε δίκτυα \textlatin{SNN} με \textlatin{BPTT} με χρήση \textlatin{SG}.

\subsection{\textlatin{SuperSpike}: Εποπτευόμενη μάθηση σε πολυστρωματικά νευρωνικά δίκτυα ακίδων}
Το \textlatin{Superspike} είναι μια προσέγγιση υποκαταστάσης παραγωγού, μια μη γραμμική τριών παραγόντων που βασίζεται στην τάση.Ενας κανόνας μάθησης ικανός να εκπαιδεύσει δίκτυα πολλαπλών στρωμάτων ντετερμινιστικών νευρώνων \textlatin{LIF} για να εκτελούν μη γραμμικούς υπολογισμούς σε χωροχρονικά μοτίβα ακίδων. Με τη μετάφραση του \textlatin{backpropagation} στον τομέα των ακίδων, είναι μία από τις λίγες πρωτοβουλίες για την αντιμετώπιση της δυσκολίας εκπαίδευσης των \textlatin{SNN }με κρυφές μονάδες για την επεξεργασία ακριβών χρονομετρημένων σειρμών ακίδων εισόδου και εξόδου.

Η μερική παράγωγος αυτής της προσέγγισης είναι της μορφής $\partial S_{i}(t) / \partial w_{i j}$ όπου $S_{i}(t)=\sum_{k} \delta\left(t-t_{i}^{k}\right)$ είναι ο συρμός ακίδων του κρυμμένου νευρώνα $i$ και $w_{i j}$ είναι ένα κρυφό βάρος.

Σε σύγκριση με άλλες μεθόδους στη βιβλιογραφία, το \textlatin{Superspike} επιτρέπει σε δίκτυα πολλαπλών στρωμάτων ντερεμινιστικών νευρώνων \textlatin{LIF} να επιλύουν εργασίες που περιλαμβάνουν μετασχηματισμούς μοτίβου χωροχρονικών ακίδων χωρίς ανάγκη για έγχυση θορύβου, ακόμη και όταν οι κρυφές μονάδες είναι αρχικά εντελώς αθόρυβες. Αντί του μετασυναπτικού συρμού ακίδων, η μερική παράγωγος των εξόδων της κρυφής μονάδας προσεγγίζεται ως το προϊόν του φιλτραρισμένου συρμού ακίδων και μια μη γραμμική συνάρτηση της μετασυναπτικής τάσης.

\subsubsection{Κανόνας μάθησης}
Είναι επιθυμητό ένας μεμονωμένος νευρώνας LIF να εκπέμπει δεδομένο σειρμό ακίδων εξόδου $\hat{S}_{i}$ για μια δεδομένη είσοδο. Αυτό το πρόβλημα μπορεί να θεωρηθεί πρόβλημα βελτιστοποίησης της ελαχιστοποίησης της απόστασης \textlatin{van Rossum} \cite{rossum2001} μεταξύ $\hat{S}_{i}$ και του πραγματικού σειρμού ακίδων εξόδου $S_{i}$. Αρχικά, ας περιγράψουμε τι είναι η απόσταση του \textlatin{van Rossum} . Η απόσταση \textlatin{van Rossum} στοχεύει στην επίλυση του προβλήματος της διάκρισης μεταξύ δύο σειρμών ακίδων . Ο  \textlatin{van Rossum} εισήγαγε ένα μέτρο για την απόσταση μεταξύ των δυο αυτών σειρμών.
Με στόχο ένα απλό μέτρο απόστασης, δεδομένου ενός σειρμού ακίδων με χρόνους πυροδότησης $t_{i}$
\selectlanguage{english}
\begin{equation}
f^{\text {orig }}(t)=\sum_{i}^{M} \delta\left(t-t_{i}\right)
\end{equation}
\selectlanguage{greek}
όπου υποτίθεται ότι όλα τα $t_{i}>0$. Η συνάρτηση δέλτα που σχετίζεται με κάθε ακίδα αντικαθίσταται με μια εκθετική συνάρτηση, δηλαδή μια εκθετική "ουρά" προστίθεται σε όλες τις ακίδες,
\selectlanguage{english}
\begin{equation}
f(t)=\sum_{i}^{M} H\left(t-t_{i}\right) e^{-\left(t-t_{i}\right) / t_{c}}
\end{equation}
\selectlanguage{greek}
\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{Learning Methods/superspike/vanrossum/vanrossum.jpg}
    \caption{(Πάνω) Δύο συρμοί ακίδων (το ένα αναποδογυρισμένο) συνελλίσονται με εκθετικό με σταθερά χρόνου $t_{c}$. (Κάτω) Η διαφορά στο τετράγωνο των σειρμών. Το ολοκλήρωμα της καμπύλης δίνει την επιθυμητή απόσταση
    \cite{rossum2001}
}
    \label{fig:vanrossum}
\end{figure}

$t_{c}$ είναι η χρονική σταθερά της εκθετικής συνάρτησης και $H$ είναι η βηματική συνάρτηση \textlatin{Heaviside}  $(H(x)=0$ εάν $x<0$ καί $H(x)=1$ εάν $x \geq 0) .$ . Οποιαδήποτε συνάρτηση μπορεί να χρησιμοποιηθεί για συνέλιξη με άλλη από την εκθετική, αλλά η εκθετική επιλέχθηκε λόγω της βιολογικής της εγγύτητας. Η απόσταση μεταξύ δύο συρμών $f$ και $g$ ορίζεται ως (βλέπε Εικ. \ref{fig:vanrossum})
\selectlanguage{english}
\begin{equation}
D^{2}(f, g)_{t_{c}}=\frac{1}{t_{c}} \int_{0}^{\infty}[f(t)-g(t)]^{2} d t
\end{equation}
\selectlanguage{greek}
Η απόσταση είναι η Ευκλείδεια απόσταση των δύο φιλτραρισμένων συρμών ($t_{c}$ ως ελεύθερη παράμετρος).
Για να καταλάβουμε την έννοια αυτής της απόστασης ας δούμε το ακόλουθο παράδειγμα. Λαμβάνουμε υπόψη μας τις δυο οριακές τιμές του $t_{c}.$ Για $t_{c}$ πολύ μικρότερα από το μεσοδιάστημα των ακίδων, οι επιχρισμένες συναρτήσεις $f$ και $g$ συνεισφέρουν στο ολοκλήρωμα μόνο εάν οι ακίδες δεν απέχουν περισσότερο από $t_{c}$. Αυτό μας θυμίζει τον εντοπισμό συμπτώσεων (\textlatin{"coincidence detection"}). Για του περισσότερους συρμούς ακίδων, οι συμπίπτουσες αιχμές μπορούν να αγνοηθούν στο όριο του μηδενικού $t_{c}$. έτσι, αν το $f$ περιέχει $M$ και η $g$ περιέχει $N$ αιχμές, έχουμε:
\selectlanguage{english}
\begin{equation}
\lim _{t_{c} \rightarrow 0} D^{2}(f, g)_{t_{c}}=\frac{1}{t_{c}} \int_{0}^{\infty}\left[f^{2}(t)+g^{2}(t)\right] d t=\frac{M+N}{2}
\end{equation}
\selectlanguage{greek}
Αυτή η απόσταση βασικά μετρά τις μη συμπίπτουσες ακίδες.
Ωστόσο, για μεγάλα $t_{c}$, η κύρια συνεισφορά στο ολοκλήρωμα προέρχεται από χρονικές στιγμές που η τελευταία ακίδα έχει ήδη συμβεί αλλά ο εκθέτης δεν έχει ακόμη υποστεί πλήρης κόρεσμο. Το ολοκλήρωμα επηρεάζεται κυρίως από τις χρονικές στιγμές που έχει περάσει η τελευταία ακίδα αλλά ο εκθέτης δεν έχει ακόμη υποστεί πλήρης κορεσμό. Υποθέτοντας ότι το $f(g)$ περιέχει $M(N)$ αιχμές, μπορεί κανείς να κάνει την εξής προσέγγιση
\selectlanguage{english}
\begin{equation}
\lim _{t_{c} \rightarrow \infty} D^{2}(f, g)_{t_{c}}=\frac{1}{t_{c}} \int_{0}^{\infty}\left(M e^{-t / t_{\mathrm{c}}}-N e^{-t / t_{c}}\right)^{2} d t=\frac{(M-N)^{2}}{2}
\end{equation}
\selectlanguage{greek}
In this limit, $D$ measures the difference in total spike count.The upper limit of the integral needs to be taken to be infinity as we interested for the tail of the last spike that died out rather than the time of the last spike

The distance thus interpolates between the two extremes of coincidence detection and measuring difference in total spike count. After filtering, the spike trains this leaves a sum of $N+M$ terms.
Changing integration variables leads to an alternative expression for the distance,

Σε αυτό το όριο, το $D$ μετρά τη διαφορά στο συνολικό αριθμό ακίδων. Το ανώτατο όριο του ολοκληρώματος πρέπει να θεωρηθεί ως το άπειρο, καθώς μας ενδιαφέρει η ουρά της τελευταίας ακίδας που υπέστη πλήρης κορεσμό και όχι ο χρόνος της τελευταίας ακίδας.

Η απόσταση παρεμβάλλεται έτσι μεταξύ των δύο άκρων της ανίχνευσης σύμπτωσης και της μέτρησης της διαφοράς στο συνολικό αριθμό ακίδων. Μετά το φιλτράρισμα, οι συρμοί ακίδων αφήνουν ένα ποσό $N+M$ όρων.
Η αλλαγή των μεταβλητών ολοκλήρωσης οδηγεί σε μια εναλλακτική έκφραση για την απόσταση,
\selectlanguage{english}
\begin{equation}
D^{2}(f, g)=\frac{1}{2} \int_{-\infty}^{\infty} C_{f-g, f-g}(t) e^{-|t| t_{c}} d t
\end{equation}
\selectlanguage{greek}

όπου $C_{f-g, f-g}(t)$ είναι η αυτοσυσχέτιση της διαφοράς των αρχικών συρμών, $f^{\text {orig }}(t)-g^{\text {orig }}(t) .$ Αυτό δείχνει ότι η απόσταση μπορεί να αναπαρασταθεί ως σταθμισμένο ολοκλήρωμα επί του αυτοσυσχέτισης, με τη στάθμιση να ποικίλλει με $t_{c}$. Αποδεικνύει επίσης ότι η απόσταση δεν επηρεάζεται από την αντιστροφή του χρόνου, δηλαδή, η απόσταση θα ήταν η ίδια εάν οι εκθετικές ουρές ήταν προσαρτημένες στις ακίδες στις αντίθετες πλευρές. Όσον αφορά την επιλογή του εκθετικού, η συνέλιξη σε υψηλότερη -τάξης νευρώνα για μικρές και μεσαίες περιόδους μπορεί να διαβαστεί ως μετασυναπτικά δυναμικά. Μεγαλύτερο $t_{c}$ φαίνεται να εξυπηρετείται καλύτερα από βραδύτερα ρεύματα που προκαλούν δεύτεροι αγγελιοφόροι ή παρουσία ασβεστίου .
Τώρα ας συνεχίσουμε με το πρόβλημα βελτιστοποίησης της ελαχιστοποίησης της απόστασης \textlatin{van Rossum} \cite {rossum2001} μεταξύ $\hat{S}_{i}$ και του πραγματικού συρμού εξόδου $S_{i}$:
\begin{equation}
L=\frac{1}{2} \int_{-\infty}^{t} d s\left[\left(\alpha * \hat{S}_{i}-\alpha * S_{i}\right)(s)\right]^{2}
\end{equation}

όπου $\alpha$ είναι ένας κανονικοποιημένος ομαλός πυρήνας χρονικής συνέλιξης. Επειδή μπορούν να υπολογιστούν εύκολα σε πραγματικό χρόνο και να εφαρμοστούν ως ηλεκτρικά ή χημικά ίχνη στη νευροβιολογία, χρησιμοποιούνται διπλοί εκθετικοί αιτιώδεις πυρήνες. Υπολογισμός της παραγώγου της παραπάνω εξίσωσης ως προς τα συναπτικά βάρη $w_{i j}$:
\selectlanguage{english}
\begin{equation}
\frac{\partial L}{\partial w_{i j}}=-\int_{-\infty}^{t} d s\left[\left(\alpha * \hat{S}_{i}-\alpha * S_{i}\right)(s)\right]\left(\alpha * \frac{\partial S_{i}}{\partial w_{i j}}\right)(s)
\end{equation}
\selectlanguage{greek}
στο οποίο η παράγωγος $\partial S_{i} / \partial w_{i j}$ είναι η παράγωγο του συρμού ακίδων. Αυτή η παράγωγος για τα περισσότερα μοντέλα νευρώνων είναι μηδενικό εκτός από τους χρόνους ακίδων κατά τους οποίους δεν ορίζεται. Αυτό είναι ένα τεράστιο πρόβλημα για το \textlatin{backpropagation}. Συνήθως οι αλγόριθμοι εκπαίδευσης αποφεύγουν αυτό το πρόβλημα είτε εκτελώντας βελτιστοποίηση απευθείας στο δυναμικό της μεμβράνης $U_{i}$ είτε εισάγοντας θόρυβο που κάνει την πιθανότητα του συρμού $\left\langle S_{i}(t)\right\rangle$ ομαλή συνάρτηση του δυναμικού της μεμβράνης. Το \textlatin{Superspike} συνδυάζει αυτές τις δύο προσεγγίσεις αντικαθιστώντας το τρένο αιχμής $S_{i}(t)$ με μια συνεχή βοηθητική (\textlatin{auxiliary}) συνάρτηση $\sigma\left(U_{i}(t)\right)$  του δυναμικού της μεμβράνης. Για λόγους επιδόσεων, επιλέγουμε $\sigma(U)$ ως την αρνητική πλευρά ενός γρήγορου σιγμοειδούς. Αυτή η "βοηθητική" συνάρτηση "αντικαθιστά το παράγωγο του συρμού ακίδων ως εξής:
\selectlanguage{english}
\begin{equation}
\frac{\partial S_{i}}{\partial w_{i j}} \quad \rightarrow \quad \sigma^{\prime}\left(U_{i}\right) \frac{\partial U_{i}}{\partial w_{i j}}
\end{equation}
\selectlanguage{greek}
Για να υπολογίσoυμε την παράγωγο $\partial U_{i} / \partial w_{i j}$ στην παραπάνω έκφραση, για μοντέλα \textlatin{LIF} το δυναμικό μεμβράνης  $U_{i}(t)$ ως μοντέλο απόκρισης αιχμής:
\selectlanguage{english}
\begin{equation}
U_{i}(t)=\sum_{j} w_{i j}\left(\epsilon * S_{j}(t)\right)+\left(\eta * S_{i}(t)\right)
\end{equation}
\selectlanguage{greek}

Ο πυρήνας αιτιώδους μεμβράνης $\epsilon$ αντιστοιχεί στο σχήμα του μετασυναπτικού δυναμικού ( \textlatin{postsynaptic potential}-\textlatin{PSP}) και το $\eta$ περιγράφει τη δυναμική των αιχμών και την επαναφορά του δυναμικού μεμβράνης. Λόγω του δεύτερου όρου, το $U_{i}$ εξαρτάται από το παρελθόν του μέσω της εξόδου του τρένου $S_{i}$.

Η εξάρτηση από το παρελθόν του δεν μας επιτρέπει να υπολογίσουμε την παράγωγο $\frac{{\partial U}_{i}}{\partial w_{i j}}$ απευθείας. Ωστόσο, διασφαλίζοντας χαμηλούς ρυθμούς πυροδότησης (με την προσθήκη ομοιοστατικών μηχανισμών που ρυθμίζουν τα επίπεδα νευρωνικής δραστηριότητας) μπορούμε να αγνοήσουμε τον δεύτερο όρο.

Αγνοώντας τον δεύτερο όρο, η εξίσωση γίνεται η φιλτραρισμένη προσυναπτική δραστηριότητα $\frac{\partial U_{i}}{\partial w_{i j}} \approx\left(\epsilon * S_{j}(t)\right)$. Βιολογικά, αυτό μπορεί να ερμηνευτεί ως η συγκέντρωση νευροδιαβιβαστών στη σύναψη.

Αντικαθιστώντας αυτήν την προσέγγιση πίσω στην Εξίσωση. (4.26), ο κανόνας μάθησης \textlatin{gradient descent} για έναν μόνο νευρώνα γίνεται:
\selectlanguage{english}
\begin{equation}
\label{eqn:superspike}
\frac{\partial w_{i j}}{\partial t}=r \int_{-\infty}^{t} d s \underbrace{e_{i}(s)}_{\text {Error signal }} \underbrace{\alpha * \underbrace{\sigma^{\prime}\left(U_{i}(s)\right)}_{\text {Post }} \underbrace{\left(\epsilon * S_{j}\right)(s)}_{\text {Pre }})}_{\equiv \lambda_{i j}(s)}
\end{equation}
\selectlanguage{greek}
The learning rate is symbolized with $r$ and short notation for the output error signal $e_{i}(s) \equiv \alpha *\left(\hat{S}_{i}-S_{i}\right)$ and the eligibility trace (a temporary
record of the occurrence of an event)  $\lambda_{i j} .$ In practice the expression  is evualated on minibatches and Superspike suggests using a per-parameter learning rate $r_{i j}$ to speed up learning.

Ο ρυθμός μάθησης συμβολίζεται με $r$ και για το σήμα σφάλματος εξόδου έχουμε $e_{i}(s) \equiv \alpha *\left(\hat{S}_{i}-S_{i}\right)$ και το \textlatin{eligibility trace} (προσωρινή
καταγραφή της εμφάνισης ενός συμβάντος) είναι $\lambda_{i j} .$ Στην πράξη, η έκφραση υπολογίζεται σε μικρές παρτίδες και προτείνεται χρήση ενός ρυθμού εκμάθησης ανά παράμετρο $r_{i j}$ για να επιταχυνθεί η μάθηση.

Η τελευταία εξίσωση αντιστοιχεί στον κανόνα μάθησης \textlatin{SuperSpike} για τον νευρώνα εξόδου $i$.
Επαναπροσδιορίζοντας το σήμα σφάλματος $e_{i}$ ως σήμα ανατροφοδότησης, μπορεί να χρησιμοποιηθεί επίσης ο ίδιος κανόνας για κρυφούς νευρώνες. Σημαντικές ιδιότητες:
\begin{itemize}
  \item Περιλαμβάνει έναν όρο \textlatin{Hebbian}  που συνδυάζει την προσυναπτική με την μετασυναπτική δραστηριότητα
  \item Ο κανόνας μάθησης βασίζεται στην τάση
  \item είναι ένας μη γραμμικός \textlatin{Hebbian} κανόνας λόγω της εμφάνισης του $\sigma^{\prime}\left(U_{i}\right)$
  \item Η αιτιώδης συνέλιξη με το $\alpha$ λειτουργεί ως \textlatin{eligibility trace} για την επίλυση του προβλήματος της απομακρυσμένης ανταμοιβής (\textlatin{distal reward problem}) λόγω των σημάτων σφάλματος που έρχονται μετά από την εμφάνιση ενός σφάλματος στην εκπαίδευση \cite{izike2007}
  \item Είναι ένας κανόνας τριών παραγόντων στον οποίο το σήμα σφάλματος παίζει το ρόλο ενός τρίτου παράγοντα \cite{fremaux2016} . Το σήμα σφάλματος είναι συγκεκριμένο για τον μετασυναπτικό νευρώνα.
\end{itemize}

Το "πρόβλημα μακρινής ανταμοιβής" είναι ένα αιτιολογικό αίνιγμα που εμφανίζεται όταν οι ανταμοιβές φτάνουν δευτερόλεπτα μετά από γεγονότα που προκαλούν ανταμοιβή. Πώς αναγνωρίζει ο εγκέφαλος ποια μοτίβα πυροδότησης των οποίων οι νευρώνες είναι υπεύθυνοι για την ανταμοιβή εάν 1) τα μοτίβα δεν είναι πλέον εκεί όταν φτάσει η ανταμοιβή και 2) όλοι οι νευρώνες και οι συνάψεις είναι ενεργές κατά την περίοδο αναμονής της ανταμοιβής;

\subsubsection{Μοντέλο Νευρώνα}

Το μοντέλο νευρώνων \textlatin{LIF} με συναπτική είσοδο ρεύματος \textlatin{"current-based input"} χρησιμοποιείται στην μορφή ολοκληρώματος. Για την προσομοίωση της δυναμικής της μεμβράνης, η τάση $U_{i}$ του νευρώνα $i$ μπορεί να υπολογιστεί ως εξής:
\selectlanguage{english}
\begin{equation}
\label{eqn:lifvoltage}
\tau^{\mathrm{mem}} \frac{d U_{i}}{d t}=\left(U^{\text {rest }}-U_{i}\right)+I_{i}^{\mathrm{syn}}(t)
\end{equation}
in which the synaptic input current $I_{i}^{\text {syn }}(t)$ changes according to the following equation:
\begin{equation}
\frac{d}{d t} I_{i}^{\mathrm{syn}}(t)=-\frac{I_{i}^{\mathrm{syn}}(t)}{\tau^{\mathrm{syn}}}+\sum_{j \in \mathrm{pre}} w_{i j} S_{j}(t)
\end{equation}
\selectlanguage{greek}
Η τιμή των $I_{i}^{\mathrm{syn}}(t)$ πηδά κατά $w_{i j}$ τη στιγμή της άφιξης από προσυναπτικούς νευρώνες $S_{j}(t)=\sum_{k} \delta\left(t-t_{j}^{k}\right)$ .To $\delta$ υποδηλώνει τη λειτουργία \textlatin{Dirac} $\delta$ και τα $t_{j}^{k}(k=1,2, \cdots)$ είναι οι χρόνοι πυροδότησης νευρώνα $j .$ / Για να δείτε τις τιμές των σταθερών που χρησιμοποιήθηκαν στα πειράματα διαβάστε την πρωτότυπη έρευνα του \textlatin{Superspike} καθώς δεν μας ενδιαφέρουν οι λεπτομέρειες εφαρμογής εδώ αλλά περισσότερο η θεωρητική βάση της μεθόδου εκμάθησης αυτής.

\begin{itemize}
    \item Ένα δυναμικό δράσης (\textlatin{action potential}) ενεργοποιείται όταν η τάση μεμβράνης του νευρώνα $i$ φτάσει σε μια τιμή πάνω από μια τιμή κατωφλίου $\vartheta$ .
    \item Μετά από μια ακίδα, η τάση $U_{i}$ παραμένει σταθερή στα $U_{i}^{\latintext {rest }}$ για $\tau^{\mathrm{ref}}$ για να εξομοίωσουμε μια πυρίμαχη περίοδο \textlatin{refractory period}.
    \item  Μετά τη παραγωγή ακίδων, οι ακίδες διαδίδονται σε άλλους νευρώνες με αξονική καθυστέρηση.
\end{itemize}


\subsubsection{Μοντέλο πλαστικότητας}

Ο κανόνας μάθησης μπορεί να ερμηνευτεί ως ένας μη γραμμικός \textlatin{ Hebbian} κανόνας τριών παραγόντων. Ο μη γραμμικός \textlatin{ Hebbian} όρος ανιχνεύει συμπτώσεις μεταξύ προσυναπτικής δραστηριότητας και μετασυναπτικής εκπόλωσης. Αυτές οι χωροχρονικές συμπτώσεις στη μεμονωμένη σύναψη  $w_{i j}$ αποθηκεύονται στη συνέχεια παροδικά από τη χρονική συνέλιξη με τον αιτιώδη πυρήνα $\alpha$. Αυτό το βήμα μπορεί να ερμηνευτεί ως ένα συναπτικό \textlatin{eligibility trace}.

Όλες οι απαραίτητες ποσότητες υπολογίζονται σε πραγματικό χρόνο χωρίς την ανάγκη να διαδοθούν σήματα σφάλματος προς τα πίσω με την πάροδο του χρόνου. Έτσι, το \textlatin{Superspike} μπορεί να ερμηνευθεί ως μια υλοποίηση επαναλαμβανόμενης μάθησης σε πραγματικό χρόνο (\textlatin{ real-time recurrent learning }-\textlatin{RTRL}) \cite{williams1989} για νευρωνικά δίκτα ακίδων.

Στο μοντέλο, όλη η πολυπλοκότητα της νευρικής ανατροφοδότησης της μάθησης απορροφάται στο σήμα ανά νευρώνα $e_{i}(t) .$ . Επειδή δεν είναι σαφές εάν και πώς αποστέλλεται τέτοια εσφαλμένη ανατροφοδότηση σε μεμονωμένους νευρώνες στη βιολογία, οι συγγραφείς δοκίμασαν πολλούς τρόπους τους οποίους δεν θα αναφερθούμε σε βάθος. Η προαναφερθείσα εξίσωση του κανόνα μάθησης \textlatin{Superspike} ενσωματώνεται σε πεπερασμένα χρονικά διαστήματα πριν από την ενημέρωση των βαρών. Ο πλήρης κανόνας εκμάθησης μπορεί να γραφτεί ως εξής:

\selectlanguage{english}
\begin{equation}
\label{eqn:integratedsuperspike}
\Delta w_{i j}^{k}=r_{i j} \int_{t_{k}}^{t_{k+1}} \underbrace{e_{i}(s)}_{\text {Error signal }} \alpha *(\underbrace{\sigma^{\prime}\left(U_{i}(s)\right)}_{\text {Post }} \underbrace{\left(\epsilon * S_{j}\right)(s)}_{\text {Pre }}) d s
\end{equation}
\selectlanguage{greek}
The evaluation of the Superspike learning rule equation can be described as follows: i) evaluation of presynaptic traces, ii) evaluation of Hebbian coincidence and computation of synaptic eligibility traces, iii) computation and propagation of error signals, and iv) integration of the given Superspike equation and weight update. 

O υπολογισμός της εξίσωσης του κανόνα μάθησης \textlatin{Superspike} μπορεί να περιγραφεί ως εξής: 1) αξιολόγηση προσυναπτικών ιχνών΄(\textlatin{presynaptic traces}), 2) υπολογισμός της σύμπτωσης \textlatin{Hebbian} και υπολογισμός των \textlatin{eligibility traces}, 3) υπολογισμός και διάδοση σημάτων σφάλματος και 4) ολοκλήρωση της δοσμένης εξίσωσης \textlatin{Superspike} και αναπροσαρμογής βάρους.

\subsubsection{Προσυναπτικά ίχνη}
Επειδή το  $\epsilon$ είναι ένα διπλό εκθετικό φίλτρο, η χρονική συνέλιξη στην έκφραση των προσυναπτικών ιχνών (Εξ.\ref{eqn:integratedsuperspike}), μπορεί να αξιολογηθεί αποτελεσματικά σε πραγματικό χρόνο με εκθετικό φιλτράρισμα δύο φορές. Πρώτον, η ολοκλήρωση του μοναδικού εκθετικού ίχνους είναι:
\selectlanguage{english}
\begin{equation}
\frac{d z_{j}}{d t}=-\frac{z_{j}}{\tau_{\text {rise }}}+S_{j}(t)
\end{equation}
\selectlanguage{greek}
σε κάθε χρονικό βήμα το οποίο στη συνέχεια τροφοδοτείται σε μια δεύτερη εκθετική συστοιχία φίλτρων
\selectlanguage{english}
\begin{equation}
\tau_{\text {decay }} \frac{d \tilde{z}_{j}}{d t}=-\tilde{z}_{j}+z_{j}
\end{equation}
\selectlanguage{greek}
με την συνάρτηση $\tilde{z}_{j}(t) \equiv\left(\epsilon * S_{j}\right)(t)$ να προσαρμόζει το σχήμα ενός μετασυναπτικού δυναμικού.
\subsubsection{Ανίχνευση σύμπτωσης \textlatin{Hebbian}}
Για τον υπολογισμό του όρου \textlatin{Hebbian}, το \textlatin{SG} $\sigma^{\prime}\left(U_{i}\right)$ υπολογίζεται σε κάθε χρονικό βήμα.

Το εξωτερικό προϊόν μεταξύ των αργοπορημένων προσυναπτικών ιχνών $\tilde{z}_{j}(t-\Delta)$ και των υποκατάστατων μερικών παραγώγων $\sigma^{\prime}\left(U_{i}\right)(t-\Delta)$  υπολογίζεται σε κάθε χρονικό βήμα.
\subsubsection{Συναπτικά \textlatin{eligibility traces} }
To implement the synaptic eligibility trace as given by the temporal filter $\alpha$, the values of Hebbian product term are filtered with two exponential filters z\textsubscript{j} . these traces now need to be computed for each synapse $w_{i j}$ which makes the algorithm scale as $O\left(n^{2}\right)$ for $n$ being the number of neurons. For SuperSpike to function properly, it is important that these transients are long enough to coincide with any related error signal $e_{i}(t)$. The duration of the transient in the model is given by the filter kernel shape used to compute the van Rossum distance.

Για να υλοποιήσουμε το συναπτικό \textlatin{eligibility trace} όπως δίνεται από το χρονικό φίλτρο $\alpha$, οι τιμές του όρου \textlatin{Hebbian} φιλτράρονται με δύο εκθετικά φίλτρα z\textsubscript{\textlatin{j}}. Aυτά τα ίχνη πρέπει τώρα να υπολογιστούν για κάθε σύναψη $w_{i j}$  που κάνει την πολυπλοκότητα του αλγορίθμου $O\left(n^{2}\right)$ με $n$ να είναι ο αριθμός των νευρώνων. Για να λειτουργεί σωστά το \textlatin{Superspike}, είναι σημαντικό αυτά τα \textlatin{traces} να είναι αρκετά μεγάλα ώστε να συμπίπτουν με οποιοδήποτε σχετικό σήμα σφάλματος $e_{i}(t)$. Η διάρκεια του \textlatin{trace} στο μοντέλο δίνεται από το σχήμα του πυρήνα του φίλτρου που χρησιμοποιείται για τον υπολογισμό της απόστασης του \textlatin{van Rossum}.

\subsubsection{Σήματα σφάλματος}
Τα σήματα σφάλματος εξόδου συνδέονται με μονάδες εξόδου που έχουν συγκεκριμένο σήμα -στόχο. Τα στοιχεία τους καθορίζονται από την υποκείμενη συνάρτηση κόστους.

Στο επίπεδο των σφαλμάτων εξόδου μπορούν να χρησιμοποιηθούν δύο τρόποι για να μάθουμε την έξοδο. Για να μάθουμε ακριβείς αιχμές εξόδου, τα σήματα σφάλματος εξόδου δόθηκαν ακριβώς από $e_{i}=\alpha *\left(\hat{S}_{i}-S_{i}\right)$  για μονάδα εξόδου $i$. Όπως φαίνεται από την εξίσωση, το σήμα σφάλματος $e_{i}$ είναι μηδενικό μόνο εάν ο στόχος και ο συρμός εξόδου ταιριάζουν ακριβώς με τη χρονική ακρίβεια της προσομοίωσής μας. Όλες οι τιμές συνάρτησης κόστους μπορούν να υπολογιστούν ως το τετράγωνο μέσης ρίζας.

Ο δεύτερος τρόπος είναι να ταξινομήσουμε τα μοτίβα αιχμής εισόδου, εισαγάγωντας κάποια χαλάρωση στον υπολογισμό του σήματος σφάλματος. Η άμεση αρνητική ανατροφοδότηση σφάλματος δίνεται από $e_{i}=-\alpha * S_{i}^{\latintext {err }}$ για κάθε περιττή επιπλέον αύξηση $S_{i}^{\latintext {err }}$. Ένα θετικό σήμα ανατροφοδότησης δίνεται απο τον τύπο $e_{i}=\alpha * S_{i}^{\latintext {miss }}$ όταν ένα ερέθισμα απέτυχε να εκπέμψει μια ακίδα εξόδου όταν έπρεπε.

\subsubsection{Σήματα ανατροφοδότησης}
Τα σήματα ανατροφοδότησης, προέρχονται από σήματα σφάλματος εξόδου τροφοδοτώντας τα πίσω στις κρυφές μονάδες.

Μπορούν να χρησιμοποιηθούν διαφορετικές στρατηγικές εκχώρησης πιστώσεων για κρυφές μονάδες. Η συμμετρική, τυχαία και ομοιόμορφη ανάδραση είναι μερικά παραδείγματα σημάτων ανάδρασης.

Συμμετρική ανατροφοδότηση: υπολογίζεται ως το σταθμισμένο άθροισμα $e_{i}=\sum_{k} w_{k i} e_{k}$  των μεταγενέστερων σημάτων σφάλματος χρησιμοποιώντας τα πραγματικά βάρη προώθησης $w_{i k} .$  

Τυχαία ανατροφοδότηση: Υπολογίζεται ως η τυχαία προβολή $e_{i}=\sum_{k} b_{k i} e_{k}$ με τυχαίους συντελεστές $b_{k i}$ που προέρχονται από μια κανονική κατανομή με μηδενικό μέσο όρο και διακύμανση μονάδας ,

Ομοιόμορφη ανατροφοδότηση: όλοι οι συντελεστές στάθμισης απλώς ορίστηκαν σε ένα $e_{i}=\sum_{k} e_{k}$ που αντιστοιχεί πιο κοντά σε έναν κοινό τρίτο συντελεστή που κατανέμεται σε όλους τους νευρώνες, (διάχυτο νευροδιαμορφωτικό σήμα). Για την υλοποιήση των ενημερώσεων βάρους σε προσομοιώση διαβάστε την ενότητα 4.4.2 του \textlatin{Superspike}. 


\subsubsection{Πολυεπίπεδη Εκπαίδευση και Περιορισμοί}

Ο τύπος \ref{eqn:integratedsuperspike} απαιτεί επέκταση σε κρυμμένα επίπεδα. Μπορεί να χρησιμοποιηθεί ο ίδιος κανόνας εκμάθησης για κρυφές μονάδες, με την τροποποίηση ότι το $e_{i}(t)$ γίνεται μια περίπλοκη συνάρτηση που εξαρτάται από τα βάρη και τη μελλοντική δραστηριότητα όλων των νευρώνων. Αυτή η μη τοπικότητα στο χώρο και στο χρόνο παρουσιάζει σοβαρά προβλήματα, τόσο από άποψη βιολογικής αληθοφάνειας όσο και από τεχνικής υλοποιήσης. Τεχνικά, αυτός ο υπολογισμός απαιτεί είτε \textlatin{back-propagation} στο χρόνο μέσω του πυρήνα του μετασυναπτικού δυναμικού είτε υπολογισμό όλων των σχετικών ποσοτήτων στο \textlatin{online} όπως στην περίπτωση του \textlatin{RTRL}.
\subsubsection{Συμπέρασμα}
Οι συγγραφείς προτείνουν ότι αντί για ένα μόνο κοινό σήμα ανατροφοδότησης, απαιτείται ενα υψηλότερης διαστατικότητας νευροδιαμορφωτικό ή ηλεκτρικό σήμα ανατροφοδότησης για μάθηση με πιθανώς κάποια γνώση της πορείας τροφοδοσίας. Προτείνουμε ότι οι ταλαντώσεις μπορούν να εξυπηρετήσουν αυτόν τον σκοπό της ευφυούς νευροδιαμόρφωσης. Ο εγκέφαλος χρησιμοποιεί μια μέθοδο διαμερισματοποίησης υπολογισμών, η οποία μπορεί να του επιτρέψει να εκτελέσει πιο πολύπλοκες και δύσκολες εργασίες, αλλά απαιτεί επίσης την ανάγκη κεντρικού ελέγχου για την ενσωμάτωση δεδομένων από διάφορες περιοχές του εγκεφάλου. Η απόκλιση προβολής νευροδιαμορφωτή μεγάλης εμβέλειας (\textlatin{Long-range neuromodulator projection divergence}) φαίνεται να είναι κατάλληλη για τον συντονισμό της επικοινωνίας μεταξύ των περιοχών του εγκεφάλου \cite{ito2008}. Η νευρική μετάδοση χαρακτηρίζεται από ταλαντωτική δραστηριότητα του εγκεφάλου. Έτσι, η ικανότητα των νευροδιαμορφωτών να τροποποιούν τη μετάδοση σήματος με τρόπο που εξαρτάται από τη συχνότητα παρέχει έναν βαθύτερο βαθμό ελέγχου και θα μπορούσε να αποτελέσει αντικείμενο έρευνας για νευρωνικά δίκτυα ακίδων.

\subsection{\textlatin{Synaptic Plasticity Dynamics for Deep Continuous Local Learning}-\textlatin{Decolle}}
H μέθοδος \textlatin{Synaptic Plasticity Dynamics for Deep Continuous Local Learning} \cite{kaiser2020} επιτρέπει εκμάθηση σε πραγματικό χρόνο χρησιμοποιώντας τοπικές συναρτήσεις σφάλματος. Αυτή η ιδιότητα καθιστά αυτόν τον κανόνα εκμάθησης κατάλληλο για νευρομορφικό υλικό, καθώς δεν απαιτεί επιβάρυνση μνήμης. Οι κανόνες της συναπτικής πλαστικότητας λαμβάνονται συστηματικά από συναρτήσεις κόστους που καθορίζονται από τον χρήστη και νευρωνικές δυναμικές χρησιμοποιώντας μεθόδους αυτόματης διαφοροποίησης . Ο αλγόριθμος λύνει ένα βασικό πρόβλημα του Το \textlatin{BPTT} που δεν είναι βιολογικά ρεαλιστικό επειδή στο \textlatin{Decolle} οι νευρώνες κάνουν τους υπολογισμούς τους τοπικά. Επίσης, η συνεχής χρονική δυναμική των SNN δημιουργεί ένα πρόβλημα χρονικής ανάθεσης πιστώσεων \textlatin{temporal credit assignment problem}.
\subsubsection{Λειτουργία του Κανόνα Μάθησης}
Το Decolle μπορεί να υπολογίσει τις παραγώγους τοπικά χρησιμοποιώντας τοπικά \textlatin{readouts } ανά στρώμα \cite{neftci2017}. Η χρονική δυναμική αντιμετωπίζεται από την καθιερωμένη ισοδυναμία \textlatin{SNN} και \textlatin{RNN} όπως αναφέρεται στην ενότητα Surrogate Gradient Learning \cite{neftci2019}. Ο κανόνας της πλαστικότητας είναι τοπικό χρονικά επειδή το \textlatin{Decolle} είναι διαμορφωμένο με τέτοιο τρόπο ώστε οι πληροφορίες που απαιτούνται για τον υπολογισμό της παραγώγου να προωθούνται προς τα μπροστά. Αυτή η ιδέα δεν είναι εντελώς νέα, αλλά παρόμοιες μέθοδοι απαιτούν μεταβλητές ειδικής κατάστασης για κάθε σύναψη, κλιμακώνοντας έτσι τουλάχιστον τετραγωνικά με τον αριθμό των νευρώνων, όπως το \textlatin{Superspike} με τοπικούς κανόνες μάθησης. Ο \textlatin{Decolle} κλιμακώνεται γραμμικά με τον αριθμό των νευρώνων, ο οποίος απαιτεί τάξεις μεγέθους λιγότερη μνήμη. Χρησιμοποιώντας \textlatin{readouts}, μια συνάρτηση τοπικού κόστους χρονικά και χωρικά. Οι συγγραφείς επισημαίνουν την ομοιότητα με τους μηχανισμούς ανάγνωσης (\textlatin{readout mechanisms}) που χρησιμοποιούνται στα \textlatin{ Liquid State Machines} \cite{markram2002}. Οι νευρώνες ανάγνωσης σε \textlatin{ Liquid State Machines} μπορούν να μάθουν να εξάγουν πληροφορίες από ορισμένα μικροκυκλώματα και να αναφέρουν αυτές τις πληροφορίες σε άλλα κυκλώματα. Τα \textlatin{readouts} στο \textlatin{Decolle} πραγματοποιούνται σε έναν σταθερό τυχαίο συνδυασμό εξόδων νευρώνων. Οι συγγραφείς το θεωρούν επίσης μια προσέγγιση συνθετικής παραγώγου (\textlatin{synthetic gradient })με τυχαία εκκίνηση του τοπικού \textlatin{readout}.

\subsubsection{Πλεονεκτήματα της μεθόδου}
Το \textlatin{Decolle}, όπως και το \textlatin{SuperSpike}, χρησιμοποιεί υποκατάστατες παραγώγους για να ενημερώσει τα βάρη, αλλά σε αντίθεση με το \textlatin{SuperSpike}, η συνάρτηση κόστους είναι τοπική σε χρόνο και χώρο, απαιτώντας μόνο ένα ίχνος ανά νευρώνα εισόδου. Αυτό επιτρέπει στη μέθοδο να κλιμακωθεί γραμμικά στο χώρο. Επιπλέον, ο υπολογισμός των παράγωγων στο Decolle μπορεί να επαναχρησιμοποιήσει τις μεταβλητές που υπολογίζονται για τη δυναμική προώθησης προς τα εμπρός, με αποτέλεσμα να μην υπάρχει επιπλέον επιβάρυνση μνήμης κατά τη μάθηση. Η τοπική ανάγνωση του Decolle λειτουργεί ως ένα επίπεδο αποκωδικοποιητή και μαθαίνει τα εσωτερικά βάρη με τα βάρη ανάγνωσης να είναι τυχαία και σταθερά. Η εσωτερική εκπαίδευση με βάρη επιτρέπει στο δίκτυο να μαθαίνει παραστάσεις που διευκολύνουν τα επόμενα επίπεδα να ταξινομήσουν εισόδους \cite{neftci2017}. Αυτό είναι κάπως παρόμοιο με τα reservoir networks, αλλά τα βάρη ανάγνωσης εκεί είναι εκπαιδεύομενα, το Decolle το αποφεύγει αυτό για να μειώσει το κόστος υπολογισμού ενώ ταυτόχρονα καταφέρνει να επιτύχει μεγάλη ακρίβεια ταξινόμησης.Αυτό δεν σημαίνει οτι δεν μπορούμε να εκπαιδεύσουμε νευρωνικα δίκτυα ακίδων τύπου \textlatin{reservoir} δικτύων. Για παράδειγμα, ενα νευρωνικό δίκτυο \textlatin{Convolutional Spiking} που βασίζεται σε \textlatin{Reservoir} δίκτυα χρησιμοποιήθηκε για την εκπαίδευση ενός σύνολο δεδομένων δυναμικής όρασης με μεγάλη ακρίβεια \cite{george2020}.

Επισημαίνουμε ξανά ότι τα \textlatin{SNN} είναι επαναλαμβανόμενα (\textlatin{recurrent}) ακόμη και όταν όλες οι συνδέσεις είναι προωθημένες προς τα εμπρός, επειδή οι νευρώνες διατηρούν μια κατάσταση που διαδίδεται προς τα εμπρός σε κάθε χρονικό βήμα. Επειδή η πλήρης ακολουθία και οι καταστάσεις δραστηριότητας που προκύπτουν αποθηκεύονται για τον υπολογισμό παραγώγων, οι τεχνικές που βασίζονται σε \textlatin{BPTT} μπορούν να υπολογίσουν καλά τις παραγώγους, αλλά με κόστος στη μνήμη. Επιπλέον, στα \textlatin{SNN} πρέπει να χρησιμοποιήσουμε πολύ μεγάλο αριθμό χρονικών βημάτων για να καταγράψουμε τη χρονική δυναμική της εισόδου (συνιστάται χρόνος προσομοίωσης 1 \textlatin{ms} ή λιγότερο). Αυτό προφανώς σημαίνει ότι πρέπει να χρησιμοποιήσουμε ένα μεγάλο παράθυρο περικοπής ανάλογα με τη συχνότητα των σημαντικών συμβάντων στα δεδομένα εισόδου (1000 χρονικά βήματα εάν αυτά,τα συμβάντα, συμβαίνουν κάθε 1 δευτερόλεπτο). Το μέγεθος του \textlatin{SNN} που εκπαιδεύεται από το \textlatin{BPTT} περιορίζεται σημαντικά από τη διαθέσιμη μνήμη \textlatin{GPU} \cite{ochard2018}. Αυτό το γεγονός επιβεβαιώνεται και από εμάς, όπως θα εξηγηθεί σε επόμενες ενότητες.Το \textlatin{Decolle} απαιτεί μια τάξη \textlatin{T} λιγότερους πόρους μνήμης σε σύγκριση με το \textlatin{BPTT}, όπου \textlatin{T} είναι το μήκος της ακολουθίας.



\subsubsection{Μοντέλο νευρώνα και σύναψης}
Τα μοντέλα νευρώνων και συνάψεων που χρησιμοποιούνται στα πειράματα του \textlatin{Decolle} ακολουθούν \textlatin{leaky, current-based integrate-and-fire dynamics} με σχετικό \textlatin{refractory} μηχανισμό. Έχουμε ορίσει αυτό το μοντέλο νευρώνα προηγουμένως, αλλά το ορίζουμε ξανά για να μας επιτρέψει να εξηγήσουμε εύκολα τις εξισώσεις που χρησιμοποιούνται για μάθηση. Η συμπεριφορά του δυναμικού μεμβράνης $U_{i}$ ενός νευρώνα $i$ περιγράφεται από τις ακόλουθες διαφορικές εξισώσεις:

\selectlanguage{english}
\begin{equation}
\begin{aligned}
U_{i}(t) &=V_{i}(t)-\rho R_{i}(t)+b_{i} \\
\tau_{m e m} \frac{\mathrm{d}}{\mathrm{d} t} V_{i}(t) &=-V_{i}(t)+I_{i}(t) \\
\tau_{r e f} \frac{\mathrm{d}}{\mathrm{d} t} R_{i}(t) &=-R_{i}(t)+S_{i}(t)
\end{aligned}
\end{equation}

\selectlanguage{greek}
$S_{i}(t)$ έχει δυαδική τιμή $(0$ or 1$)$ που αντιπροσωπεύει εάν ο νευρώνας $i$ αυξήθηκε τη στιγμή $t$. Το δυναμικό της μεμβράνης χωρίζεται σε δύο μεταβλητές \textlatin{U} και \textlatin{V} είναι βιολογικά εμπνευσμένο και αντιπροσωπεύει ένα μοντέλο δύο διαμερισμάτων, ένα δενδριτικό (\textlatin{V}) και ένα σωματικό (\textlatin{U}) διαμέρισμα. Όταν το δυναμικό της μεμβράνης φτάσει σε ένα κατώφλι δημιουργείται μια ακίδα $S_{i}(t)=\Theta\left(U_{i}(t)\right)$, όπου $\Theta(x)=0$ εάν $x<0$, διαφορετικά το 1 είναι η βηματική συνάρτηση. Το σταθερό $b_{i}$  αντιπροσωπεύει την εγγενή διέγερση του νευρώνα. Ο \textlatin{refractory} μηχανισμός περιγράφεται με $R_{i}$. Βασικά ο νευρώνας αναστέλλεται μετά τη βολή, με σταθερό βάρος $\rho$ αλλά μπορεί ακόμα να εκπέμψει μια ακίδα δεδομένης μιας αρκετά ισχυρής εισόδου. Αυτή η συμπεριφορά διέπεται από την τελευταία διαφορική εξίσωση παραπάνω. Συνήθως στα μοντέλα \textlatin{Integrate-and-Fire} ο νευρώνας δεν μπορεί να εκπέμψει μια ακίδα αμέσως μετά ακόμη και με ισχυρή είσοδο. Οι παράγοντες $\tau_{r e f}$ και $\tau_{m e m}$ είναι χρονικές σταθερές της μεμβράνης και της δυναμικής του \textlatin{refractory} μηχανισμού. $I_{i}$ περιγράφει το συνολικό συναπτικό ρεύμα του νευρώνα $i$, εκφρασμένο ως:

\selectlanguage{english}
\begin{equation}
\tau_{s y n} \frac{\mathrm{d}}{\mathrm{d} t} I_{i}(t)=-I_{i}(t)+\sum_{j \in \mathrm{pre}} W_{i j} S_{j}(t)
\end{equation}
\selectlanguage{greek}
$W_{i j}$ είναι τα συναπτικά βάρη μεταξύ του προ-συναπτικού νευρώνα $j$ και του μετα-συναπτικού νευρώνα $i$. $V_{i}$ και $I_{i}$ είναι γραμμικά ως προς τα βάρη $W_{i j}$, επομένως ξαναγράφουμε το $ V_ {i} $ ως:
\selectlanguage{english}
\begin{equation}
\begin{aligned}
V_{i}(t) &=\sum_{j \in \text { pre }} W_{i j} P_{i j}(t) \\
\tau_{m e m} \frac{\mathrm{d}}{\mathrm{d} t} P_{i j}(t) &=-P_{i j}(t)+Q_{i j}(t) \\
\tau_{s y n} \frac{\mathrm{d}}{\mathrm{d} t} Q_{i j}(t) &=-Q_{i j}(t)+S_{j}(t)
\end{aligned}
\end{equation}
\selectlanguage{greek}
Η κατάσταση $P$  περιγράφει τα ίχνη της μεμβράνης και $Q$  περιγράφει τα ίχνη της συνάψεως. Για κάθε εισερχόμενη άνοδο, το ίχνος $Q$ υφίσταται άλμα ύψους 1 και διαφορετικά υποκύπτει σε κορεσμό εκθετικά με χρονική σταθερά $\tau_{\mathrm{syn}}$. Τα μετασυναπτικά δυναμικά του νευρώνα $i$ που προκαλούνται από τον νευρώνα εισόδου $j$ προκύπτουν από τη στάθμιση του ίχνους $Q_{i j}$ με το συναπτικό βάρος $W_{i j}$. Τα $P_{i j}$ και $Q_{i j}$ οδηγούνται μόνο από το $S_{j}$, και έτσι ο δείκτης $i$ αγνοείται. Ως αποτέλεσμα, οι μεταβλητές $P$ και $Q$ είναι τόσες όσες και οι προ-συναπτικοί νευρώνες, ανεξάρτητα από τον αριθμό των συνάψεων.
Ωστόσο, για την προσομοίωση σε υπολογιστή χρειαζόμαστε τις εξισώσεις σε διακριτό χρόνο, το χρονικό βήμα προσομοίωσης συμβολίζεται με $\Delta t$. Το υπεργράφημα $l$ αντιπροσωπεύει το επίπεδο στο οποίο ανήκει ο νευρώνας. Οι παραπάνω εξισώσεις ξαναγράφονται ως εξής:
\selectlanguage{english}
\begin{equation}
\begin{aligned}
U_{i}^{l}[t]=\sum_{j} W_{i j}^{l} p_{j}^{l}[t]-\rho R_{i}^{l}[t]+b_{i}^{l} \\
S_{i}^{l}[t]=\Theta\left(U_{i}^{l}[t]\right) \\
P_{j}^{l}[t+\Delta t]=\alpha P_{j}^{l}[t]+(1-\alpha) Q_{j}^{l}[t] \\
Q_{j}^{l}[t+\Delta t]=\beta Q_{j}^{l}[t]+(1-\beta) S_{j}^{l-1}[t] \\
R_{i}^{l}[t+\Delta t]=\gamma R_{i}^{l}[t]+(1-\gamma) S_{i}^{l}[t]
\end{aligned}
\end{equation}
\selectlanguage{greek}
Οι σταθερές $\alpha=\exp \left(-\frac{\Delta t}{\tau_{\latintext {mem }}}\right), \gamma=\exp \left(-\frac{\Delta t}{\tau_{\latintext {ref }}}\right)$, and $\beta=\exp \left(-\frac{\Delta t}{\tau_{s y n}}\right)$ αντικατοπτρίζουν τη δυναμική κορεσμού του δυναμικού της μεμβράνης $U$, της \textlatin{refractory} κατάστασης $R$ και της συναπτικής κατάστασης $Q$ κατά τη διάρκεια ενός χρονικού βήματος $\Delta t$.
\subsubsection{Deep Learning}
Θεωρείται μια συνάρτηση συνολικού κόστους: $\mathcal{L}\left(S^{N}\right)$. Ορίζεται στις αιχμές $S^{N} $ του ανώτατου επιπέδου και στόχους $\hat{Y}$, οι παράγωγοι σε σχέση με τα βάρη στο στρώμα $l$ είναι:
\selectlanguage{english}
\begin{equation}
\frac{\partial \mathcal{L}\left(S^{N}\right)}{\partial W_{i j}^{l}}=\frac{\partial \mathcal{L}\left(S^{N}\right)}{\partial S_{i}^{l}} \frac{\partial S_{i}^{l}}{\partial U_{i}^{l}} \frac{\partial U_{i}^{l}}{\partial W_{i j}^{l}}
\end{equation}
\selectlanguage{greek}
Ο συντελεστής $\frac{\partial \mathcal{L}\left(S^{N}\right)}{\partial S_{i}^{l}}$ περιγράφει τα σφάλματα που προκύπτουν απο \textlatin{backpropagation}. Τα σφάλματα αυτά αντιπροσωπεύουν τον τρόπο με τον οποίο η έξοδος του νευρώνα $i$ στο επίπεδο $l$ τροποποιεί την απώλεια σε όλο το δίκτυο. Αυτό το πρόβλημα είναι γνωστό ως πρόβλημα εκχώρησης πίστωσης. Ωστόσο, συνήθως περιλαμβάνει μη τοπικούς όρους, τους νευρώνες δραστηριότητας, τα λάθη τους και πώς συμπεριφέρθηκαν στο παρελθόν. Λαμβάνοντας υπόψη τη μη-τοπικότητα, ένας κρυμμένος νευρώνας δεν μπορεί να προβλέψει πώς η τροποποίηση της συμπεριφοράς του θα επηρεάσει το κόστος του ανώτερου επιπέδου. Οι προσεγγίσεις στα \textlatin{backpropagation} σφάλματα στα \textlatin{SNN} μπορούν να επιτρέψουν την τοπική εκμάθηση, για παράδειγμα στην \cite{lillicrap2016}. Ωστόσο, η αποτελεσματική διατήρηση της ιστορίας της δυναμικής του δικτύου παραμένει ένα ανοιχτό πρόβλημα. Αυτό που παραμένει ως ανοιχτό πρόβλημα είναι πώς να αποθηκεύσουμε αποτελεσματικά τις προηγούμενες δυναμικές και αυτό είναι ένα από τα κύρια επιτεύγματα της \textlatin{Decolle}, μειωμένη απαίτηση μνήμης GPU συγκριτικά με τη χρήση μεθόδων \textlatin{BPTT} .

Το\textlatin{Decolle} επικεντρώνεται σε έναν τύπο βαθιάς τοπικής μάθησης κατά τον οποίο τυχαίες αναγνώσεις \textlatin{random readouts} προσαρτώνται σε βαθιά στρώματα και καθορίζονται βοηθητικές συναρτήσεις κόστους μέσω των \textlatin{readouts} \ref{fig:readout}.
\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Learning Methods/decolle/READOUT.jpg}
    \caption{Μηχανισμός \textlatin{Readout}  . 
    \label{fig:readout}}
\end{figure}

Για νευρώνες στα βαθιά στρώματα, αυτές οι συναρτήσεις βοηθητικού κόστους παρέχουν μια πηγή σφάλματος. Πολλαπλασιάζοντας τις νευρικές ενεργοποιήσεις με μια τυχαία και σταθερή μήτρα, λαμβάνεται η τυχαία ανάγνωση. Εκπαίδευση με βοηθητικά τοπικά σφάλματα που μειώνουν το κόστος τοπικά επιτρέπουν στο δίκτυο στο σύνολό του να επιτύχει χαμηλό κόστος συνολικά. Όπως συζητήθηκε στο \cite{mostafa2017}, η μείωση της απώλειας ταξινόμησης μιας τοπικής ανάγνωσης ασκεί πίεση στα βαθιά στρώματα να αποκτήσουν σημαντικά χαρακτηριστικά, επιτρέποντας στους τυχαίους τοπικούς ταξινομητές να είναι αρκετά ακριβείς ώστε να μειωθεί η απώλεια σε όλο το δίκτυο. Επιπλέον, κάθε στρώμα βασίζεται στα χαρακτηριστικά του προηγούμενου στρώματος για να μάθει χαρακτηριστικά για τον τοπικό τυχαίο ταξινομητή του που είναι πιο "ξεμπλεγμένα" σε σχέση με τις κατηγορίες από το προηγούμενο επίπεδο. Σε αυτό το άρθρο, επικεντρωνόμαστε στο γεγονός ότι, υπό την προϋπόθεση των τοπικών λειτουργιών απώλειας, η υποκατάστατη μάθηση σε νευρωνικά δίκτυα ακίδων βαθιάς μάθησης γίνεται ιδιαίτερα αποτελεσματική. Αυτό έχει ως αποτέλεσμα μια αποδοτική προσέγγιση υποκατάστατης παραγώγου για αυτά τα δίκτυα.
\subsection{Κανόνας Μάθησης}
Η τυχαία ανάγνωση που επισυνάπτεται σε κάθε ένα από τα στρώματα των $N$ επιπέδων νευρώνων ακίδας δίνεται από:
\selectlanguage{english}
\begin{equation}
Y_{i}^{l}=\sum_{j} G_{i j}^{l} S_{j}^{l}
\end{equation}
\selectlanguage{greek}
όπου τα $G_{i j}^{l}$ είναι σταθεροί, τυχαίοι πίνακες (ένας για κάθε επίπεδο $l$) και $\Theta$  είναι μια συνάρτηση ενεργοποίησης. Η καθολική συνάρτηση απώλειας ορίζεται ως το άθροισμα των συναρτήσεων απώλειας κατά επίπεδο που ορίζονται στα τυχαία \textlatin{readouts}, δηλαδή $\mathcal{L}=\sum_{l=1}^{N} L^{l}\left(Y^{l}\right)$. Για την επιβολή της τοπικότητας, το \textlatin{Decolle} θέτει στο μηδέν όλες τις μη τοπικές παραγώγους, δηλαδή $\frac{\partial L^{l}}{\partial W_{i j}^{m}}=0$ εάν $m \neq l$ . Οι ενημερώσεις βάρους σε κάθε επίπεδο είναι:
\selectlanguage{english}
\begin{equation}
\Delta W_{i j}^{l}=-\eta \frac{\partial L^{l}}{\partial W_{i j}^{l}}=-\eta \frac{\partial L^{l}}{\partial S_{i}^{l}} \frac{\partial S_{i}^{l}}{\partial W_{i j}^{l}}
\end{equation}
\selectlanguage{greek}
όπου $\eta$ είναι ο ρυθμός εκπαίδευσης. Αν υποθέσουμε ότι η συνάρτηση απώλειας εξαρτάται μόνο από μεταβλητές στο ίδιο χρονικό βήμα, ο όρος $\frac{\partial L^{l}}{\partial S_{i}^{\prime}}$, μπορεί να υπολογιστεί χρησιμοποιώντας τον κανόνα της αλυσίδας . Η εφαρμογή του κανόνα της αλυσίδας στις παραγώγους δεύτερης τάξης μας δίνει:
\selectlanguage{english}
\begin{equation}
\frac{\partial S_{i}^{l}}{\partial W_{i j}^{l}}=\frac{\partial \Theta\left(U_{i}^{l}\right)}{\partial U_{i}^{l}} \frac{\partial U_{i}^{l}}{\partial W_{i j}^{l}}
\end{equation}
\selectlanguage{greek}
Due to the sparse, binary activation of spiking neurons, this expression vanishes everywhere except at 0 , where it is infinite . To solve this problem Decolle uses surrogate gradient-based learning:
Λόγω της αραιής, δυαδικής ενεργοποίησης των νευρώνων με αιχμές, αυτή η έκφραση εξαφανίζεται παντού εκτός από το 0, όπου είναι άπειρο. Για την επίλυση αυτού του προβλήματος,χρησιμοποίουμε την προσέγγιση των \textlatin{SG}:
\selectlanguage{english}
\begin{equation}
\frac{\partial S_{i}^{l}}{\partial W_{i j}^{l}}=\sigma^{\prime}\left(U_{i}^{l}\right) \frac{\partial U_{i}^{l}}{\partial W_{i j}^{l}}
\end{equation}
\selectlanguage{greek}
όπου $\sigma^{\prime}\left(U_{i}^{l}\right)$ είναι το \textlatin{SG} της μη διαφοροποιήσιμης συνάρτησης  $\Theta\left(U_{i}^{l}\right)$.
\selectlanguage{english}
\begin{equation}
\frac{\partial U_{i}^{l}}{\partial W_{i j}^{l}}=P_{j}^{l}-\rho \frac{\partial R_{i}^{l}}{\partial W_{i j}^{l}}
\end{equation}
\selectlanguage{greek}
Οι όροι που περιλαμβάνουν $R_{i}^{l}$ είναι δύσκολο να υπολογιστούν επειδή εξαρτώνται από το ιστορικό του νευρώνα. Όπως και στο \textlatin{Superspike}, οι όροι που περιλαμβάνουν $R_{i}^{l}$ αγνοούνται. Η κανονικοποίηση εφαρμόζεται για να ευνοήσει τα χαμηλά ποσοστά πυροδότησης, γεγονός που προκαλεί τον όρο $R_{i}^{l}$ να έχει αμελητέα επιρροή. Ο κανόνας που περιγράφει τον τρόπο ενημέρωσης των συναπτικών βαρών είναι:
\selectlanguage{english}
\begin{equation}
\Delta W_{i j}^{l}=-\eta \frac{\partial L^{l}}{\partial S_{i}^{l}} \sigma^{\prime}\left(U_{i}^{l}\right) P_{j}^{l}
\end{equation}
\selectlanguage{greek}
Η απώλεια του επιπέδου $l$ αν χρησιμοποιήσουμε την συνάρτηση σφάλματος \textlatin{mean square error}, δίνεται ως εξής:
\selectlanguage{english}
\begin{equation}
L^{l}=\frac{1}{2} \sum_{i}\left(Y_{i}^{l}-\hat{Y}_{i}^{l}\right)^{2}
\end{equation}
\selectlanguage{greek}
οπότε έχουμε:
\selectlanguage{english}
\begin{equation}
\begin{aligned}
\Delta W_{i j}^{l} &=-\eta \operatorname{error}_{i}^{l} \sigma^{\prime}\left(U_{i}^{l}\right) P_{j}^{l} \\
\operatorname{error}_{i}^{l} &=\sum_{k} G_{k i}^{l}\left(Y_{k}^{l}-\hat{Y}_{k}^{l}\right)
\end{aligned}
\end{equation}
\selectlanguage{greek}
όπου $\hat{Y}^{l}$  είναι το διάνυσμα ψευδο-στόχου για το επίπεδο $l$.
\selectlanguage{english}
\begin{equation}
\mathcal{L}_{g}=\sum_{l} L^{l}+\lambda_{1}\left\langle\left[U_{i}^{l}+0.01\right]^{+}\right\rangle_{i}+\lambda_{2}\left[0.1-\left\langle U_{i}^{l}\right\rangle_{i}\right]^{+}
\end{equation}
\selectlanguage{greek}
\subsubsection{Υπολογιστική Πολυπλοκότητα}
Οι μεταβλητές \textlatin{P} και \textlatin{U} που απαιτούνται για τη μάθηση είναι τοπικές και είναι διαθέσιμες από τη  προωθούμενη δυναμική. Επειδή τα σφάλματα υπολογίζονται τοπικά σε κάθε επίπεδο, δεν χρειάζεται να αποθηκεύσουμε επιπλέον ενδιάμεσες μεταβλητές. Το υπολογιστικό κόστος της ενημέρωσης βάρους είναι μία προσθήκη και δύο πολλαπλασιασμοί ανά σύνδεση. Αυτό καθιστά το \textlatin{DECOLLE} σημαντικά αποδοτικότερο να εφαρμοστεί σε σύγκριση με το \textlatin{BPTT} για την εκπαίδευση \textlatin{SNN } που κλιμακώνεται χωρικά ως O (NT), T είναι ο αριθμός των χρονικών βημάτων. Το \textlatin{Decolle} χρησιμοποιεί εργαλεία αυτόματης διαφοροποίησης. το μόνο που απαιτείται είναι \textlatin{backpropagation} σε ένα υπογράφημα που αντιστοιχεί σε ένα επίπεδο στο ίδιο χρονικό βήμα. Επειδή οι πληροφορίες που απαιτούνται για τον υπολογισμό των παραγώγων μεταφέρονται στο χρόνο και οι τοπικές συναρτήσεις απώλειας παρέχουν τις παραγώγους που απαιτούνται για κάθε επίπεδο. Οι ακριβείς λεπτομέρειες εφαρμογής της αυτόματης διαφοροποίησης δίνονται στις υποσημειώσεις του \textlatin{Decolle}.

\subsubsection{Ο προβληματισμός μας}
Σε σύγκριση με το \textlatin{BPTT}, το \textlatin{Decolle } είναι μια πολύ χρήσιμη προσέγγιση επειδή έχει τάξεις μεγέθους μικρότερη πολυπλοκότητα χώρου. Το \textlatin{Decolle} είναι απλώς μια προσωρινή λύση για την εκπαίδευση συνόλων δεδομένων σε νευρομορφικό υλικό μέχρι να καταλάβουμε τα πιο έξυπνα και περίπλοκα διαμορφωτικά σήματα που χρησιμοποιεί ο εγκέφαλός μας. Υπάρχει επίσης το ζήτημα της απώλειας σημαντικών πληροφοριών/χαρακτηριστικών σε μεγαλύτερες ακολουθίες δεδομένων εισόδου, όπου ο \textlatin{e-prop } φαίνεται να είναι σε θέση να χειριστεί καλύτερα τέτοιου είδους δεδομένα όπως θα δούμε.Επιπλέον, σε σύγκριση με τον εγκέφαλο των θηλαστικών, το παρόν νευρομορφικό υλικό είναι αρκετά πρωτόγονο. Δεδομένης της λιγοστής κατανόησής μας για το πώς τα διαμορφωτικά σήματα εκμεταλλεύονται την περίπλοκη τρισδιάστατη δομή του εγκεφάλου και τον τεράστιο αριθμό νευρώνων και συνάψεων, οι υπάρχουσες προσεγγίσεις γιa εκμάθηση νευρωνικών δικτύων είναι επίσης πολύ πρωτόγονες. Οι μέθοδοι μάθησης θα πρέπει να ακολουθούν την κατανόησή μας για τον εγκέφαλο των θηλαστικών, ο οποίος επηρεάζεται από τη διαθέσιμη τεχνολογία απεικόνισης του εγκεφάλου, εκτός εάν κάποιος μπορεί να νικήσει δισεκατομμύρια χρόνια εξέλιξης.
\selectlanguage{english}
\subsection{e-Prop - A solution to the learning dilemma for recurrent networks of spiking neurons}
\selectlanguage{greek}
\chapter{Αισθητήρες Δυναμικής Όρασης}

\begin{wrapfigure}{r}{0.25\textwidth} %this figure will be at the right
    \centering
     \includegraphics[width=0.25\textwidth]{DVS/sensor.PNG}
    \caption{Αρχιτεκτονική ενός αισθητήρα \textlatin{DVS} της εταιρίας \textlatin{iniLabs} }
    \label{fig:dvs-sensor}
\end{wrapfigure}.

Οι αισθητήρες δυναμικής όρασης (\textlatin{Dynamic Vision Sensors}-\textlatin{DVS}) ή οι κάμερες συμβάντων είναι αισθητήρες βιολογικής έμπνευσης που λειτουργούν με κάπως διαφορετικό τρόπο από τις συμβατικές κάμερες. Υπολογίζουν τις αλλαγές φωτεινότητας ανά \textlatin{pixel} ασύγχρονα αντί να συλλέγουν εικόνες σε καθορισμένο χρόνο. Οι δυναμικοί αισθητήρες όρασης παρέχουν μεγάλα πλεονεκτήματα σε σύγκριση με τις τυπικές κάμερες. Αυτά περιλαμβάνουν υψηλότερη χρονική ανάλυση, πολύ υψηλό δυναμικό εύρος (140 \textlatin{dB} έναντι 60 \textlatin{dB}). Επίσης, καταναλώνουν πολύ λιγότερη ενέργεια και παρέχουν υψηλό εύρος ζώνης εικονοστοιχείων, το οποίο έχει ως αποτέλεσμα τη λιγότερη θολούρα κίνησης. Αυτά τα πλεονεκτήματα σε σχέση με τις τυπικές κάμερες μπορούν να βοηθήσουν τη ρομποτική που απαιτεί κάμερες χαμηλής καθυστέρησης, υψηλής ταχύτητας και αποδοτικότητας για να λειτουργήσει καλά και αποτελεσματικά ενα ρομπότ. Ο \textlatin{DVS} έχει επίσης τη δυνατότητα να μειώσει το κόστος εκπαίδευσης των βαθύ νευρωνικών δικτύων \textlatin{Computer Vision} προσφέροντας ισάξιας ή καλύτερης ακρίβειας. Ωστόσο, επειδή οι κάμερες συμβάντων διαφέρουν από τις συνηθισμένες κάμερες στο ότι μετρούν τις παραλλαγές φωτεινότητας ανά \textlatin{pixel} (που ονομάζονται "συμβάντα") ασύγχρονα και δεν εγγράφουν "απόλυτης" φωτεινότητας με σταθερό ρυθμό, χρειάζονται νέες μέθοδοι για την ανάλυση της εξόδου τους.

\subsubsection{Εφαρμογές}
Μερικές εφαρμογές αυτών των αισθητήρων αποτελούν:
\begin{itemize}
    \item Ιχνηλάτιση αντικειμένων
    \item Επιτήρηση και παρακολούθηση
    \item Αναγνώριση αντικειμένου/χειρονομίας
    \item Εκτίμηση βάθους 
    \item Υπολογισμός \textlatin{Optical Flow}
    \item \textlatin{SLAM}
    \item \textlatin{Deblurring} εικόνας
\end{itemize}
\subsubsection{Αρχές λειτουργίας}
Πώς ακριβώς υπολογίζουν οι κάμερες συμβάντων τις αλλαγές φωτεινότητας ανά \textlatin{ pixel}; Κάθε συμβάν εξόδου ή ακίδα αντιπροσωπεύει μια αλλαγή φωτεινότητας (Λογαριθμική ένταση) που εξαρτάται από ένα όριο, σε μια συγκεκριμένη στιγμή. Κάθε εικονοστοιχείο πρέπει να θυμάται τη δική του ένταση καταγραφής κάθε φορά που στέλνει μια ακίδα και είναι έτοιμο να στείλει ξανά εάν γίνει αντιληπτή μια ορισμένη αλλαγή στη φωτεινότητα. Αυτό είναι διαφορετικό από τις παραδοσιακές κάμερες όπου οι πληροφορίες σχετικά με το περιβάλλον καταγράφονται σε καρέ ανά δευτερόλεπτο (σε κύκλους) αντί συνεχής παρακολούθησης του περιβάλλοντος. Ένα συμβάν εξόδου περιέχει την ($x$,$y$) τοποθεσία, την ώρα $t$ και την πολικότητα 1-\textlatin{bit} $p$ (αύξηση φωτεινότητας "ON" ή μείωση "\textlatin{OFF}, δείτε επίσης \ref{fig:dvs-overview}b,\ref{fig:dvs-overview}e, \ref{fig:dvs-overview} \cite{davis}).

\begin{wrapfigure}{r}{0.5\textwidth} %this figure will be at the right
    \centering
     \includegraphics[width=0.5\textwidth]{DVS/dvs-overview.PNG}
    \caption{\textlatin{DAVIS}}
    \label{fig:dvs-overview}
\end{wrapfigure}

The events are exported from the camera using a shared digital output bus, usually by using address-event readout(AER) \cite{boahen2004} \cite{liu2015} . The amount of data-points generated by those sensors depends on the amount of information that changes in time, in the environment. This is achieved by adapting the sampling rate to the rate of change of the log intensity signal. The time resolution is in the order of microseconds while latency is a little slower in the order of sub-milliseconds. This allows sensors to react quickly to visual stimuli and makes the sensors appropriate for autonomous driving. Additionally, the DVS brightness change events have a built-in invariance to scene illumination.

Τα συμβάντα εξάγονται από την κάμερα χρησιμοποιώντας ένα κοινόχρηστο ψηφιακό δίαυλο εξόδου, συνήθως χρησιμοποιώντας ανάγνωση διεύθυνσης-συμβάντος (\textlatin{AER}) \cite{boahen2004} \cite{liu2015}. Ο όγκος των σημείων δεδομένων που παράγονται από αυτούς τους αισθητήρες εξαρτάται από τον όγκο των πληροφοριών που αλλάζουν στο χρόνο, στο περιβάλλον. Αυτό επιτυγχάνεται με την προσαρμογή του ρυθμού δειγματοληψίας στον ρυθμό μεταβολής της λογαριθμικής έντασης του σήματος . Η χρονική ανάλυση είναι της τάξης των μικροδευτερολέπτων, ενώ η καθυστέρηση είναι λίγο πιο αργή στη σειρά των \textlatin{ms}. Αυτό επιτρέπει στους αισθητήρες να αντιδρούν γρήγορα σε οπτικά ερεθίσματα και καθιστά τους αισθητήρες κατάλληλους για αυτόνομη οδήγηση. Επιπλέον, τα συμβάντα αλλαγής φωτεινότητας των \textlatin{DVS} παραμένουν αναλλοιώτα απο τον φωτισμό της σκηνής.
\subsubsection{Διαθέσιμες Συσκευές}
Εκτός από την αρχική κάμερα DVS \cite{Lichtsteiner2008}, υπήρξαν πρόσφατες εξελίξεις \cite{posch2014} , \cite{liu2015}, \cite{indiveri2015}, \cite{delbruck2010} .
Ένας από τους πιο ευρέως χρησιμοποιούμενους αισθητήρες είναι ο \textlatin{Dynamic and Active Pixel Vision Sensor} \cite{davis} \ref{fig:dvs-sensor}.Ο \textlatin{DAVIS } συνδυάζει έναν συμβατικό ενεργό αισθητήρα pixel (\textlatin{active pixel sensor}-\textlatin{APS})  \cite{fossum1997}  στο ίδιο pixel με DVS.
\subsubsection{Προκλήσεις}
\begin{itemize}
    \item Η έξοδος των καμερών συμβάντων διαφέρει σημαντικά από τις συμβατικές κάμερες: τα γεγονότα είναι ασύγχρονα και χωρικά αραιά, ενώ οι εικόνες είναι σύγχρονες και πυκνές. Ως αποτέλεσμα, δεν εφαρμόζονται αλγόριθμοι όρασης βασισμένοι σε ανάλυση πλαισίων.
    \item Κάθε συμβάν περιέχει πληροφορίες σχετικά με τον τρόπο αλλαγής της φωτεινότητας. Ωστόσο, οι αλλαγές φωτεινότητας δεν σχετίζονται μόνο με την τρέχουσα φωτεινότητα της σκηνής, αλλά με την τρέχουσα και την προηγούμενη σχετική κίνηση μεταξύ της σκηνής και της κάμερας.
    \item Θόρυβος: Λόγω του εγγενή θορύβου λήψης στα φωτόνια και του θορύβου κυκλωμάτων τρανζίστορ, όλοι οι αισθητήρες όρασης είναι θορυβώδεις.
\end{itemize}
\subsubsection{Παραγωγή Συμβάντων(\textlatin{Event Generation})}

Ένας αισθητήρας συμβάντων \cite{Lichtsteiner2008} έχει ανεξάρτητα εικονοστοιχεία που ανταποκρίνονται
σε αλλαγές της λογαριθμικής έντασης της φωτεινότητας $L=\textlatin{log}(\textlatin{I})$ . Ένα συμβάν $e\textsubscript{k}=(\textlatin{x}\textsubscript{\textlatin{k}},t\textsubscript{\textlatin{k}},p\textsubscript{k}$) ενεργοποιείται στο \textlatin{pixel} $\textlatin{x}\textsubscript{\textlatin{k}}=(\textlatin{x}\textsubscript{\textlatin{k}},y\textsubscript{\textlatin{k}})^T$ στη χρονική στιγμή $t_k$ όταν η φωτεινότητα αυξάνει ένα ορισμένο χρονικό όριο αντίθεσης $C$ από το τελευταίο συμβάν.

\begin{equation}
    \Delta L(\textlatin{x}\textsubscript{\textlatin{k}},\textlatin{t}\textsubscript{\textlatin{k}}) = L(\textlatin{x}\textsubscript{\textlatin{k}},t\textsubscript{\textlatin{k}})-(\textlatin{x}\textsubscript{\textlatin{k}},t\textsubscript{\textlatin{k}}-\Delta t\textsubscript{\textlatin{k}})
\end{equation}
\selectlanguage{english}
\begin{equation}
   \Delta L(x\textsubscript{k},t\textsubscript{k}) = p\textsubscript{k}C 
\end{equation}
\selectlanguage{greek}
Όπου \(C > 0, \Delta t\textsubscript{\textlatin{k}}\) είναι το χρονικό διάστημα που μεσολαβεί από το τελευταίο συμβάν στο ίδιο \textlatin{pixel} και η πολικότητα \(p\textsubscript{\textlatin{k}} \in {+1,-1}\) είναι το πρόσημο της αλλαγής φωτεινότητας\cite{Lichtsteiner2008}. Η Ευαισθησία αντίθεσης $C$ καθορίζεται από τα ρεύματα πόλωσης του \textlatin{pixel} \cite{nozaki2017} \cite{Gallego2020}, τα οποία επίσης ορίζουν την ταχύτητα και το κατώφλι του ανιχνευτή αλλαγής στο \ref{fig:dvs-overview}. Τα τυπικά κατώφλια \textlatin{DVS} ορίζονται στο 10-50 τοις εκατό της αλλαγής φωτισμού. Υπάρχουν πιο προηγμένες μέθοδοι δημιουργίας γεγονότων, αλλά δεν αποτελούν το αντικείμενο αυτής της διατριβής. Ορισμένες από αυτές τις μεθόδους περιλαμβάνουν τον καθορισμό ενός ορίου στο μέγεθος της αλλαγής φωτεινότητας από το τελευταίο συμβάν και χρησιμοποιούνται ως βάση αλγορίθμων που βασίζονται σε φυσικά γεγονότα \cite{Gallego2019}. Τα συμβάντα μπορούν επίσης να ρυθμιστούν ώστε να δημιουργούνται απο κινούμενες ακμές. Τα πιο πολύπλοκα μοντέλα περιλαμβάνουν τη λήψη θορύβου αισθητήρα και την αναντιστοιχία τρανζίστορ καθιστώντας αυτά τα μοντέλα στοχαστικά.

\subsection{Event Processing}
Event processing aims to extract useful information from the data, but is also depended on the type of task we are interested in. Approaches for event processing that work on an event-by-event basis, with the state of the system (estimated unknowns) changing with each arrival of a single event, resulting in the shortest possible latency do not provide enough information for estimation and so methods that operate on events in groups or packets are more appropriate.These however, result in some increased latency but still provide a state update on event arrival. Given that events are processed in an optimized framework, another distinction is the type of objective or loss function used: geometric vs. temporal vs. photometric (e.g., a function of event polarity or activity).

Η επεξεργασία συμβάντων στοχεύει στην εξαγωγή χρήσιμων πληροφοριών από τα δεδομένα, αλλά εξαρτάται επίσης από τον τύπο της εφαρμογής που μας ενδιαφέρει. Προσεγγίσεις για την επεξεργασία συμβάντων που λειτουργούν κατά συμβάν σε συμβάν, με την κατάσταση του συστήματος (εκτιμώμενοι άγνωστοι) να αλλάζει με κάθε άφιξη ενός μεμονωμένου συμβάντος,έχουν τη μικρότερη δυνατή καθυστέρηση,όμως δεν παρέχουν αρκετές πληροφορίες για εκτίμηση κίνησης και έτσι οι μέθοδοι που λειτουργούν σε γεγονότα σε ομάδες ή πακέτα είναι πιο κατάλληλες. Ωστόσο, αυτά οδηγούν σε αυξημένη καθυστέρηση, αλλά εξακολουθούν να παρέχουν μια  ενημέρωση κατάστασης σε κάθε άφιξη ενός συμβάντος. Δεδομένου ότι τα γεγονότα υποβάλλονται σε επεξεργασία σε ένα βελτιστοποιημένο πλαίσιο, μια άλλη διάκριση είναι ο τύπος της συνάρτησης αντικειμένου ή απώλειας που χρησιμοποιείται: γεωμετρική έναντι χρονικής έναντι φωτομετρικής (π.χ., συνάρτηση της πολικότητας ή της δραστηριότητας γεγονότων).

\subsubsection{Μέθοδοι αναπαράστασης συμβάντων}
Αυτές οι μέθοδοι μπορούν να θεωρηθούν ως το στάδιο προεπεξεργασίας πριν από την εισαγωγή των δεδομένων στο επιθυμητό νευρωνικό δίκτυο. Η επιλογή ωστόσο επηρεάζεται επίσης από τον τύπο νευρωνικού δικτύου που χρησιμοποιείται.
\begin{figure}[htp] %this figure will be at the right
    \centering
     \includegraphics[width=16cm]{DVS/representation-methods.PNG}
    \caption{Αναπαράσταση συμβάντων}
    \label{fig:representation-methods}
\end{figure}


(\textlatin{a}) Γεγονότα στο χωροχρόνο (η θετική πολικότητα είναι σε μπλε χρώμα ενώ η αρνητική πολικότητα στο κόκκινο).

(\textlatin{b}) Τα συμβάντα συσσωρεύονται σε μια δισδιάστατη εικόνα. Αυτό επιτρέπει την εφαρμογή συμβατικών αλγορίθμων όρασης υπολογιστή. Ωστόσο, αυτή η μέθοδος έρχεται σε αντίθεση με τον στόχο μας: να καταγράφουμε γεγονότα στη χρονική διάσταση,οπότε χάνουμε πολύτιμες πληροφορίες .

(\textlatin{c}) Η επιφάνεια χρόνου είναι ένας δισδιάστατος χάρτης όπου κάθε εικονοστοιχείο αποθηκεύει μια μοναδική τιμή χρόνου. Αυτό ονομάζεται \textlatin{Motion History Images} στην όραση υπολογιστή \cite{ahad2012}. Ένα παράδειγμα αποτελεί το \cite{lagorge2017} .Τα γεγονότα μπορούν να μετατραπούν σε εικόνα που αντιπροσωπεύει το πρόσφατο ιστορικό κίνησης που εκθέτει τις πλούσιες χρονικές πληροφορίες των καταγεγραμμένων δεδομένων. Όμως,Η αποτελεσματικότητά τους υποβαθμίζεται σε σκηνές με υφή.

(\textlatin{d}) Παρόμοια έννοια του (c) αλλά τώρα η 3η διάσταση επιτρέπει την αποθήκευση ολόκληρης της ακολουθίας των γεγονότων (μόνο τα αρνητικά γεγονότα φαίνονται στο σχήμα).

(\textlatin{e}) Εικόνα συμβάντος με αντιστάθμιση κίνησης \cite{Gallego2019}. Αναπαράσταση που βασίζεται τόσο σε γεγονότα όσο και σε υποθέσεις κίνησης. Η θεωρία πίσω από την αντιστάθμιση κίνησης είναι ότι καθώς μια ακμή κινείται πάνω από το επίπεδο της εικόνας, δημιουργεί γεγονότα στα εικονοστοιχεία από τα οποία διέρχεται. η κίνηση της ακμής μπορεί να υπολογιστεί με στρέβλωση των γεγονότων σε χρόνο αναφοράς και βελτιστοποίηση της ευθυγράμμισης τους, με αποτέλεσμα μια ευκρινή εικόνα (δηλ., ιστόγραμμα) στρεβλωμένων γεγονότων. Αυτό είναι κυρίως χρήσιμο εάν κάποιος θέλει να ιχνιλατεί αντικείμενα σε ένα βίντεο.

(\textlatin{f}) Ανακατασκευή ένταση εικόνας\cite{rebecq2019}.


\chapter{Υπολογιστικά Πειράματα}
\selectlanguage{english}
\bibliographystyle{IEEEtran}
\bibliography{library}
\end{document}
