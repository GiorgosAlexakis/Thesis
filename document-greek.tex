\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[english,greek]{babel}
\usepackage[LGR,T1]{fontenc}
\usepackage[font=medium]{caption}
\usepackage{amsmath}
\usepackage{extsizes}
\usepackage[export]{adjustbox}
\usepackage{subcaption}
\selectlanguage{greek}

\usepackage{cite}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{wrapfig}
\graphicspath{{Images/}}
\usepackage{hyphenat}
\usepackage{makeidx}

\title{Νευρωνικά Δίκτυα Αιχμών, Ταξινόμηση Δεδομένων Αισθητήρων Δυναμικής Όρασης}
\author{Αλεξάκης Γεώργιος, Κορακοβούνης Δημήτριος}
\begin{document}



\maketitle
\frontmatter
\selectlanguage{greek}
\tableofcontents{}
\chapter{Εισαγωγή}

\section{Περιγραφή Διπλωματικής}
Η μηχανική μάθηση, ένα υποσύνολο της τεχνητής νοημοσύνης, έχει αυξηθεί σε δημοφιλία τα τελευταία χρόνια, εξαιτίας κυρίως των εξελίξεων στο υλικό υπολογιστών που σχετίζονται με γραφικά (κάρτες γραφικών) και των τεράστιων όγκων δεδομένων που δημιουργούνται από την ψηφιακή εποχή. Οι έννοιες και οι αλγόριθμοι που χρησιμοποιούνται στον τομέα σήμερα κυκλοφορούν εδώ και δεκαετίες, αλλά δεν είχαμε τη δυνατότητα να τις χρησιμοποιήσουμε μέχρι τώρα. Η πλειοψηφία των αλγορίθμων μηχανικής μάθησης χρησιμοποιεί μια απλή δομή τεχνητού νευρωνικού δικτύου που αποτελείται από πολλαπλά στρώματα διασυνδεδεμένων νευρώνων. Στη βαθιά μάθηση, ένας όρος που χρησιμοποιείται όταν ο αριθμός των επιπέδων είναι μεγάλος, κάθε επίπεδο μαθαίνει να μετατρέπει τα δεδομένα εισόδου του σε μια ελαφρώς πιο αφηρημένη και σύνθετη αναπαράσταση. Στην επεξεργασία εικόνας, για παράδειγμα, τα χαμηλότερα στρώματα μπορεί να εντοπίζουν ακμές, ενώ τα υψηλότερα στρώματα μπορεί να προσδιορίζουν δομές κατανοητές απο τον άνθρωπο, όπως αριθμούς, γράμματα ή πρόσωπα.  

Ωστόσο, ενώ τα δίκτυα βαθιάς μάθησης έχουν προχωρήσει σε σημείο που υπερτερούν της ανθρώπινης απόδοσης σε πολλαπλές δοκιμασίες, η αποδοτικότητα αυτών των δικτύων είναι τάξεις μεγέθους χαμηλότερη σε σύγκριση με τον ανθρώπινο εγκέφαλο. Συνεπώς, είναι λογικό να συνεχίσουμε να διερευνάμε τη δομή και την εσωτερική λειτουργία του ανθρώπινου εγκεφάλου, προκειμένου να αυξήσουμε την απόδοση των αλγορίθμων μηχανικής μάθησης και του υλικού που χρησιμοποιούμε για την εφαρμογή τους.

Τα νευρωνικά δίκτυα αιχμών \textlatin{(Spiking Neural Networks)} που εμπνέονται πολύ περισσότερο από την επεξεργασία πληροφοριών στη βιολογία από τα προηγούμενα (ANN) μπορεί να αποτελέσουν μια αποδοτικότερη λύση. Ο εγκέφαλος κωδικοποιεί πληροφορίες σε αραιά και ασύγχρονα σήματα που είναι εγγενώς επεξεργασμένα παράλληλα. Τα νευρωνικά δίκτυα βαθιάς εκμάθησης επεξεργάζονται την είσοδο στρώμα προς στρώμα και τα λάθη πρέπει να διαδίδονται προς τα πίσω με μη βιολογικά εύλογο τρόπο. Η επεξεργασία πληροφοριών στρώμα προς στρώμα υποδηλώνει ότι οι πληροφορίες δεν υποβάλλονται σε ασύγχρονη επεξεργασία. Αυτός ο περιορισμός επιβάλλεται από το υποκείμενο υλικό, τα σύγχρονα κυκλώματα. Ένα σύγχρονο κύκλωμα είναι ένα ψηφιακό κύκλωμα στα ψηφιακά ηλεκτρονικά μέσα στο οποίο οι αλλαγές στην κατάσταση των στοιχείων μνήμης συγχρονίζονται βάση ενός ρολογιού. Οι μέθοδοι εκμάθησης στα  νευρωνικά δίκτυα αιχμών και ένας νέος τύπος υλικού υπολογιστών, τα νευρομορφικά υπολογιστικά συστήματα, κάνουν μια προσπάθεια να χρησιμοποιήσουν ασύγχρονη επεξεργασία.

Μας ενδιαφέρει η επεξεργασία βίντεο χρησιμοποιώντας μηχανική μάθηση, δεδομένου ότι αυτή τη στιγμή είναι μία από τις πιο υπολογιστικά δαπανηρές εργασίες. Δεδομένου ότι οι τυπικές κάμερες καταγράφουν βίντεο σε πλαίσια εικόνας, το νευρωνικό δίκτυο πρέπει να επεξεργάζεται όλα τα εικονοστοιχεία κάθε φορά που εισάγεται ένα νέο πλαίσιο.Θα ήταν πιο οικονομικό αν μπορούσαμε να επεξεργαζόμαστε τα μεταβαλλόμενα εικονοστοιχεία ασύγχρονα . Αυτός είναι ο λόγος για τον οποίο αναπτύχθηκαν σένσορες δυναμικής όρασης \textlatin{(Dynamic Vision Sensors)} ονομάζονται και κάμερες συμβάντων \textlatin{(Event Cameras)}. Οι κάμερες συμβάντων είναι αισθητήρες βιολογικής έμπνευσης που λειτουργούν με κάπως διαφορετικό τρόπο από τις συμβατικές κάμερες. Υπολογίζουν τις αλλαγές φωτεινότητας ανά pixel ασύγχρονα αντί να συλλέγουν εικόνες ανα συγκεκριμένες χρονικές στιγμές. Ως αποτέλεσμα, παράγεται μια ροή γεγονότων που κωδικοποιεί την χρονική στιγμή, τη θέση και το πρόσημο των αλλαγών φωτεινότητας.

Προς το παρόν, αυτοί οι αισθητήρες είναι αρκετά ακριβοί, αλλά , έχουν καταγραφεί αρκετά σύνολα δεδομένων \textlatin{DVS} για ερευνητές όπως εμάς που θέλουν να δοκιμάσουν και να αναπτύξουν αλγόριθμους μηχανικής μάθησης νευρωνικών δικτύων σε δεδομένα που λαμβάνονται από αυτούς τους τύπους αισθητήρων.

Βλέπουμε τη διατριβή μας ως μια ευκαιρία να εισαγάγουμε ευρήματα από τη νευροεπιστήμη σε αναγνώστες υπόβαθρο ηλεκτρολόγων μηχανικών και μηχανικών πληροφορικής για να τους εμπνεύσουμε να εμβαθύνουν σε πράγματα που ο εγκέφαλος έχει να τους μάθει και πώς αυτό θα μπορούσε να οδηγήσει σε νέα και πιο εξελιγμένη τεχνητή νοημοσύνη και γιατί αυτό είναι απαραίτητο λαμβάνοντας υπόψη την τεράστια κατανάλωση ενέργειας των τρεχουσών μεθόδων. Ενημερώνουμε πρώτα τους αναγνώστες για τις ενεργειακές απαιτήσεις των σημερινών νευρωνικών δικτύων. Η διατριβή περιγράφει την έρευνα της νευροεπιστήμης με απλό τρόπο. Προσπαθούμε να περιγράψουμε ευρήματα από νευρώνες, συνάψεις, δενδρίτες και πώς αυτά σχηματίζουν μεγαλύτερες δομές, στη συνέχεια συνεχίζουμε με την κωδικοποίηση πληροφοριών, τη χρονική κωδικοποίηση των πληροφοριών μέσω πειραμάτων σε διάφορους οργανισμούς και μερικούς τρόπους με τους οποίους γίνεται η επεξεργασία πληροφοριών στον εγκέφαλο. Στη συνέχεια συγκρίνουμε το υλικό που χρησιμοποιεί απλά νευρικά δίκτυα, νευρομορφικά, με αρχιτεκτονική υπολογιστή  \textlatin{Von-Neumann}. Έπειτα εξηγούμε τις μεθόδους και τις εξισώσεις που χρησιμοποιούνται για την προσομοίωση μοντέλων νευρώνων και ποιοι αλγόριθμοι μάθησης μπορούν να χρησιμοποιηθούν για να ελέγξουν την αποτελεσματικότητα και την απόδοσή τους με βιβλιοθήκες λογισμικού που βασίζονται σε υπάρχουσες βιβλιοθήκες μηχανικής εκμάθησης όπως οι \textlatin{PyTorch} και \textlatin{TensorFlow}, λαμβάνοντας υπόψη την ανάγκη εφαρμογής αυτών των ευρημάτων στη βιομηχανία. Στην τελευταία ενότητα παρουσιάζουμε τα πειράματά μας σχετικά με τα σύνολα δεδομένων αισθητήρα δυναμικής όρασης και εξηγούμε λεπτομερώς τα αποτελέσματα.

\section{Το προβληματικό μέγεθος καταναλώσης ενέργειας της μηχανικής μάθησης}
Οι τεράστιοι υπολογιστικοί πόροι που απαιτούνται για την εκπαίδευση των μοντέλων βαθειών νευρωνικών δικτύων καταναλωνούν σημαντική ποσότητα ηλεκτρικής ενέργειας. Η εκπαίδευση συνήθως διαρκεί μερικές ημέρες, αν όχι εβδομάδες ανάλογα με το μέγεθος των δεδομένων εκπαίδευσης. Ως αποτέλεσμα, λόγω του κόστους υλικού, ηλεκτρικής ενέργειας καί χρόνου εκπαίδευσης, αυτά τα μοντέλα είναι οικονομικά και περιβαλλοντικά δαπανηρά για εκπαίδευση και κατασκευή, αφήνοντας υψηλό αποτύπωμα άνθρακα. Επιπλέον, η εκπαίδευση είναι ακόμα αργή σε σύγκριση με αυτό που είναι ικανός ένας ανθρώπινος εγκέφαλος. Οι ερευνητές σε μια έρευνα \cite{Strubell2019},υπολόγισαν την κατανάλωση ενέργειας για να εκπαιδεύσουν ένα μεγάλα νευρωνικά μοντέλα επεξεργασία φυσικής γλώσσας και έκαναν συγκρίσεις με παραραδείγματα απο την καθημερινή ζωή . Ένα μοντέλο μπορεί να καταναλώσει περισσότερα από μισό εκατομμύριο λίβρες \textlatin{CO}\textsubscript{2}, που είναι σχεδόν 5 φορές περισσότερο \textlatin{CO}\textsubscript{2} το οποίο παράγει ενα αυτοκίνητο στην διάρκεια ζωής του (δείτε το Σχήμα \ref{fig:energy-requirements}). Η ενέργεια που χρειάζεται για να εκπαιδεύσουμε το νευρωνικό μοντέλο \textlatin{BERT} είναι περίπου ο ίδιος με μια δια-αμερικανική πτήση. Ένας άνθρωπος, από την άλλη πλευρά, παράγει περίπου 11.000 κιλά \textlatin{CO}\textsubscript{2} κάθε χρόνο. Οι ερευνητές πρέπει να δώσουν έμφαση στο σχεδιασμό αποδοτικότερων μοντέλων και υλικού υπό το φως αυτών των πραγματικοτήτων. τρένο αιχμών
\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{Introduction/energy-requirements.PNG}
    \caption{}
    \label{fig:energy-requirements}
\end{figure}
\begin{figure}[htp]
    \centering
    \includegraphics[width=14cm]{Introduction/costs.PNG}
    \caption{}
    \label{fig:costs}
\end{figure}

Η μεταφορά ενός υπάρχοντος μοντέλου σε μια νέα δοκιμασία \textlatin{(task)} ή η δημιουργία νέων μοντέλων από την αρχή απαιτεί πρόσθετους πόρους. Το κόστος υπολογισμού σε \textlatin{cloud} για μερικά από τα πιο ακριβή μοντέλα γλωσσών καθώς και οι χρόνοι εκπαίδευσης φαίνονται στο Σχήμα \ref{fig:costs}. Ένα άλλο ζήτημα είναι ότι τα περισσότερα από τα μοντέλα που συζητήθηκαν σε αυτήν τη έρευνα \cite{Strubell2019} δημιουργήθηκαν εκτός ακαδημαϊκού χώρου. Οι πρόσφατες εξελίξεις στην κατέστησαν δυνατές χάρη στην εμπορική πρόσβαση σε υπολογισμούς μεγάλης κλίμακας. Λόγω του υψηλού κόστους εκπαίδευσης, ωστόσο, οι υπολογιστικοί πόροι δεν είναι προσβάσιμοι σε όλους, εμποδίζοντας φτωχότερες χώρες να αναπτύξουν δικά τους νευρωνικά μοντέλα. 


\chapter{Ο βιολογικός εγκέφαλος}
Η κλίμακα του εγκεφάλου των θηλαστικών είναι τεράστια. Κάθε ανθρώπινος εγκέφαλος περιλαμβάνει περίπου 25 χιλιάδες νευρώνες και 10 επί 10\textsuperscript{8} συνάψεις ανά κυβικό εκατοστό \cite{nguyen2013} στο νεοφλοιό (Ο νεοφλοιός είναι το εξωτερικό στρώμα του εγκεφάλου) . Υπολογίζεται ότι σε ηλικία 20 ετών, ο νεοφλοιός συνολικά, περιέχει περίπου 25 επί 10\textsuperscript{9} νευρώνες και σχεδόν 180 τρισεκατομμύρια συνάψεις! Θα διερευνήσουμε κάποια ευρήματα από έρευνες και πειράματα του εγκεφάλου και θα δούμε πώς αυτά μπορούν να μας βοηθήσουν να αναπτύξουμε αποτελεσματικότερες  μεθόδους μηχανικής μάθησης.

\section{Δομικά στοιχεία εγκεφάλου}
\subsection{Νευρώνες}
Αυτό το κεφάλαιο ξεκινά καθορίζοντας τον νευρώνα, την βασικότερο κύτταρο του εγκεφάλου \cite{gerstner2014}. Αυτά τα κύτταρα είναι υπεύθυνα για την επεξεργασία αισθητηριακών ανατροφοδοτήσεων από τον έξω κόσμο και τον μετασχηματισμό και επεξεργασία ηλεκτρικών σημάτων. \cite{balduzzi2013}. Ο \textlatin{Ramón y Cajal´} ήταν ο πρώτος ερευνητής που σχεδίασε απεικονίσεις νευρώνων αφού τους παρατήρησε κάτω από μικροσκόπιο \cite{garcialopezp2010}
.Ένα παράδειγμα αυτών απεικονίζεται στο σχήμα \ref{fig:neurons-ramoncajal}. Για καλύτερη κατανόηση απο τι αποτελείται ενας νευρώνας, δείτε το σχήμα. \ref{fig:neurons-multipolar}.

Οι νευρώνες μπορούν να συγκεντρωθούν (νευρωνικές συνθέσεις) μαζί στον εγκέφαλο και μπορούν να εκτείνονται από 1-2 mm και διαρκούν εκατοντάδες χιλιοστά του δευτερολέπτου. Ως αποτέλεσμα, ενδέχεται να είναι σε θέση να συνδέσουν γεγονότα από κάτω προς τα πάνω, σε μικρο-κλίμακα με γεγονότα από πάνω προς τα κάτω, μακρο-κλίμακας \cite{Badin2017}.
\begin{figure}[htp]
    \centering
    \includegraphics[width=4cm]{Neurons-Synapses/Ramon y Cajal.jpeg}
    \caption{Δενδρίτες: έχουν τραχιά επιφάνεια και φεύγουν από το κύτταρο πλάγια και προς τα πάνω. Οι νευράξονες είναι μικρές, ευθείες που φτάνουν προς τα κάτω με αρκετούς κλάδους αριστερά και δεξιά.\textlatin{Ramon y Cajal},1909.}
    \label{fig:neurons-ramoncajal}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{Neurons-Synapses/Blausen_0657_MultipolarNeuron.png}
    \caption{Εικονογράφηση νευρώνων. Παρατηρήστε τους δενδρίτες,  νευράξονες και τα συναπτικά τερματικά (\textlatin{synaptic terminals}) }
    \label{fig:neurons-multipolar}
\end{figure}
\subsection{Συνάψεις}
Ο εγκέφαλος αποτελείται από ένα τεράστιο δίκτυο νευρώνων. Οι νευρώνες επικοινωνούν μεταξύ τους μέσω συνάψεων, οι οποίες είναι εξειδικευμένες κυτταρικές συνδέσεις. Οι συνάψεις απαιτούνται όχι μόνο για νευρωνική σηματοδότηση και υπολογισμό αλλά και για μακροπρόθεσμες αλλαγές (συναπτική πλαστικότητα) που υποστηρίζουν την αποθήκευση πληροφοριών, όπως η μάθηση και η μνήμη, στον εγκέφαλο \cite{li2003}. Η περιοχή επαφής μεταξύ δύο νευρώνων που επικοινωνούν ορίζεται από δύο περιοχές: το προ-συναπτικό τερματικό (\textlatin{pre-synaptic terminal}) και την μετα-συναπτική θέση (\textlatin{post-synaptic target site}), τα οποία χωρίζονται από μια συναπτική σχισμή (\textlatin{synaptic cleft}) .
Στην νευροεπιστήμη έχουν ανακαλυφθεί χημικές και ηλεκτρικές συνάψεις, αλλά εμείς θα επικεντρωθούμε στις ηλεκτρικές συνάψεις. Οι ηλεκτρικές συνάψεις προκαλούν μια προ-συναπτική ώθηση (\textlatin{pre-synaptic impulse}) η οποία μετατραπέται γρήγορα σε ένα ηλεκτρικό διεγερτικό μετασυναπτικό δυναμικό (\textlatin{excitatory postsynaptic potential}) στο μετα-διασταυρούμενο κύτταρο (\textlatin{post-junctional cell}). Η ενεργοποίηση των καναλιών ιόντων με τάση οδηγεί στη δημιουργία δυναμικών δράσης (\textlatin{action potentials})  εάν το ρεύμα που μεταδίδεται στο μετασυναπτικό κύτταρο είναι αρκετό για να αποπολώσει τη μεμβράνη πάνω από ένα ορισμένο όριο \cite{Hormuzdi2004}. Ωστόσο, η ποσότητα διέγερσης και στα δύο κύτταρα δεν είναι ίση. Ένα λιγότερο αποπολωμένο συζευγμένο κύτταρο μπορεί να διεγερθεί από ένα πιο εκπολωμένo κύτταρο-σύντροφο (\textlatin{depolarized cell}) και ένα πιο εκπολωμένο κύτταρο μπορεί να ανασταλεί από ένα λιγότερο εκπολωμένο κύτταρο. Η σύναψη μπορεί επίσης να προκαλέσει μια συμπεριφορά ανόρθωσης (\textlatin{rectifying behavior}) \cite{Furshpan1959}. Οι ηλεκτρικές συνάψεις είναι πολύ ενδιαφέρουσες για τους ερευνητές λόγω της μοναδικής τους ικανότητα αμοιβαιότητας (επιτρέπει την μεταφορά πληροφορίας απο ένα νευρώνα σε άλλους) καθώς και λόγω της ικανότητάς τους να μεταφέρουν ηλεκτρικά δυναμικά κάτω από το κατώφλι που επιτρέπουν συγχρονισμένη δραστηριότητα νευρώνων.

\begin{figure}[htp]
    \centering
    \includegraphics[width=18cm]{Neurons-Synapses/Neurites-and-Synapses-header.jpg}
    \caption{Σύναψη}
    \label{fig:synapse}
\end{figure}

Έχει παρατηρηθεί ότι οι διασταυρώσεις διαστήματος (\textlatin{gap junctions}) εξυπηρετούν σημαντικό ρόλο στην ανάπτυξη του νευρικού συστήματος. Παράγουν επίσης μεγάλες λειτουργικές ομάδες συζευγμένων νευρώνων, οι οποίες συνήθως οργανώνονται σε κάθετες στήλες που εκτείνονται σε πολλά φλοιώδη στρώματα \cite{Fischbach1972} \cite{Peinado1993} \cite{Yuste1992} \ref{fig:verticalcolumns} .

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Neurons-Synapses/verticalcolumns.jpg}
    \caption{Κάθετες στήλες νευρώνων \cite{molnar2020}}
    \label{fig:verticalcolumns}
\end{figure}
\begin{figure}[htp]
    \centering
    \includegraphics[width=5cm]{Neurons-Synapses/angry-y-u-no.jpg}
    \caption{Σύναψη}
    \label{fig:spines}
\end{figure}

Δεδομένου ότι ενδιαφερόμαστε για το πώς τα ευρήματα από την έρευνα του εγκεφάλου μπορούν να εφαρμοστούν σε εφαρμογές μηχανικής μάθησης υπολογιστικής όρασης, είναι απαραίτητο να κατανοήσουμε πώς οι συνάψεις βοηθούν στην επεξεργασία των οπτικών ερεθισμάτων στον εγκέφαλο. Οι εισροές προ-συναπτικών νευρώνων μεταδίδονται μέσω μικροσκοπικών προεξοχών που ονομάζονται αγκάθια (\textlatin{spines}) στους μετασυναπτικούς δενδρίτες (βλ. Εικ. \ref{fig:spines}) \cite{tobias2017}. Ωστόσο,έχει σημασία που βρίσκεται μια είσοδος στο δενδριτικό δέντρο και το αν ενεργοποιείται από παρόμοια ερεθίσματα με εκείνα που πυροδοτούν τους γείτονές του , επιτρέποντας ταυτόχρονα ενεργές εισροές \cite{London2005}.

Για μεγάλο χρονικό διάστημα, ήταν ασαφές ποιες πληροφορίες λαμβάνει κάθε νευρώνας από διάφορα μέρη του οπτικού πεδίου και πώς αυτές οι πληροφορίες σχετίζονται με τα οπτικά χαρακτηριστικά που κωδικοποιούνται από το χωρικό δεκτικό πεδίο του νευρώνα έως ότου πρόσφατη έρευνα κατέδειξε περισσοτέρα για τον μηχανισμό αυτό. Οι είσοδοι που αντιπροσωπεύουν παρόμοια οπτικά χαρακτηριστικά από την ίδια θέση στον οπτικό χώρο ήταν πιο πιθανό να συγκεντρωθούν σε γειτονικά αγκάθια. Οι δενδριτικοί κλάδοι υψηλότερης τάξης συχνά συνάπτουν εισόδους από περιοχές οπτικού πεδίου πέρα από το δεκτικό πεδίο του μετασυναπτικού νευρώνα. Όταν το δεκτικό πεδίο της εισόδου μετατοπίζεται χωρικά κατά μήκος του άξονα της κατεύθυνσης του δέκτη του μετασυναπτικού νευρώνα (\textlatin{receptive field}), αυτές οι φερόμενοι είσοδοι μεγάλης εμβέλειας είναι πιο συχνές και πιο πιθανό να μοιραστούν την προτίμηση του μετασυναπτικού νευρώνα για προσανατολισμένες ακμές. Ως αποτέλεσμα, οι νευρώνες με εκτοπισμένα δεκτικά πεδία συνδέονται κατά προτίμηση όταν τα δεκτικά πεδία τους είναι προσανατολισμένα και ομοαξονικά ευθυγραμμισμένα. Αυτή η οργάνωση συναπτικής συνδεσιμότητας είναι κατάλληλη για την "ενίσχυση" επιμήκων ακμών,δηλαδή τονίζονται περισσότερο τα χαρακτηριστικά του αντικειμενού που παρατηρείται , και χρησιμεύει ως ένα πιθανό "πλαίσιο" για ολοκλήρωση περιγράμματος (αναφερόμαστε στην ικανότητα του οπτικού συστήματος να συνδέει ασύνδετα τοπικά στοιχεία σε συνεκτικά σχήματα \cite{persike2016}) και στην ομαδοποίηση αντικειμένων του οπτικού συστήματος \cite{Iacaruso2017} και παρέχει στοιχεία για την ιδέα ότι ο οπτικός χώρος "αποτυπώνεται" στους δενδρίτες με συγκεκριμένο τρόπο και όχι τυχαία.


\section{Αναπαράσταση και επεξεργασία πληροφοριών στον εγκέφαλο}
Μετά την ανασκόπηση των λειτουργικών στοιχείων του εγκεφάλου, ήρθε η ώρα να δούμε πώς οι πληροφορίες αντιπροσωπεύονται στον εγκεφαλικό φλοιό με τρένα αιχμών νευρωνικών πληθυσμών επιγραμματικά . Αυτά τα ευρήματα μπορούν να μας βοηθήσουν στην έρευση πιο σύνθετων μεθόδων επεξεργασίας αιχμών και καλύτερων κανόνων μηχανικής μάθησης.
\subsection{Νευρωνική αναπάρασταση}

Ένα μήνυμα που χρησιμοποιεί τους κανόνες και τις δομές με τις οποίες ένα σήμα μεταφέρει πληροφορίες (νευρωνικός κώδικας) για να εξυπηρετήσει μια συνάρτηση μπορούμε να τo αποκαλέσουμε νευρωνική αναπαράσταση.Η αναπαράσταση αποτελείται απο περιεχόμενο και η λειτουργία του . Το σήμα που μεταφέρει μια αισθητηριακή είσοδο είναι το περιεχόμενο της αναπαράστασης, ενώ η λειτουργία του είναι η επεξεργασία του αισθητηριακού σήματος εισόδου (γνωστική διαδικασία) και το αποτέλεσμα της γνωστικής διαδικασίας αυτής.

Οι οργανισμοί παράγουν έναν εσωτερικό αντίκτυπο που αναπαριστά τα ερεθίσματα του περιβάλλοντος τους \cite{Koch1994}. Το σήμα εισόδου πρέπει στη συνέχεια να έχει προβολές που του επιτρέπουν να παίζει ρόλο στις δραστηριότητες του οργανισμού για να προσαρμοστεί στο περιβάλλον του. Ο μηχανισμός μετασχηματισμού των παραστάσεων ονομάζεται αναπαράσταση. Οι μετασχηματισμοί που εξυπηρετούν οι αναπαραστάσεις θα ήταν αδύνατοι χωρίς υπολογιστικές διαδικασίες. Καθώς οι νευρωνικές αναπαραστάσεις προβάλλονται από τη μια φλοιώδη περιοχή στην άλλη, συχνά μεταμορφώνονται. Τα νευρωνικά κυκλώματα περιέχουν πληροφορίες στην μορφή αιχμών , τις οποίες στη συνέχεια μετατρέπουν χρησιμοποιώντας υπολογιστικές διαδικασίες \cite{decharms2000}. 

\subsection{Nευρωνική Κωδικοποίηση}

Ο τρόπος με τον οποίο οι πληροφορίες αντιπροσωπεύονται και επεξεργάζονται  ακριβώς στον εγκέφαλο εξακολουθεί να είναι ένα ανοιχτό πρόβλημα για τη νευροεπιστήμη, αλλά υπάρχουν αρκετές υποθέσεις. Η υπόθεση κωδικοποίησης ρυθμού (μέσος αριθμός αιχμών σε κάποιο χρονικό διάστημα) \cite{Salzman1992} \cite{Tovee1993} υποστηρίζει ότι οι πληροφορίες μεταφέρονται από τον μέσο ρυθμό πυροδότησης, ενώ η υπόθεση χρονικής κωδικοποίησης \cite{Bair1996} \cite{buracas1998} \cite{Rucci2018} ισχυρίζεται ότι ο ακριβής χρόνος των αιχμών είναι αυτός που κωδικοποιεί τις πληροφορίες που μεταφέρονται.
Η διάκριση μεταξύ αυτών των δύο υποθέσεων είναι το χρονικό διάστημα που χρησιμοποιείται για τον υπολογισμό των αιχμών, στην χρονική κωδικοποίηση το διάστημα είναι τόσο μικρό που μετράται μόνο μία ακίδα. Το διάστημα για ένα πείραμα που χρησιμοποιεί κωδικοποίηση ρυθμού αποφασίζεται με βάση τα χρονικά διαστήματα που θεωρούνται σημαντικά για μια δεδομένη κατάσταση, όπως το πόσο γρήγορα μετατοπίζεται το ερέθισμα, ο χρόνος ολοκλήρωσης μιας νευρικής μεταβλητής, η διαδικασία κωδικοποίησης ή η σχετική χρονική κλίμακα συμπεριφοράς του νευρώνα.

Μια σειρά απο αιχμές (\textlatin{spike train}) είναι ένα σύνθετο σήμα που μεταβάλλεται στο χρόνο και αποτελείται από πολλαπλές αιχμές που παράγονται από τον νευρώνα σε συγκεκριμένες χρονικές στιγμές. Η υπόθεση κωδικοποίησης ρυθμού θεωρεί σημαντικό μόνο έναν αριθμό, το μέσο ποσοστό αυτών των αιχμών. Παρόλο που η κωδικοποίηση και η αποκωδικοποίηση (η νευρωνική απόκριση αποκωδικοποιείται με την καταμέτρηση των αιχμών και το ερέθισμα κωδικοποιείται ρυθμίζοντας τον ρυθμό πυροδότησης ανάλογο με την τιμή κάποιας παραμέτρου ερεθίσματος) είναι απλές, αυτή η υπόθεση φαίνεται να είναι μια υπεραπλούστευση. Σε πειράματα συμπεριφοράς, οι χρόνοι απόκρισης είναι συχνά πολύ σύντομοι για να επιτρέψουν αργό χρονικό μέσο όρο \cite{thorpe1996}. Σε ένα άλλο πείραμα σε έναν οπτικό νευρώνα μύγας, το ερέθισμα που εξαρτάται από το χρόνο ανακατασκευάστηκε με επιτυχία από τους χρόνους πυροδότησης νευρώνων \cite{Bialek1991}. Υπάρχουν επίσης ενδείξεις συγχρονισμένων χρονικών συσχετίσεων μεταξύ παλμών διαφορετικών νευρώνων \cite{Lestienne1996}. 

\subsubsection{Νευρωνικές Συνθέσεις(\textlatin{Neuron Assemblies})}

Ο \textlatin{Yoshio Sakurai} στην έρευνά του \cite{sakurai1999} αναφέρει ότι οι μεμονωμένοι νευρώνες είναι ανεπαρκείς ως βασικός μηχανισμός κωδικοποίησης. Όπως αναφέραμε νωρίτερα, ο εγκέφαλος περιέχει έναν αμέτρητο αριθμό συνάψεων, που σημαίνει ότι κάθε νευρώνας λαμβάνει σήματα από χιλιάδες άλλους νευρώνες. Αυτό κάνει έναν μόνο νευρώνα να έχει μια ασταθή συμπεριφορά πυροδότησης καθώς το δυναμικό της μεμβράνης του υφίσταται μεγάλες διακυμάνσεις.

Επιπλέον, μεμονωμένοι νευρώνες έχουν ελάχιστη μόνο επιρροή σε άλλους νευρώνες και δεν μπορούν να παράγουν αρκετά ισχυρή μετάδοση για να προκαλέσουν αιχμές στους επόμενους νευρώνες όταν πρόκειται για λειτουργικές μεταδόσεις μεταξύ νευρώνων. Ο αριθμός των νευρώνων του εγκεφάλου είναι ανεπαρκής για να συλλάβει τον τεράστιο όγκο δεδομένων που επεξεργάζεται ένα ζώο κατά τη διάρκεια της ζωής του. Επειδή οι συνδυασμοί και οι διαμορφώσεις των στοιχείων παράγουν νέα στοιχεία, ο αριθμός των στοιχείων πληροφοριών είναι σχεδόν απεριόριστος. Η αναπαράσταση ενιαίων νευρώνων αυτών των στοιχείων είναι επίσης άβολη για τη συσχέτιση και τη διάκριση μεταξύ στοιχείων πληροφοριών, υποδεικνύοντας τον βαθμό ομοιότητας ή διαφοράς μεταξύ των στοιχείων ή τη δημιουργία νέων εννοιών και ιδεών από διαφορετικά στοιχεία. Έτσι, η υπόθεση του ενιαίου νευρώνα φαίνεται απίθανο να εξηγήσει την κωδικοποίηση και επεξεργασία του εγκεφάλου. Η δραστηριότητα του συνόλου ενός πληθυσμού νευρώνων φαίνεται πιο ικανή να κωδικοποιεί πληροφορίες στον εγκέφαλο. Αυτή η υπόθεση ονομάζεται κωδικοποίηση συνόλου.

Θα μπορούσαμε να περιγράψουμε τις νευρωνικές συνθέσεις ως ένα σύνολο νευρώνων που σχηματίζει μια λειτουργική ομάδα εάν τα νευρωνικά ερεθίσματα είναι συντονισμένα, στο βαθμό που οι χρονικές τους σχέσεις είναι διατεταγμένες, τουλάχιστον πιθανολογικά, σε χαρακτηριστικά μοτίβα \cite{gerstein1978}. Ένας νευρώνας δεν χρειάζεται να συμμετέχει σε μία μόνο λειτουργική ομάδα, μπορεί να είναι μέρος πολλαπλών νευρωνικών συνόλων σε διαφορετικούς χρόνους. Επιπλέον, οι νευρώνες που αποτελούν μια λειτουργική ομάδα δεν χρειάζεται να βρίσκονται σε άμεση συναπτική επαφή ή σε κοντινή απόσταση ανατομικά. Οι νευρωνικές συνθέσεις μπορούν επίσης να σχηματιστούν όταν οι νευρώνες μοιράζονται την ίδια δραστηριότητα εισόδου. Επειδή θα εστιάσουμε στην οπτική επεξεργασία σε αυτή τη διατριβή, ας εξετάσουμε πώς οι νευρωνικές συνθέσεις μπορούν να παίξουν ρόλο στα διάφορα στάδια της οπτικής επεξεργασίας.

Ορισμένες διαδικασίες που λαμβάνουν χώρα κατά τη διάρκεια μιας περιόδου μεταγεννητικής ανάπτυξης έχουν ως αποτέλεσμα την επιλεκτική σταθεροποίηση των συνδέσεων μεταξύ των νευρωνικών στοιχείων που συχνά έχουν συνδεδεμένη δραστηριότητα, επιτρέποντας την τροποποίηση της συνδεσιμότητας με βάση τις λειτουργικές παραμέτρους τους \cite{singer1990}. Στο στάδιο εισόδου των νευρωνικών συνόλων του ραβδωτού φλοιού μπορούν να συμβάλλουν στη βελτιστοποίηση της αντιστοίχισης μεταξύ των παραστάσεων των δύο ματιών. Βοηθά επίσης στην κατασκευή νευρωνικών αναπαραστάσεων για συχνά επαναλαμβανόμενες διαμορφώσεις χαρακτηριστικών (\textlatin{feature configurations}) και σε μεταγενέστερο επίπεδο επεξεργασίας συμμετέχοντας στο σχηματισμό επιλεκτικών συνδέσεων μεταξύ φλοιώδων στηλών (βλ. Επίσης ενότητα 2.1.2 για τις φλοιώδεις στήλες).

Η ενισχυμένη συσχετισμένη δραστηριότητα μπορεί να προκύψει από την επιλεκτική ενίσχυση των συναπτικών συνδέσεων μεταξύ των νευρώνων. Μια τέτοια επιλεκτική ενίσχυση αναφέρεται στην προσωρινή δυναμική που διατηρείται κατά τη διάρκεια μιας νευρικής δραστηριότητας, παρά στη μόνιμη αλλαγή της συναπτικής αποτελεσματικότητας μεταξύ των νευρώνων. Ως αποτέλεσμα, η προαναφερθείσα δυναμική διαμόρφωση σχετιζόμενων με γεγονότα και συμπεριφορές συσχετισμένων δραστηριοτήτων σε νευρώνες υποστηρίζει την ιδέα ότι οι νευρώνες μπορούν να συνδεθούν γρήγορα σε μια λειτουργική ομάδα προκειμένου να επεξεργαστούν τις απαιτούμενες πληροφορίες ενώ παραμένουν αποσυνδεδεμένοι από ταυτόχρονα ενεργοποιημένες ομάδες που ανταγωνίζονται. Τέλος, ας εξετάσουμε μερικές σημαντικές ιδιότητες της κωδικοποίησης αυτής:

\begin{enumerate}
    \item Επικαλυπτόμενη κωδικοποίηση στοιχείων πληροφοριών. Ο ίδιος νευρώνας είναι μέρος πολλών διαφορετικών νευρωνικών συνθέσεων. 
    \item Αραιή κωδικοποίηση πληροφοριών. Οποιαδήποτε μεμονωμένη νευρωνική σύνθεση περιέχει ένα μικρό υποσύνολο όλων των νευρώνων στον φλοιό.  
    \item Δυναμική κατασκευή και ανακατασκευή. Οι νευρωνικές συνθέσεις είναι χρονικά σύνολα νευρώνων που συνδέονται μεταξύ τους με ευέλικτες λειτουργικές συνάψεις.
    \item Δυναμική διατήρηση. Η ενεργοποίηση μιας νευρωνικής σύνθεσης θα επιμείνει για ένα διάστημα μέσω ανατροφοδότησης λόγω των διεγερτικών συνάψεων μεταξύ των νευρώνων.
    \item Δυναμική ολοκλήρωση. Η ενεργοποίηση ενός αρκετά μεγάλου υποσυνόλου μιας νευρωνικής σύνθεσης έχει ως αποτέλεσμα την ενεργοποίηση ολόκληρης της νευρωνικής σύνθεσης.
\end{enumerate}
Η υπόθεση κωδικοποίησης νευρωνικής σύνθεσης είναι συναρπαστική, καθώς μπορεί να επιβεβαιωθεί από πολλές ερευνητικές μελέτες στον ανθρώπινο εγκέφαλο.Για παράδειγμα, διαπιστώθηκε λειτουργική αλληλεπικάλυψη μεμονωμένων νευρώνων και δυναμική συσχέτισης μεταξύ πολλών νευρώνων στη λειτουργική μνήμη και στη μνήμη αναφοράς.
Οι νευρωνικές συνθέσεις αποτελούνται από μεμονωμένους νευρώνες που σχετίζονται με μια λειτουργία,αυτο μπορεί να περιγράψει την δυνατότητα διπλής κωδικοποίησης από νευρωνικές συνθέσεις και τον "ενιαίο νευρώνα τους" . Τα πειραματικά δεδομένα για αυτά τα πειράματα μπορούν να βρεθούν εδώ: \cite{sakurai1999}. Η κατανόηση των σημαντικών αποτελεσμάτων πίσω από αυτά τα πειράματα μπορεί να επιτρέψει πιο αποτελεσματικές μεθόδους μηχανικής μάθησης που προσομοιάζουν περισσότερο τον εγκέφαλο.
\begin{figure}[htp]
    \includegraphics[width=10cm,right]{Brain Inspiration/neuron assemblies/assemblies.PNG}
    \caption{Οπτικοποίηση ορισμένων ιδιοτήτων των νευρωνικών συνθέσεων.}
    \label{fig:assemblies}
\end{figure}


\subsection{Συναπτική Πλαστικότητα}
Η συναπτική πλαστικότητα είναι μια διαδικασία που τροποποιεί τη συνδεσιμότητα των νευρώνων που επηρεάζεται από την ποσότητα διέγερσης μεταξύ τους . Ο νευροεπιστήμονας \textlatin{Shatz} το περιέγραψε ως "κύτταρα που πυροδοτούνται μαζί, συνδέονται μαζί"  (\textlatin{"cells that fire together, wire together"}) \cite{shatz1992}. Πιο συγκεκριμένα, εάν ένα από τον νευρώνα είναι συστηματικά ενεργό λίγο πριν από έναν άλλο, η πυροδότηση του πρώτου μπορεί να έχει αιτιώδη σχέση με τη πυροδότηση ενός δεύτερου, το οποίο μπορεί αργότερα να ανακαλέσει με την ενίσχυση των συνδέσεων,αυτήν την ικάνοτητα των νευρώνων την ορίζουμε ως συναπτική πλαστικότητα. Ο συγχρονισμός είναι σημαντικός αφού μπορεί να αποκαλύψει αιτιότητα και μπορεί επίσης να χρησιμεύσει ως ένα χρονικό σχήμα κωδικοποίησης σε χρονική κλίμακα χιλιοστών του δευτερολέπτου.

Στα μέσα της δεκαετίας του 1990 ,κυριαρχούσε η ιδέα του \textlatin{Hebb} οτι που έχει σημασία στην στην πλαστικότητα είναι η συμπτωματική δραστηριότητα σε συνδεδεμένους νευρώνες . Ομώς,συνειδητοποιήθηκε ότι οι συναπτικές συνδέσεις του εγκεφάλου έχουν μηχανισμούς που θα τους έκαναν ιδιαίτερα ευαίσθητους στο χρονισμό κάτι που επιβεβαιώθηκε σε πολυάριθμες μελέτες  \cite{markram1995}\cite{markram1997}\cite{Gerstner1996} .Οι νευρωεπιστήμονες ονόμασαν τα ευρήματα τους ως Εκμάθηση Πλαστικότητας που εξαρτάται από το χρόνο (\textlatin{Spike Time Dependent Plasticity -  STDP}). Με την ενίσχυση εκείνων των εισροών που προβλέπουν τη δική τους δραστηριότητα αιχμών, ένας νευρώνας που περιέχεται σε ένα νευρωνικό δίκτυο μπορεί να επιλέξει τους γειτονικούς νευρώνες που "αξίζει να ακούσει" με το \textlatin{STDP}. Ο επίμαχος νευρώνας, από την άλλη πλευρά, δίνει λιγότερη προσοχή στους γύρω νευρώνες που δεν το κάνουν. Ως αποτέλεσμα, ένας νευρώνας μπορεί να ενσωματώσει εισόδους με προγνωστική δύναμη και να τις μεταφράσει σε μια σημαντική πρόβλεψη εξόδου, ακόμη και αν η έννοια δεν είναι πλήρως κατανοητή από τον νευρώνα. Ως αποτέλεσμα, το \textlatin{STDP} παρέχει έναν πολύ βασικό και κομψό μηχανισμό για τη σωστή "σύνδεση" των νευρώνων στον εγκέφαλο \cite{Markram2012}.

\subsection{Ταλαντώσεις}
Οι ταλαντώσεις που συμβαίνουν σε διαφορετικές συχνότητες θεωρούνται ως λειτουργικά συναφή σήματα του εγκεφάλου και η έρευνα υποστηρίζει ότι οι ταλαντώσεις που σχετίζονται με γεγονότα γεφυρώνουν το χάσμα μεταξύ μεμονωμένων νευρώνων και νευρωνικών συνθέσεων \cite{Basar2000}. Η έρευνα υποθέτει επίσης ότι τα επιλεκτικά κατανεμημένα συστήματα ταλάντωσης δέλτα, θήτα, άλφα και γάμμα λειτουργούν ως ηχηρά δίκτυα επικοινωνίας μέσω μεγάλου πληθυσμού νευρώνων. Υποστηρίζουν τον χρονικό συγχρονισμό της συμπεριφοράς των νευρωνικών πληθυσμών και έχουν εμπλακεί ως ένας μηχανισμός που επιλέγει υποσύνολα νευρώνων για περαιτέρω συλλογική επεξεργασία και ενδεχομένως ως ένας μηχανισμός αναπαράστασης των ερεθισμάτων, επειδή μπορούν να εμφανίσουν εξάρτηση από μια εργασία (\textlatin{task}) ή απο ένα ερέθισμα \cite{Singer1995} \cite{Singer1999}.

\chapter{Νευρομορφικά Υπολογιστικά Συστήματα}

Ο νόμος του \textlatin{Moore}, ο οποίος προέβλεψε εκθετική αύξηση του αριθμού των τρανζίστορ που θα μπορούσαν να γίνουν σε ένα μόνο μικροτσίπ, οδήγησε τις εξελίξεις στην τεχνολογία μικροτσίπ. Η εκθετική χρονική σταθερά είναι μικρή, διπλασιάζεται κάθε 18 μήνες. Ο νόμος του \textlatin{Moore} "εφαρμόστηκε" κυρίως μέσω της μείωσης του μεγέθους των τρανζίστορ, καθώς οσο τα τρανζίστορ CMOS γίνονται μικρότερα, γίνονται φθηνότερα, γρηγορότερα και πιο ενεργειακά αποδοτικά.
Τα Νευρομορφικά Υπολογιστικά Συστήματα περιλαμβάνουν ένα ευρύ φάσμα τεχνικών επεξεργασίας πληροφοριών, όλες διακριτές από τα συμβατικά συμβατικά συστήματα υπολογιστών λόγω κάποιου βαθμού νευροβιολογικής έμπνευσης. Η θεωρία στην οποία στηρίζονται τα Νευρομορφικά μπορεί να εντοπιστεί στο θεμελιώδες έργο του \textlatin{Carver Mead} στο \textlatin{Caltech} στα τέλη της δεκαετίας του 1980. Αυτό το πρώιμο έργο ενέπνευσε άλλους να συνεχίσουν να αναπτύσσουν νευρομορφικές συσκευές και οι προαναφερθείσες εξελίξεις στην τεχνολογία \textlatin{VLSI} βοήθησαν τη συνεχή επέκταση στο μέγεθος και τη λειτουργικότητα των νευρομορφικών συσκευών \cite{furber2016}.

Τα τρέχοντα ψηφιακά σχέδια υπολογιστών γενικής χρήσης παρέχουν την "ανοσία" θορύβου και την προβλέψιμη συμπεριφορά για την οποία είναι γνωστή η μηχανή \textlatin{Turing}. Η βιολογία παραιτείται από τον ντετερμινισμό υπέρ της αποδοτικότητας, ο οποίος θα μπορούσε να ενδιαφέρει μελλοντικούς μηχανικούς υπολογιστών που απασχολούνται με συστήματα όπως συστήματα όρασης ρομπότ, όπου η απόλυτη ακρίβεια είναι αδύνατο να επιτευχθεί και η ενεργειακή απόδοση είναι κορυφαία προτεραιότητα. Τα Νευρομορφικά Υπολογιστικά Συστήματα στοχεύουν στην εξαγωγή ή στην μίμηση της πολυπλοκότητας του ανθρώπινου εγκεφάλου και των αρχών λειτουργίας του σε πιο αφηρημένες μεθόδους που μπορούν να εφαρμοστούν σε ένα υπολογιστικό σύστημα αυτού του τύπου. Τα νευρομορφικά υπολογιστικά συστήματα τηρούν την αρχή του κατανεμημένου υπολογισμού, δηλαδή έχουν μεγάλη ποσότητα μικρών υπολογιστικών «πυρήνων» ανάλογων με νευρώνες συνδεδεμένους σε δίκτυα με κάποιο βαθμό επιτρεπόμενης ευελιξίας συνδεσιμότητας. 

\section{Δομικά στοιχεία και αρχιτεκτονική}

Τα νευρομορφικά συστήματα και οι εφαρμογές τείνουν να αποκλίνουν από τη νόρμα και αντί της χρήσης τυπικών τρανζίστορ και στοιχείων κυκλώματος (π.χ. αντιστάσεων) που χρησιμοποιούν οι περισσότερες ηλεκτρονικές πλατφόρμες, εκμεταλλεύονται χαρακτηριστικά δομικών στοιχείων που παρουσιάζουν επιθυμητή συμπεριφορά όπως η μνήμη. Παρ' όλα αυτά, υπάρχουν υλοποιήσεις με τυπικά ηλεκτρονικά στοιχεία και παρουσιάζουν ικανοποιητικές επιδόσεις, όπως στο \cite{Clayton2011}

Μια τέτοια ηλεκτρονική συσκευή που χρησιμοποιείται για την ανάπτυξη νευρομορφικής πλατφόρμας είναι το \textlatin{CMOS}, Συμπληρωματικός ημιαγωγός μετάλλου-οξειδίου (εικ. 3.1). Ένας πλήρης οδηγός για το \textlatin{CMOS} παρουσιάζεται στο βιβλίο \cite{Baker2005}. Τα \textlatin{CMOS} προσφέρουν μεγάλα πλεονεκτήματα σε σχέση με τυπικά στοιχεία κυκλώματος, όπως η χαμηλή κατανάλωση ενέργειας και η επεκτασιμότητα και έχουν χρησιμοποιηθεί σε προηγούμενα έργα όπως στο \cite{Nair2020}, όπου οι συγγραφείς ανέπτυξαν Χρονικό Νευρωνικό Δίκτυο με 32 εκατομμύρια \textlatin{CMOS} των 7\textlatin{nm} που καλύπτουν συνολικά 1,54\textlatin{mm}\textsuperscript{2} με μόνο 7,26\textlatin{mW} ισχύ με δυνατότητα επεξεργασίας εικόνας εισόδου 28\textlatin{x}28 στα 9,34\textlatin{ns}. Μεγάλα έργα όπως το τσιπ \textlatin{TrueNoth} της \textlatin{IBM}, χρησιμοποιούν επίσης την τεχνολογία \textlatin{CMOS} για νευρομορφικούς υπολογισμούς.

\begin{figure}[htp]
    \centering
    \includegraphics[width=3cm]{Neuromorphics/1200px-CMOS_inverter.svg.png}
    \caption{Σχηματικό διάγραμμα CMOS.}
    \label{fig:truenorth}
\end{figure}

Ωστόσο, μία από τις πιο σημαντικές συσκευές για νευρομορφικούς υπολογιστές είναι το memristor \cite{Chua1971}. Το \textlatin{Memristor} είναι ένα βασικό ηλεκτρικό εξάρτημα δύο τερματικών, το οποίο παρουσιάζει τη συμπεριφορά της μνήμης θυμώντας το ιστορικό της. Αυτή η ιδιότητα καθιστά αυτό το στοιχείο κατάλληλο για νευρομορφικούς υπολογισμούς καθώς μπορεί να μιμηθεί τη συμπεριφορά του βιολογικού νευρώνα και έχει χρησιμοποιηθεί επανειλημμένα ως σύναψη, παρέχοντας το χαρακτηριστικό της συναπτικής πλαστικότητας στο δίκτυο \cite{Wang2012}, \cite{Jo2010}. Λόγω του μεγάλου ενδιαφέροντος της ερευνητικής κοινότητας για τη χρήση των memristors στις νευρομορφικές πλατφόρμες, έχει αναπτυχθεί μια μεγάλη ποικιλία εφαρμογών που βασίζονται σε \textlatin{memristor}, προκειμένου να αυξηθεί η αποτελεσματικότητά τους, όπως η \textlatin{multi resistive} σύναψη που προτείνεται στο έργο των Boybat \textlatin{et}. \textlatin{al}. \cite{Boybat2018}.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.3\linewidth]{Neuromorphics/Memristor-Symbol.jpg}
  \caption{Σύμβολο \textlatin{memristor}}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.7\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{Neuromorphics/memristor-vi-curve-with-qf-curve.png}
  \caption{Χαρακτηριστική συνάρτηση τάσης-έντασης}
  \label{fig:sub2}
\end{subfigure}
\begin{subfigure}{.8\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{Neuromorphics/pssa201700875-blkfxd-0001-m.jpg}
  \caption{Παράδειγμα χρήσης των \textlatin{memristor} σε προσομοίωση νευρώνων}
  \label{fig:sub2}
\end{subfigure}
\caption{Χαρακτηριστικά και χρήσεις των \textlatin{memristor} }
\label{fig:test}
\end{figure}

Σε συνέχεια της υψηλής αξιοποίησης του χώρου και της ενέργειας των νευρομορφικών υπολογιστών, η χρήση των \textlatin{spintronics} \cite{Utic2004} εισήχθη στο πεδίο. Τα \textlatin{Spintronics} είναι συσκευές που χρησιμοποιούν τόσο τις ηλεκτρικές όσο και τις μαγνητικές ιδιότητες των ηλεκτρονίων αυξάνοντας τις υπολογιστικές δυνατότητες κάθε μεμονωμένης συσκευής με χαμηλή κατανάλωση ενέργειας. Το 2009 ο \textlatin{Wang } \textlatin{et}. \textlatin{al}. στο έργο τους \cite{Wang2009}, πρότειναν την ανάπτυξη \textlatin{spintronic memristors}, δηλαδή \textlatin{spintronic} συσκευών με \textlatin{memristive} χαρακτηριστικά. Ο \textlatin{Torrejon}  το 2017 \cite{Torrejon2017} έδειξαν ότι η βιολογική ταλαντωτική συμπεριφορά των νευρώνων μπορεί να εφαρμοστεί με τη χρήση ταλαντωτών νανοκλίμακας. Όπως δείχνουν, τέτοιες συσκευές παρουσιάζουν αντοχή στο θόρυβο και μπορούν να παρέχουν σημαντικά αποτελέσματα στις δοκιμές. Στο έργο των \textlatin{Grollier} \textlatin{et}. \textlatin{al}. \cite{Grollier2020}, οι μέθοδοι και τα αποτελέσματα των \textlatin{spintronic memristors} , ταλαντωτών και άλλων μεθόδων που βασίζονται σε \textlatin{spintronic} συνοψίζονται και εξετάζονται, παρέχοντας μια πλήρη εικόνα των \textlatin{spintronics} στα νευρομορφικά συστήματα.

Η ιδιότητα της συναπτικής πλαστικότητας είναι μια σημαντική πτυχή της έρευνας στα νευρομορφικα. Ένας διαφορετικός τρόπος επαγωγής της συναπτικής πλαστικότητας στο υλικό είναι η χρήση κατάλληλων τύπων \textlatin{RAM} (\textlatin{Random Access Memory} - Μνήμη Τυχαίας Προσπέλασης) ως συνάψεις. Οι πιο αξιοσημείωτοι τύποι \textlatin{RAM} που χρησιμοποιούνται στα νευρομορφικά δίκτυα είναι η αντιστατική \textlatin{RAM} (\textlatin{RRAM}), η \textlatin{RAM}  αγώγιμης γεφύρωσης (\textlatin{CBRAM}), η \textlatin{RAM} αλλαγής φάσης (\textlatin{PRAM} ή \textlatin{RCM}) και η μαγνητική \textlatin{RAM} (\textlatin{MRAM}) που εξετάστηκαν και μελετήθηκαν λεπτομερώς στις ακόλουθες εργασίες \cite{Moon2019}, \cite{Cha2020}, \cite{Park2020}. Στην ακόλουθη εργασία \cite{Suri2011}, οι \textlatin{Suri} \textlatin{et}. \textlatin{al}. έδειξε ότι η \textlatin{RCM} μπορεί να μιμηθεί συμπεριφορές βιολογικών συνάψεων όπως τα φαινόμενα πλαστικότητας Μακροπρόθεσμης Ενδυνάμωσης (\textlatin{LTP}) και Μακροπρόθεσμης Καταστολής (\textlatin{LTD}), τα οποία είναι σημαντικά χαρακτηριστικά για τους νευρομορφικούς υπολογιστές.

Όσο χρήσιμη και αν είναι η ανάπτυξη ιδανικών δομικών στοιχείων για νευρομορφικούς υπολογισμούς, το ίδιο χρήσιμη είναι και η πρόοδος στον σχεδιασμό της αρχιτεκτονικής τέτοιων δικτύων. Σημαντικά χαρακτηριστικά μελετώνται συνεχώς, όπως επεκτάσιμα δίκτυα \cite{Merolla2007}, τα οποία προσφέρουν επεκτασιμότητα χωρίς πρόσθετη καθυστέρηση ή τοπολογίες που αυξάνουν την απόδοση του δικτύου. Στo \cite{Akbari2017}, οι συγγραφείς προτείνουν μια νέα τοπολογία της τεχνολογίας \textlatin{Network on Chip} , βασισμένη στην τοπολογία της λιβελλούλης (\textlatin{Dragonfly}), η οποία προσφέρει ταχύτερη επεξεργασία πληροφοριών στο δίκτυο.

\begin{figure}
\centering
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{Neuromorphics/Sample-Dragonfly-topology-with-h2-p2-a4-36-routers-and-72-compute-nodes.png}
  \caption{Η τοπολογία Dragonfly μεταξύ 5 ομάδων}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{Neuromorphics/Screenshot 2021-09-20 225336.png}
  \caption{Προσαρμοσμένη τοπολογία Dragonfly που προτείνεται στο \cite{Akbari2017}}
  \label{fig:sub2}
\end{subfigure}
\caption{\small Διαφορές στην τοπολογία Dragonfly (α) και στην προσαρμοσμένη τοπολογία που προτάθηκε από τους συγγραφείς του \cite{Akbari2017} (β). Στο (β) οι κόμβοι κάθε ομάδας είναι συνδεδεμένοι μεταξύ τους μέσω ενός διαύλου ο οποίος δεν σχεδιάζεται για απλότητα.}
\label{fig:test}
\end{figure}


Η χωρική και ενεργειακή απόδοση είναι μια σημαντική πτυχή των νευρομορφικών υπολογισμών, η οποία καθιστά την τεχνολογία ελκυστική στην ερευνητική κοινότητα και αναπτύσσεται συνεχώς. Με την αυξανόμενη έρευνα της μικροηλεκτρονικής και της νανοτεχνολογίας, πολλές συσκευές μπαίνουν στο παιχνίδι για να ξεπεράσουν τους ανταγωνιστές τους. Οι νανοΐνες, τα ηλεκτροχημικά τρανζίστορ, οι νανοσωλήνες, οι κβαντικές κουκκίδες και πολλά άλλα βρίσκονται στο μικροσκόπιο για να βελτιώσουν την απόδοση των μευρωμορφικών υπολογιστών.

Τα νευρομορφικά προσφέρουν έναν νέο τρόπο υπολογισμού. Χρησιμοποιώντας τις κατάλληλες μονάδες που δείχνουν επιθυμητή συμπεριφορά, όπως η μη πτητική μνήμη, η επεξεργαστική ισχύς τους αυξάνεται ραγδαία, ενώ ο στοχευμένος σκοπός αυτών των συστημάτων, κυρίως στα Νευρωνικά Δίκτυα αιχμών, επιτρέπει τη χρήση αρχιτεκτονικών διαφορετικών από το \textlatin{von Neumann}  - η πιο κοινή αρχιτεκτονική που χρησιμοποιείται στους σύγχρονους υπολογιστές, για την εξειδίκευση και την εκμετάλλευση χαρακτηριστικών που η προαναφερθείσα αρχιτεκτονική δεν μπορεί να προσφέρει.

\section{Νευρομορφικοί Υπολογιστές}

Τα τελευταία χρόνια, εμφανίστηκε ένας αριθμός νευρομορφικών συστημάτων μεγάλης κλίμακας, χρησιμοποιώντας τον τεράστιο πόρο τρανζίστορ που είναι τώρα διαθέσιμος σε ένα μόνο μικροτσίπ και, σε μια περίπτωση, ολόκληρο δίσκος πυριτίου. Οι δυνατότητες της τεχνολογίας συνδυάζονται με κλιμακούμενες αρχιτεκτονικές για να επιτρέψουν την αύξηση των νευρομορφικών δυνατοτήτων για την υποστήριξη νευρωνικών δικτύων με εκατομμύρια νευρώνες και δισεκατομμύρια συνάψεις. Οι υπολογιστικοί νευροεπιστήμονες μπορούν τώρα να εξετάσουν την ανάπτυξη μοντέλων ολόκληρου του εγκεφάλου των πλασμάτων που κυμαίνονται από έντομα έως μικροσκοπικά θηλαστικά ή μεγάλες υπο-περιοχές του ανθρώπινου εγκεφάλου. Τα ίδια συστήματα παρέχουν επίσης πλατφόρμες ικανές να υποστηρίξουν νέες κλίμακες γνωστικής αρχιτεκτονικής. Μερικά από τα πιο αξιοσημείωτα παραδείγματα είναι τα ακόλουθα.
\subsubsection{\textlatin{IBM TrueNorth}}
Το τσιπ \textlatin{IBM TrueNorth}  βασίζεται σε κατανεμημένα ψηφιακά νευρωνικά μοντέλα που στοχεύουν σε γνωστικές εφαρμογές σε πραγματικό χρόνο. Το τσιπ είναι ένα πολύ μεγάλο, \textlatin{CMOS} τσιπ 28 νανόμετρων 5,4 εκατομμυρίων τρανζίστορ που ενσωματώνει 4096 νευροσυναπτικούς πυρήνες όπου κάθε πυρήνας περιλαμβάνει 256 νευρώνες ο καθένας με 256 συναπτικές εισόδους. Η διασταυρούμενη έξοδος συνδέει το μοντέλο του ψηφιακού νευρώνα, το οποίο εφαρμόζει μια μορφή αλγορίθμου \textlatin{integrate-and-fire} με 23 παραμετροποιήσιμες παραμέτρους που μπορούν να προσαρμοστούν για να αποδώσουν μια ποικιλία διαφορετικών συμπεριφορών και ψηφιακές ψευδοτυχαίες πηγές χρησιμοποιούνται για τη δημιουργία στοχαστικών συμπεριφορών, διαμορφώνοντας συναπτικές συνδέσεις, κατώφλι νευρώνων και διαρροή νευρώνων \cite{cassidy2013}. Οι έξοδοι των γεγονότων ακίδας του νευρώνα κάθε πυρήνα ακολουθούν ατομικά διαμορφώσιμες διαδρομές από σημείο σε σημείο στην είσοδο ενός άλλου πυρήνα, οι οποίες μπορεί να είναι στον ίδιο ή ένα διαφορετικό τσιπ \textlatin{TrueNorth}. Όταν η έξοδος ενός νευρώνα πρέπει να συνδέεται με δύο ή περισσότερους νευροσυναπτικούς πυρήνες, ο νευρώνας διπλασιάζεται στον ίδιο πυρήνα (βλ. Σχήμα \ref{fig:truenorth}) . Ο ντετερμινιστικός χαρακτήρας του ψηφιακού μοντέλου διασφαλίζει ότι όλα τα αντίγραφα παράγουν πανομοιότυπα τρένα ακίδας (\textlatin{spike trains}) \cite{furber2016}.
\begin{figure}[htp]
    \centering
    \includegraphics[width=14cm]{Neuromorphics/truenorth.PNG}
    \caption{Οι επικοινωνίες του \textlatin{TrueNorth}  βασίζονται σε συνδέσμους από σημείο σε σημείο που μεταφέρουν αιχμές από έναν μόνο νευρώνα σε έναν μόνο νευροσυναπτικό πυρήνα, όπου οι αιχμές μπορούν να συνδεθούν με οποιονδήποτε ή όλους τους 256 νευρώνες του πυρήνα. Για παράδειγμα εδώ, ο αριστερότερος νευρώνας του πυρήνα 1 συνδέεται με τον πυρήνα 3. Ο 2ος και ο 3ος νευρώνας του πυρήνα 1 αντιγράφουν ο ένας τον άλλο για να συνδεθούν με τους πυρήνες 2 και 3 και ο καθένας κάνει μία σύνδεση. }
    \label{fig:truenorth}
\end{figure}

\subsubsection{\textlatin{SpiNNaker}}
Το έργο \textlatin{SpiNNaker} \cite{furber2014} έχει αναπτύξει έναν μαζικά παράλληλο ψηφιακό υπολογιστή του οποίου η υποδομή επικοινωνίας είναι εμπνευσμένη από το στόχο της μοντελοποίησης μεγάλης κλίμακας νευρωνικών δικτύων σε βιολογικό πραγματικό χρόνο με συνδέσεις παρόμοιες με αυτές του εγκεφάλου. Το μεγαλύτερο σύστημα \textlatin{SpiNNaker} που χρησιμοποιείται αυτή τη στιγμή έχει 1.000.000 πυρήνες. Ωστόσο, το \textlatin{SpiNNaker} δεν μοιάζει με άλλα νευρομορφικά συστήματα. Χρησιμοποιεί μικρούς ακέραιους πυρήνες (προσαρμοσμένα τσιπ) που προορίζονται για ενσωματωμένες εφαρμογές για κινητά. 
\begin{figure}[htp]
    \centering
    \includegraphics[width=14cm]{Neuromorphics/spinnaker.PNG}
    \caption{Συστήματα \textlatin{SpiNNaker} σε μικρή κλίμακα. Το σύστημα 4 κόμβων (72 πυρήνων) (αριστερά) τροφοδοτείται από υποδοχή USB και είναι ιδανικό για μάθηση και μικρές εργασίες, όπως εργασίες σε ρομπότ. Το σύστημα 48 κόμβων (864 πυρήνων) (δεξιά) είναι το βασικό δομικό στοιχείο των μεγαλύτερων υπολογιστών και μπορεί να χρησιμοποιηθεί για μεγαλύτερες εφαρμογές}
    \label{fig:spinnaker}
\end{figure}

Το ύφασμα επικοινωνίας του \textlatin{SpiNNaker} έχει σχεδιαστεί για την αποστολή μεγάλου όγκου μικρών πακέτων δεδομένων (δηλαδή αιχμές νευρώνων) σε πολλούς προορισμούς σύμφωνα με στατιστικά διαμορφωμένες διαδρομές πολλαπλής διανομής \cite{plana2011}.
Ο σχεδιασμός του \textlatin{SpiNNaker} βασίζεται σε ένα μικρό πλαστικό πακέτο 300 \textlatin{bga} (συστοιχία πλέγματος) το οποίο ενσωματώνει ένα προσαρμοσμένο τσιπ επεξεργασίας και ένα τυπικό τσιπ μνήμης 128 \textlatin{Mbyte SDRAM}. Το τσιπ επεξεργασίας, σχεδιασμένο σε τεχνολογία \textlatin{CMOS 130 nm}, περιέχει 18 πυρήνες επεξεργαστή \textlatin{ARM968}, ο καθένας με 32 \textlatin{Kbytes} μνήμης εντολών και 64 \textlatin{Kbytes} μνήμης δεδομένων, δρομολογητή πακέτων πολλαπλών εκπομπών και διάφορα στοιχεία υποστήριξης \cite{painkras2013} \cite{furber2016 }.

\begin{figure}[htp]
    \centering
    \includegraphics[width=14cm]{Neuromorphics/overview.PNG}
    \caption{Επισκόπηση των νευρομορφικών τσιπς και σύγκριση με τον ανθρώπινο εγκέφαλο. Έχουμε ακόμα πολύ δρόμο για να φτάσουμε την αποδοτικότητα του ανθρώπινου εγκεφάλου.}
    \label{fig:overview}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=14cm]{Neuromorphics/photonics.PNG}
    \caption{Σύγκριση νευρομορφικών πλατφορμών υλικού. Η υπεροχή των νευρομορφικών υπολογιστικών συστημάτων ως προς την απόδοση είναι εμφανής. Τα μελλοντικά φωτονικά νευρομορφικά συστήματα θα μπορούσαν να προσφέρουν οαόμη καλύτερες επιδόσεις, αλλά τα φωτονικά δεν αποτελούν μέρος αυτής της διπλωματικής εργασίας.\cite{shastri2018} }
    \label{fig:overview}
\end{figure}


\chapter{Μοντέλα Νευρώνων}

Σε αυτό το κεφάλαιο, θα στρέψουμε την προσοχή μας από τον βιολογικό εγκέφαλο στον τεχνητό. Τα νευρωνικά δίκτυα αιχμών (ΝΔΑ) διαφέρουν από τα κοινά τεχνητά νευρωνικά δίκτυα που έχουν αναπτυχθεί τον τελευταίο αιώνα, όπως αναφέρθηκε προηγουμένως, κυρίως στη χρονική διάσταση των δεδομένων εισόδου. Αυτή η διαφορά δημιουργεί την ανάγκη για ανάπτυξη μοντέλων νευρώνων έτσι ώστε να μπορούν να αποκωδικοποιούν και να εξάγουν πληροφορίες από τα χρονικά δεδομένα. Στην βιβλιογραφία, πολλά τέτοια μοντέλα έχουν αναπτυχθεί για την υλοποίηση ΝΔΑ, εκ των οποίοων τα πιο σημαντικά θα αναφερθούν στο κεφάλαιο αυτό. 

\section{Το μοντέλο \textlatin{Hodgkin - Huxley}}
Το 1952, οι \textlatin{Hodgkin} και \textlatin{Huxley} δημοσίευσαν 4 εργασίες σχετικά με τον τρόπο λειτουργίας των νευρώνων \cite{Johnson2017}. Πραγματοποίησαν πειράματα σε έναν γιγαντιαίο άξονα καλαμαριού και ανέπτυξαν το ακόλουθο μοντέλο. Μέσα από τις δοκιμές τους, διαπίστωσαν ότι η ιοντική κίνηση ιόντων Νατρίου (\textlatin{Na\textsuperscript{+}}), Καλίου (\textlatin{K\textsuperscript{+}}) και ένα ρεύμα, επονομαζόμενο ρεύμα διαρροής που αποτελείται κυρίως από ιόντα χλωρίου (\textlatin{Cl\textsuperscript{-}}) , που ελέγχονται από δύο κανάλια που εξαρτώνται από την τάση (κανάλια νατρίου και καλίου) είναι υπεύθυνα για την ροή ρεύματος του νευρώνα. Ενώ ο νευρώνας είναι σε ηρεμία, μέσα στον νευρώνα, μια υψηλή συγκέντρωση αρνητικά φορτισμένων ιόντων δημιουργεί μια διαφορά τάσης με το εξωτερικό του νευρώνα, το οποίο είναι θετικά φορτισμένο. Όταν η τάση φτάσει σε ένα ορισμένο όριο, οι αντλίες νατρίου και καλίου ανοίγουν, μετακινώντας ιόντα μέσα και έξω από το κύτταρο αντίστοιχα. Αυτή η ιοντική κίνηση οδηγεί σε ένα ρεύμα που κινείται ως ακίδα (αιχμή) μέσω του νευράξονα στον επόμενο νευρώνα.

Το μοντέλο \textlatin{Hodgkin-Huxley} παριστάνεται ως ηλεκτρικό κύκλωμα που φαίνεται στο σχήμα 3.1. Η τάση που συμβολίζεται ως \textlatin{V\textsubscript{m}} αντιπροσωπεύει την τάση στην κυτταρική μεμβράνη. Μπορούμε να δούμε ότι ένα ρεύμα εισόδου I\textsubscript{m} μπορεί να φορτίσει τον πυκνωτή \textlatin{C\textsubscript{m}} ή μπορεί να διαρρεύσει στα υπόλοιπα κανάλια. Το δυναμικό που προκαλείται από τα ιόντα είναι διαφορετικό για κάθε ένα από αυτά, έτσι χρησιμοποιείται μία διαφορετική μπαταρία για καθένα από τα ιόντα. Αξίζει να σημειωθεί ότι η μπαταρία νατρίου \textlatin{E\textsubscript{Na\textsuperscript{+}}} έχει αντίθετο προσανατολισμό σε σύγκριση με τις υπόλοιπες μπαταρίες ιόντων και αυτό δικαιολογείται αφού τα ιόντα νατρίου απομακρύνονται από τη μεμβράνη. Τα βέλη στους αντιστάτες του καναλιού νατρίου και καλίου υποδηλώνουν ότι οι αντιστάσεις αυτές έχουν μεταβαλόμενες τιμές και περιγράφουν τις αντλίες του βιολογικού μοντέλου.

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{Neurons-Synapses/HH model circuit.png}
    \caption{Η αντιστοιχία του μοντέλο \textlatin{Hodgkin-Huxley} σε ηλεκτρικό κύκλωμα.}
    \label{fig:neurons-multipolar}
\end{figure}

Προκειμένου να αναλυθεί το κύκλωμα, οι στατικές τιμές του καναλιού χλωρίου και του καναλιού διαρροής μπορούν να υπολογιστούν με τον νόμο του \textlatin{Kirchhoff} ως ένα κανάλι, που ονομάζεται κανάλι διαρροής, με σταθερή τιμή μπαταρίας και αντίσταση. Εφαρμόζοντας τον τρέχοντα νόμο του \textlatin{Kirchhoff} στο κύκλωμα παίρνουμε την ακόλουθη εξίσωση: \[I(t)=\sum_{k}I_k\]

Αντικαθιστώντας κάθε εξίσωση ρεύματος με το ισοδύναμο τάσης, προκύπτει μια διαφορική εξίσωση πρώτης τάξης λόγω του πυκνωτή \textlatin{C\textsubscript{m}}. Για κάθε κανάλι νατρίου, καλίου και διαρροής, πρέπει να υπολογισθεί η αντίστοιχη αγωγιμότητα. Οι \textlatin{Hodgkin} και \textlatin{Huxley} διαπίστωσαν ότι δύο τύποι αντλιών είναι υπεύθυνοι για την κίνηση ιόντων νατρίου και ένας τύπος αντλίας για το κάλιο. Για το λόγο αυτό, πρόσθεσαν τις παραμέτρους \textlatin{m, h} και \textlatin{n} για να περιγράψουν τον έλεγχο των αντλιών στην κίνηση των ιόντων. Μέσω αριθμητικών πειραμάτων κατέληξαν στο ακόλουθο αποτέλεσμα που περιγράφει το δυναμικό της κυτταρικής μεμβράνης.

\begin{equation}
C_m\frac{dV_m}{dt} = I_m - (g_{Na^+}m^3h(V_m+E_{Na^+}) + g_{K^+}n^4(V_m-E_{K^+}) + g_L(V_m-E_L))
\end{equation}

όπου οι τιμές \textlatin{m, h} και \textlatin{n} εξαρτώνται από την τάση και περιγράφονται από τις ακόλουθες διαφορικές εξισώσεις:

\begin{equation}
\frac{dm}{dt}=\alpha_m(V_m)(1-m)+\beta_m(V_m)m
\end{equation}
\begin{equation}
\frac{dh}{dt}=\alpha_h(V_m)(1-h)+\beta_h(V_m)h
\end{equation}
\begin{equation}
\frac{dn}{dt}=\alpha_n(V_m)(1-n)+\beta_n(V_m)n
\end{equation}

Τα \(\alpha_m (V_m)\), \(\alpha_h (V_m)\), \(\alpha_n (V_m)\), \(\beta_m (V_m)\), \(\beta_h (V_m)\), \(\beta_n (V_m)\) είναι συναρτήσεις που εξαρτώνται από την τάση και ορίζουν τη συμπεριφορά των μεταβλητών \textlatin{m, h} και \textlatin{n} και, συνολικά, το δυναμικό της κυτταρικής μεμβράνης. Λύνοντας κάθε εξίσωση, παίρνουμε μια εκθετική λύση με μια σταθερά εκθέτη \(\tau_m\), \(\tau_h\) και \(\tau_n\) αντίστοιχα. Στα σχήματα 4.2α και 4.2β φαίνεται η τάση της μεμβράνης που προκύπτει βάση του παραπάνω συνόλου εξισώσεων, και οι σταθερές χρόνου ως συνάρτηση του δυναμικού. Αυτή η διαφορά στις σταθερές χρόνου (εικόνα 4.2β) υποδηλώνει την ύπαρξη γρήγορων και αργών πύλων ιόντων.

\begin{figure}[htp]
    \centering
    \subfloat[Συνάρτηση της τάσης της μεμβράνης βάση του μοντέλου \textlatin{Hodgkin-Huxley}]
    {\includegraphics[width=5cm]{Neurons-Synapses/HodgkinHuxley_output.png}\label{fig:f1}}
    \hfill
    \subfloat[Οι σταθερές χρόνου συναρτήσει του δυναμικού του νευρώνα]
    {\includegraphics[width=5cm]{Neurons-Synapses/time-constants-Hodgkin-Huxley-model.png}\label{fig:f2}}
    \caption{fig:Εξαρτήσεις του μοντέλου \textlatin{Hodgkin-Huxley}}
\end{figure}

Δίνοντας συγκεκριμένες τιμές στις εξαρτώμενες από την τάση μεταβλητές \textlatin{n} και \textlatin{h}, μπορούμε να προσομοιώσουμε την τάση της κυτταρικής μεμβράνης και να μελετήσουμε τη συμπεριφορά της. Στην εργασία τους \cite{NelsonM} οι \textlatin{Nelson M.} και \textlatin{Rinzel J.} χρησιμοποίησαν το \textlatin{GENESIS tutorial squid} \cite{squid} για να δημιουργήσουν πολλές μορφές αιχμών. Το μοντέλο \textlatin{Hodgkin Huxley} αναπτύχθηκε από το 1952 για να προσαρμοστεί στα ευρήματα της συμπεριφοράς των νευρώνων και αυτό στο βιβλίο \cite{gerstner2014} παρουσιάζονται εκτεταμένες πληροφορίες σχετικά με το βιολογικό μοντέλο του νευρώνα και τη μαθηματική προσαρμογή του μοντέλου \textlatin{Hodgkin Huxley}.

Το μοντέλο των \textlatin{Hodgkin} και \textlatin{Huxley} πρωτοστάτησε στον τομέα της νευρωνικής δυναμικής από την άποψη της νευροεπιστήμης, αλλά ώθησε επίσης την έρευνα και την ανάπτυξη Νευρωνικών Δικτύων αιχμών. Από μηχανικής πλευράς, υπήρξαν πολλές επιτυχημένες προσπάθειες για την εφαρμογή ενός νευρώνα χρησιμοποιώντας το προαναφερθέν μοντέλο σε υλικό υπολογιστών όπως πεδία προγραμματιζόμενης συστοιχίας πύλης (\textlatin{FPGA}) όπως στο \cite{Levi2018}, το οποίο καταλήγει σε πιθανή επικοινωνία μεταξύ ηλεκτρικών σημάτων από έναν ζωντανό οργανισμό σε μια τεχνητή δομή και, με την ανάπτυξη ενός ΝΔΑ, σε συνεργασία μεταξύ τους. Έχουν επίσης αναπτυχθεί πολυεπίπεδα νευρικά δίκτυα αιχμών με το μοντέλο των νευρώνων \textlatin{Hodgkin-Huxley} και έχουν εκπαιδευτεί για εργασίες όπως η ανίχνευση άκρων και η ταξινόμηση προτύπων, όπως φαίνεται σε αυτήν \cite{Yedjour2017} και αυτήν \cite{pattern2016} την ερευνητική εργασία αντίστοιχα, με υποσχόμενα αποτελέσματα.

Παρά την καινοτόμο περιγραφή του νευρώνα, το μοντέλο έχει επικριθεί για την αποτελεσματικότητά του στην περιγραφή της πολύπλοκης συμπεριφοράς των πολλών διαφορετικών τύπων νευρώνων. Αυτές οι επικρίσεις προέρχονται από αδυναμίες του μοντέλου, όπως η αδυναμία του να εξηγήσει γεγονότα που μπορούν να επηρεάσουν την κατάσταση του νευρώνα \cite{limit1993}. Από μηχανική άποψη, το κύριο μειονέκτημά του για την ανάπτυξη πολύπλοκων ΝΔΑ είναι η υψηλή υπολογιστική πολυπλοκότητά του. Έχει αποδειχθεί \cite{reduction1997} ότι το μοντέλο \textlatin{Hodgkin-Huxley} μπορεί να περιγραφεί, με επαρκή ακρίβεια, ως μοντέλο κατωφλίου μονής μεταβλητής. Οι μηχανικοί έστρεψαν την προσοχή τους στην ανάπτυξη απλούστερων μοντέλων κατωφλίου ώστε να μπορέσουν να δημιουργήσουν πιο πολύπλοκες δομές που οδήγησαν στην ανάπτυξη μοντέλων νευρώνων όπως αυτά που ακολουθούν.

\medskip

\section{Μοντέλο Διαρρέον Ολοκλήρωσης και Πυροδότησης \textlatin{(Leaky Integrate-and-Fire): LIF}}

Το μοντέλο \textlatin{LIF} προτάθηκε για πρώτη φορά το 1907 \cite{Brunel2007} και παρουσιάζει τον νευρώνα ως διαρρέουσα ενοποιητική μονάδα. Το μοντέλο \textlatin{LIF}, σε αντίθεση με το μοντέλο \textlatin{Hodgkin-Huxley (ΗΗ)}, είναι ένα μοντέλο κατωφλίου, δηλαδή εκπέμπει μια ακίδα εξόδου όταν η τάση εισόδου φτάσει σε ένα προκαθορισμένο όριο. Το ισοδύναμο ηλεκτρικό κύκλωμα του μοντέλου είναι ένα κύκλωμα αντιστάτη-πυκνοτή (\textlatin{RC}) με ρεύμα εισόδου \textlatin{I\textsubscript{inject}}. Τα ρεύματα εισόδου είναι συνήθως αιχμές (σαν \(\delta\)-συναρτήσεις \textlatin{Dirac}) που, λόγω του πυκνωτή \textlatin{C}, αυξάνουν την τάση του συστήματος κατά την ίδια τιμή εισόδου. Ο αντιστάτης, ή όπως φαίνεται στο Σχήμα 3.3 ο αγωγός \textlatin{g\textsubscript{leak}}, είναι υπεύθυνος για τη διαρροή τάσης του νευρώνα, ενώ η πηγή \textlatin{E\textsubscript{m}} αντιπροσωπεύει την τάση του νευρώνα σε κατάσταση αδράνειας. Όταν η τάση φτάσει το κατώφλι, η μονάδα εκπέμπει μια ακίδα και η τάση επιστρέφει στην αρχική (ρελαντί) κατάσταση.

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{Neurons-Synapses/Simplied-circuit-of-the-LIF-neuron-model.png}
    \caption{Το Μοντέλο Διαρρέον Ολοκλήρωσης και Πυροδότησης σαν ηλεκτρικό κύκλωμα}
    \label{fig:lif-circuit}
\end{figure}

\vspace{5mm}

Επιλύοντας το κύκλωμα προκύπτει η ακόλουθη εξίσωση, η οποία περιγράφει το δυναμικό της μονάδας στο χρόνο. Η λύση της ομοιογενούς εξίσωσης \textlatin{(i\textsubscript{inject}=0)} είναι μια εκθετική μείωση. Η σταθερά \textlatin{\(\tau\textsubscript{m} = C/g\textsubscript{leak}\)} είναι η χρονική σταθερά της εκθετικής μείωσης και καθορίζεται από τους προαναφερθέντες παράγοντες, τον αγωγό και τον πυκνωτή του κυκλώματος. Το Σχήμα 3.4 δείχνει την απόκριση του μοντέλου σε μια είσοδο τρένου αιχμών αν το εχω με αιχμες ειναι λαθος νμζω , με ακιδες το ειχα βρει γοογλε.

\vspace{5mm}

\begin{equation}
\tau\textsubscript{m}\frac{dV\textsubscript{m}(t)}{dt}=-(V\textsubscript{m}-E\textsubscript{m})+\frac{i\textsubscript{inject}}{g\textsubscript{leak}}
\end{equation}

\vspace{5mm}

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Neurons-Synapses/The-illustration-of-Leaky-Integrate-and-Fire-LIF-neuron-dynamics-The-pre-spikes-are.png}
    \caption{Τάση του δυναμικού του νευρώνα και η απόκριση του. Κάθε σήμα εισόδου πολλαπλασιάζεται με το βάρος του και στη συνέχεια αθροίζεται για να δράσει στο δυναμικό του νευρώνα. Όταν το δυναμικό μεμβράνης του νευρώνα φτάσει στο κατώφλι, παράγεται μια ακίδα και η τάση επαναφέρεται στην κατάσταση αδράνειας.}
    \label{fig:lif-neuron}
\end{figure}

\medskip

Η απλότητα του μοντέλου έχει τραβήξει την προσοχή των ερευνητών τόσο για την υλοποίηση λογισμικού όσο και υλικού του νευρώνα. Το μοντέλο έχει χρησιμοποιηθεί για μεγάλη ποικιλία χρήσεων. Το 2003 το διαρρέον μοντέλο ολοκλήρωσης και πυροδότησης χρησιμοποιήθηκε για τη μοντελοποίηση του κοχλία και τον εντοπισμό χαρακτηριστικών ήχου \cite{sound2003}, ενώ πιο συνηθισμένες, για τα ΤΝΔ, εργασίες, όπως η αναγνώριση μοτίβου και η τμηματοποίηση εικόνας, έχουν αντιμετωπιστεί χρησιμοποιώντας είτε ένα \textlatin{LIF} μοντέλο νευρώνων (για αναγνώριση μοτίβων) \cite{pattern2007} ή ΝΔΑ που αποτελείται από τέτοια μοντέλα νευρώνων (τμηματοποίηση εικόνας) \cite{Chaturvedi2012}. Οι \textlatin{Doutsi E. et al.} στην εργασία τους \cite{Doutsi2021} χρησιμοποίησαν το μοντέλο \textlatin{LIF} για να μετατρέψουν το σήμα εισόδου σε τρένο αιχμών. Η εκτεταμένη έρευνα με χρήση αυτού του μοντέλο οδήγησε τους ερευνητές να δουν το ίδιο το μοντέλο ως αντικείμενο μελέτης και έχει γίνει πολλή δουλειά στη βελτίωση αυτού, όπως για παράδειγμα στην εργασία \cite{Mullowney2008}. Από την άποψη της υλοποίησης υλικού, ΝΔΑ με βάση το προαναφερθέν μοντέλο έχουν ήδη προταθεί ή αναπτυχθεί και εκπαιδευτεί για διάφορους σκοπούς, όπως φαίνεται στα \cite{Liu2019} και \cite{Chu2015}, αξιοποιώντας τα πλεονεκτήματα των ΝΔΑ. Η αυξανόμενη ερευνητική ανάπτυξη ηλεκτρονικών εξαρτημάτων, όπως τα \textlatin{memristors}, που ευνοούν την ανάπτυξη τέτοιων δικτύων \cite{Yang2020}, έχει δημιουργήσει μια έκρηξη στη δημιουργία νευρομορφικών, ακόμη και φωτονικών αρχιτεκτονικών (βλ. κεφ. 3) \cite{Nahmias2013}, που υπόσχονται χαμηλή κατανάλωση ενέργειας \cite{Liu2019}, \cite{Chatterjee2019} και αρχιτεκτονικές υψηλής συμπίεσης \cite{Rozenberg2019} για την ανάπτυξη ΝΔΑ με βάση το μοντέλο \textlatin{LIF}.

Το μειονέκτημα του μοντέλου προέρχεται από την απλότητά του. Έχει προταθεί ότι, σε σύγκριση με το μοντέλο \textlatin{HH}, το μοντέλο \textlatin{LIF} θα μπορούσε να είναι λιγότερο ανεκτικό στο θόρυβο ενώ μοιάζει λιγότερο με τον βιολογικό νευρώνα \cite{subthreshold2005}. Επιπλέον, σε σύγκριση με τα αποτελέσματα των κορυφαλιων ΤΝΔ και των αλγορίθμων μηχανικής μάθησης, τα ΝΔΑ δεν παρουσιάζουν πάντα ικανοποιητικά αποτελέσματα, όπως σε αυτή τη μελέτη \cite{SVM2014} όπου συγκρίνει τις αποδόσεις ενός ΝΔΑ με χρήση μοντέλων νευρώνων Διαρρέον Ολοκλήρωσης και Πυροδότησης και μιας μηχανής υποστίρηξης διανυσμάτων \textlatin{Support Vector Machine - SVM}.

\medskip

\section{Μοντέλο \textlatin{Izhikevich}}

Το 2003 ο \textlatin{Eugene M. Izhikevich} δημοσίευσε ένα άρθρο προτείνοντας ένα νέο μοντέλο νευρώνων \cite{Izhikevich2003}. Ο στόχος ήταν να παράσχει ένα μοντέλο με μαθηματική απλότητα και βιολογική αληθοφάνεια, συνδυάζοντας χαρακτηριστικά από το μοντέλο διαρρεόν ολοκλήρωσης και πυροδότησης και το μοντέλο \textlatin{Hodgkin-Huxley}. Το προκύπτον μοντέλο θα ήταν χρήσιμο για τη μοντελοποίηση βιολογικών νευρώνων ενώ θα μπορούσε εύκολα να εφαρμοστεί, από την άποψη της μηχανικής. Η μαθηματική περιγραφή παρουσιάζεται στις ακόλουθες εξισώσεις. Όπως το μοντέλο \textlatin{LIF}, έτσι και το προτεινόμενο μοντέλο είναι ένα μοντέλο κατωφλίου, όπου διαφορετικές τιμές των παραμέτρων θα μπορούσαν να εισαχθούν για να προσομοιώσουν μια ποικιλία νευρώνων.
\begin{equation}
    \frac{dv}{dt}(t) = 0.04v^2+5v+140-u+I(t)
\end{equation}
\begin{equation}
\frac{du}{dt}(t) = a(bv-u)
\end{equation}
\begin{equation}
if \:u\geq30mV \: then \: \left\{
\begin{array}{ll}
     v = c &  \\
     u = u+d & 
\end{array}
\right.
\end{equation}

Η τιμή \(v\) αντιπροσωπεύει το δυναμικό μεμβράνης του νευρώνα ενώ η τιμή \(u\) αντιπροσωπεύει μια μεταβλητή ανάκτησης μεμβράνης. Στο έργο του, ο \textlatin{Izhikevich} έδειξε ότι ο χειρισμός των παραμέτρων \textlatin{a, b, c} και \textlatin{d}, μπορεί να παράγει μια μεγάλη ποικιλία νευρωνικών αποκρίσεων. Αργότερα, το 2004 δήλωσε \cite{Izhikevich2004} ότι το μοντέλο \textlatin{Izhikevich} κατατάσσεται υψηλά στο συνδυασμό υψηλής βιολογικής αληθοφάνειας και χαμηλού κόστους εφαρμογής, μεταξύ των υπαρχόντων μοντέλων της εποχής του.

Πράγματι, την τελευταία δεκαετία, το μοντέλο \textlatin{Izhikevich} εφαρμόστηκε σε ολοκληρωμένα κυκλώματα αξιοποιώντας τις πρόσφατες εξελίξεις στον τομέα. Πολλαπλές τεχνικές που ανταγωνίζονται μεταξύ τους, όπως το \textlatin{MNIN} \cite{Haghiri2018} και το \textlatin{CORDIC} \cite{Elnabawy2018} υπόσχονται χαμηλότερο υπολογιστικό κόστος και σφάλμάτα. Το μοντέλο έχει δοκιμαστεί σε κλασικές εργασίες Τεχνητής Νοημοσύνης, είτε υλοποιημένες στο δικό του υλικό, όπως σε αυτό το έργο \cite{Rice2009}, όπου αναπτύχθηκε ένα \textlatin{FPGA} που αντιπροσωπεύει ένα μοντέλο  \textlatin{Izhikevich} νευρώνα, βάση του οποίου αναπτύχθηκε ένα νευρωνικό δίκτυο αιχμών και δοκιμάστηκε στην αναγνώριση χαρακτήρων, ή σε λογισμικό που προσομοιώνει την απόκριση του μοντέλου. Οι μεμονωμένοι νευρώνες έχουν δοκιμαστεί σε προβλήματα μη γραμμικής αναγνώρισης προτύπων \cite{Antonio2010} δείχνοντας την ικανότητα ενός μεμονωμένου νευρώνα στην ταξινόμηση πολλαπλών προτύπων. Μια έξυπνη εκμετάλλευση αυτής της ικανότητας παρουσιάζεται εδώ \cite{luna-a2019} όπου το πολυστρωματικό δίκτυο \textlatin{perceptron} που χρησιμοποιείται για την ταξινόμηση ενός τυπικού συνελικτικού νευρωνικού δικτύου (\textlatin{CNN}) αντικαθίσταται από έναν νευρώνα \textlatin{Izhikevich}. Ως αποτέλεσμα, αυτό το δίκτυο επιτυγχάνει παρόμοιες βαθμολογίες με τυπικά \textlatin{CNN}, μειώνοντας παράλληλα το χρόνο εκπαίδευσης.

Παρά τις πολλά υποσχόμενες δηλώσεις του δημιουργού του μοντέλου, τα τελευταία χρόνια, αυτό έχει επικριθεί. Οι ανασκοπήσεις και οι συγκρίσεις με άλλα μοντέλα (όπως το \textlatin{LIF} και το \textlatin{HH}) δείχνουν μικρά, αν υπάρχουν, πλεονεκτήματα. Το 2014, οι \textlatin{Michael J. Skocik} και \textlatin{Lyle N. Long} συνέκριναν το υπολογιστικό κόστος των μοντέλων \textlatin{Izhikevich, LIF} και \textlatin{Hodgkin-Huxley} χρησιμοποιώντας πολλαπλές αριθμητικές μεθόδους (έτσι ώστε να βρεθεί η καλύτερη εφαρμογή κάθε μοντέλου) \cite{Skocik2014}. Τα αποτελέσματά τους έδειξαν ότι το μοντέλο \textlatin{Izhikevich} είναι συγκρίσιμο με το μοντέλο \textlatin{HH} όσον αφορά το υπολογιστικό κόστος, ενώ το μοντέλο \textlatin{LIF} είναι, όπως αναμενόταν, το καλύτερο από τα τρία. Το 2017, οι \textlatin{Sergio Valadez-Godínez et al.} \cite{Godinez2017} συνέκριναν τα ίδια μοντέλα νευρώνων όσον αφορά την ακρίβεια και το κόστος σε διαφορετικούς ρυθμούς αιχμών. Ομοίως κατέληξαν, ότι στις περισσότερες περιπτώσεις, το μοντέλο \textlatin{Izhikevich} έδωσε άσχημα αποτελέσματα, ήταν λιγότερο αποδοτικό από το μοντέλο \textlatin{HH}, ενώ ήταν πιο υπολογιστικά ακριβό από το μοντέλο \textlatin{LIF}. Μια πιο πρόσφατη μελέτη του 2020 από τους ίδιους ερευνητές \cite{Valadez-Godinez2020} παρουσιάζει και συνοψίζει τα προβλήματα του μοντέλου \textlatin{Izhikevich} που προτάθηκαν στη βιβλιογραφία.

\medskip

\section{Μοντέλο Απόκρισης αιχμών - \textlatin{Spike Response Model: SRM}}

Το Μοντέλο Απόκρισης αιχμών αναπτύχθηκε, ως ιδέα, σε μια σειρά εγγράφων την τελευταία δεκαετία του 20ού αιώνα, αλλά το όνομα εισήχθη για πρώτη φορά σε αυτό το έργο \cite{Gerstner1993} του 1993. Σε αυτό το έγγραφο, οι \textlatin{Wulfram Gerstner et. al.} παρουσιάζουν το μαθηματικό μοντέλο του νευρώνα \textlatin{SRM} μαζί με αποτελέσματα προσομοίωσης. Το μοντέλο μοιάζει με το μοντέλο \textlatin{Integrate-and-Fire} (μοντέλο \textlatin{IF}), με τη διαφορά ότι το δυναμικό της μεμβράνης εξαρτάται από διάφορους γραμμικούς πυρήνες που δρουν στις εισερχόμενες αιχμές. Επιπλέον, το όριο σε αυτό το μοντέλο εξαρτάται από το χρόνο σε αντίθεση με το μοντέλο \textlatin{Integrate-and-Fire}. Σε αυτό το βιβλίο \cite{gerstner2014}, οι συγγραφείς καταδύονται σε μια εκτενή περιγραφή και εξήγηση του \textlatin{SRM} και της ομοιότητάς του με το μοντέλο \textlatin{IF}. Το ακόλουθο σύνολο εξισώσεων (3.9), (3.10) περιγράφει το δυναμικό της μεμβράνης και την τιμή κατωφλίου του μοντέλου, αντίστοιχα. Οι προαναφερθέντες πυρήνες \(\eta(), \kappa()\) και \(\theta_1()\) μπορούν να ερμηνευτούν ως φίλτρα γραμμικής απόκρισης που δρουν στις εισερχόμενες ή μετασυναπτικές αιχμές. Πιο αναλυτικά, ο πυρήνας \(\kappa\) είναι η γραμμική απόκριση της μεμβράνης για την εισερχόμενη αιχμή και ολοκληρώνει το ρεύμα εισόδου στο χρόνο. Ο πυρήνας \(\eta\) αντιστοιχεί στο δυναμικό δράσης του νευρώνα. Αυτό γίνεται καλύτερα κατανοητό από το γεγονός ότι ο πυρήνας επηρεάζει το δυναμικό της μεμβράνης και εξαρτάται από τη μεταβλητή \(t^f\), η οποία ορίζει τον χρόνο πυροδότησης του νευρώνα. Ο πυρήνας πρέπει να περιέχει ακόμη και την αρνητική υπέρβαση του δυναμικού του νευρώνα. Ο πυρήνας \(\theta_1\) περιγράφει τη βραχυπρόθεσμη πλαστικότητα του νευρώνα και αλλάζει την τιμή κατωφλίου του νευρώνα (στις περισσότερες περιπτώσεις) μετά την πυροδότηση του νευρώνα.

\begin{equation}
u(t) = \sum_{t^f}\eta(t-t^f)+\int_0^\infty\kappa(s)I(t-s)ds+u_rest
\end{equation}
\begin{equation}
\theta(t) = \theta_0 + \sum_{t^f}\theta_1(t-t^f)
\end{equation}

Το μοντέλο μπορεί επίσης να ερμηνευτεί ως σύστημα κλειστού βρόχου, όπου κάθε πυρήνας είναι ένα φίλτρο πεπερασμένης απόκρισης παλμού (φίλτρο \textlatin{FIR}), η είσοδος του συστήματος είναι το ρεύμα εισόδου του νευρώνα, η έξοδος είναι το τρένο αιχμής που παράγει ο νευρώνας λόγω της εισόδου και η ίδια η κατάσταση του συστήματος είναι το δυναμικό μεμβράνης. Η παρακάτω εικόνα από το βιβλίο \cite{gerstner2014} δίνει μια οπτική αναπαράσταση του συστήματος που αναφέρθηκε παραπάνω.

\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{Neurons-Synapses/SRM.jpg}
    \caption{Το διάγραμμα του Μοντέλου Απόκρισης αιχμών. Κάθε φίλτρο αντιστοιχεί σε έναν πυρήνα που περιγράφει μαθηματικά το μοντέλο. Το σύστημα αντιπροσωπεύει το δυναμικό της μεμβράνης και την έξοδο, δηλαδή το τρένο αιχμών του νευρώνα.}
    \label{fig:lif-neuron}
\end{figure}

Η αφηρημένη φύση του μοντέλου λόγω των μη αυστηρά καθορισμένων συναρτήσεων πυρήνα, δίνει τη δυνατότητα για την υλοποίηση μιας ποικιλίας τύπων νευρώνων και πιθανών αντιδράσεων και να ρυθμίσει λεπτομερώς τις μεταβλητές για να δημιουργήσει μοντέλα που μιμούνται τη λειτουργικότητα των πραγματικών νευρώνων. Το 2003 οι \textlatin{Reanut Jolivet} κ.ά. έδειξαν ότι το Μοντέλο Απόκρισης αιχμών μπορούσε να προβλέψει με μεγάλη ακρίβεια την απόκριση των πραγματικών νευρώνων, δεδομένης της ίδιας εισόδου \cite{Jolivet2003}. Βελτιώσεις του μοντέλου έχουν προταθεί όπως στο \cite{Bohte2012} που καθιστούν το μοντέλο πιο βιολογικά αληθοφανές.

Το μοντέλο απόκρισης αιχμών λέγεται ότι είναι μια γενίκευση του μοντέλου \textlatin{Integrate and Fire}. Τούτου λεχθέντος, μπορεί να ήταν αναμενόμενο ότι νευρωνικά δίκτυα βασισμένα σε αυτό το μοντέλο νευρώνων θα τραβούσαν την προσοχή των μηχανικών Τεχνητής Νοημοσύνης. Αν και ορισμένα έργα έχουν χρησιμοποιήσει τέτοια δίκτυα για την επίλυση προβλημάτων όπως τη Διάγνωση Καρκίνου του Μαστού, όπως φαίνεται στο \cite{Ourdighi2016}, με εντυπωσιακά αποτελέσματα, το \textlatin{SRM} δεν προσελκύει το ενδιαφέρον της κοινότητας των μηχανικών. Ως αποτέλεσμα, υπάρχουν μόνο μερικές υλοποιήσεις σε υπολογιστικό υλικό, όπως το \cite{Clayton2011}, ενώ το ίδιο το μοντέλο λειτουργεί ως εργαλείο κυρίως από βιολογική άποψη.

\section{Στοχαστικό και Πιθανολογικό Μοντέλο Νευρώνα}

Όλα τα μοντέλα νευρώνων που παρουσιάστηκαν ήταν ντετερμινιστικά συστήματα, που περιγράφονταν από εξισώσεις σταθερών παραμέτρων και ντετερμινιστικών εισόδων. Ως αποτέλεσμα, εξ ορισμού, γνωρίζοντας την αρχική κατάσταση και το σήμα εισόδου, μπορεί να συναχθεί η απόκριση του μοντέλου. Αυτό το γεγονός μπορεί να είναι χρήσιμο καθώς βοηθά στην κατανόηση σημαντικών χαρακτηριστικών όπως η σταθερότητα του μοντέλου. Ωστόσο, έχει προταθεί ότι η προσθήκη στοχαστικότητας σε τέτοια βιολογικά συστήματα μπορεί να είναι χρήσιμη, δεδομένου ότι δίνει τη δυνατότητα ανίχνευσης ασθενών σημάτων. Αυτή η ιδέα ονομάζεται στοχαστικός συντονισμός και παρουσιάστηκε το 1981. Ο στοχαστικός συντονισμός λέγεται ότι υπάρχει όταν υπάρχουν οι ακόλουθες τρεις συνθήκες στο σύστημα. Πρώτον, πρέπει να υπάρχει ένα φράγμα ενεργοποίησης, όπως η τιμή κατωφλίου που παρουσιάστηκε στα προηγούμενα μοντέλα. Επιπλέον, πρέπει να εφαρμόζονται αδύναμα και περιοδικά σήματα, όπως η αμαξοστοιχία εισόδου. Τέλος, πρέπει να εφαρμοστεί ένας θόρυβος εισόδου στο σήμα εισόδου. Ο στοχαστικός συντονισμός αναλύεται εκτενώς σε αυτό το έργο \cite{Gammaitoni1998}. Αυτό το φαινόμενο δεν είναι μόνο ένα χρήσιμο εργαλείο από πλευράς μηχανικής, αλλά παρατηρείται επίσης σε βιολογικά συστήματα \cite{Honggi2002}. Κατά συνέπεια, υπήρξε μια εκτεταμένη έρευνα για την σκόπιμη προσθήκη στοχαστικότητας (όπως με την εφαρμογή θορυβώδους εισόδου) ή πιθανότητας σε μοντέλα νευρώνων, έτσι ώστε να αξιοποιηθούν οι επιπτώσεις του στοχαστικού συντονισμού.

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Neurons-Synapses/Stochastic-resonance-occurs-when-an-optimal-level-of-noise-is-added-to-a-subthreshold.png}
    \caption{Στοχαστικός συντονισμός. Στο αδύναμο σήμα προστίθεται ένας θόρυβος εισόδου. Κατά συνέπεια, η νέα θορυβώδης είσοδος μπορεί να ξεπεράσει το όριο και να πραγματοποιηθεί η ζητούμενη ενεργοποίηση. Η ένταση του πρόσθετου θορύβου παίζει σημαντικό ρόλο στην έξοδο και πρέπει να χειριστεί προσεκτικά. Ένας θόρυβος χαμηλής έντασης δεν θα προκαλέσει την είσοδο να υπερβεί το όριο, ενώ μια υψηλή ένταση θα κυριαρχήσει στο σήμα εισόδου και οι πληροφορίες του σήματος θα χαθούν.}
    \label{fig:lif-neuron}
\end{figure}

Η στοχαστικότητα, όπως αναφέρθηκε προηγουμένως, εμφανίζεται στα βιολογικά μοντέλα. Η στοχαστικότητα του συστήματος μπορεί να προκληθεί από μια θορυβώδη είσοδο, όπως παρουσιάστηκε παραπάνω, με την ύπαρξη στοχαστικού συντονισμού σε ντετερμινιστικά μοντέλα, όπως στο \cite{Clayton2011}. Παρ 'όλα αυτά, το ίδιο το μοντέλο μπορεί να καθοδηγείται από στοχαστικές διαδικασίες και πιθανότητες ενσωματωμένες στις παραμέτρους του. Είναι γνωστό ότι το άνοιγμα και το κλείσιμο των αντλιών ιόντων στους νευρώνες καθοδηγείται από κάποια ποσότητα στοχαστικότητας που είναι ενσωματωμένη στο σύστημα. Για την προσομοίωση αυτής της συμπεριφοράς, έχουν προταθεί στοχαστικά μοντέλα του μοντέλου \textlatin{Hodgkin-Huxley} \cite{Fox1997} που αντιμετωπίζουν το άνοιγμα και το κλείσιμο των αντλιών ιόντων ως μη ντετερμινιστικά γεγονότα. Παρόλο που τέτοια μοντέλα μπορούν να είναι χρήσιμα από βιολογική άποψη, δεν εξυπηρετούν έναν αξιοσημείωτο σκοπό στην ανάπτυξη εκπαιδευτικών νευρωνικών δικτύων μεγάλης κλίμακας λόγω της πολυπλοκότητάς τους. Για τέτοια χρήση, έχουν αναπτυχθεί μη ντετερμινιστικά μοντέλα άλλων, απλών μοντέλων, όπως το μοντέλο \textlatin{LIF} και το μοντέλο απόκρισης αιχμών. Η μη-ντετερμινιστική συμπεριφορά συνήθως προστίθεται σε παραμέτρους που αντιστοιχούν στις τρεις φάσεις του νευρώνα: την προσυναπτική, τη μεμβράνη και τη μετασυναπτική. Στο έργο του \cite{Kasabov2010}, ο \textlatin{N.Kasabov} ορίζει ένα πιθανολογικό μοντέλο νευρώνων όπου καθένα από τα τρία προαναφερθέντα στάδια συμβάλλει με πιθανότητα για την άφιξη της αιχμής, τη συμβολή της αιχμής στη μεμβράνη των νευρώνων και τη δημιουργία της ακίδας. Το αναπτυγμένο δίκτυο με στοχαστικούς νευρώνες για εργασίες αναγνώρισης προτύπων δείχνουν χαρακτηριστικά που δεν έχουν τα ντετερμινιστικά ΝΔΑ. Ένα κοινό συμπέρασμα που τυπικά αναφέρεται είναι η στιβαρότητα του δικτύου σε σύγκριση με το ντετερμινιστικό, όπως αναφέρεται σε αυτή την εργασία \cite{Dhoble2011} όπου χρησιμοποιήθηκε το πιθανό μοντέλο νευρώνων \textlatin{LIF} και σε αυτό \cite{Sinyavskiy2010} όπου αυτή τη φορά χρησιμοποιήθηκε ένας στοχαστικός \textlatin{SRM} με πιθανότητα αιχμής. Σε αυτήν την εργασία \cite{Wu2012} πολλά μη-ντετερμινιστικά μοντέλα συγκρίνονται με τα αντίστοιχα ντετερμινιστικά, δείχνοντας στιβαρότητα και ρητή συμπεριφορά.

Μια άλλη προσέγγιση για την προσθήκη στοχαστικότητας στο μοντέλο είναι η έγχυση στοχαστικής διάχυσης. Το πιο κοινό μοντέλο που χρησιμοποιεί τη διαδικασία διάχυσης ως μοντέλο στοχαστικού νευρώνα είναι το μοντέλο \textlatin{Ornstein-Uhlenbeck} \cite{Lansky1995}. Ένα άλλο μοντέλο που χρησιμοποιεί την ίδια ιδέα είναι το μοντέλο \textlatin{Feller}. Ως αποτέλεσμα της μοναδικής και περίπλοκης φύσης τους, αυτοί οι τύποι μοντέλων έχουν προσελκύσει το ενδιαφέρον και η εκτίμηση των παραμέτρων έχει μελετηθεί εκτενώς σε αυτά, όπως παρουσιάζεται στο \cite{Ditlevsen2006} και στο \cite{Lansky2008}. Ωστόσο, πρέπει να αναφερθεί ότι μόνο η επιφάνεια έχει μελετηθεί από την οικογένεια μοντέλων στοχαστικών νευρώνων. Έχει αναπτυχθεί μια μεγάλη ποικιλία μοντέλων νευρώνων που εκμεταλλεύονται τη στοχαστικότητα με διαφορετικούς τρόπους και την κάνουν να δρα σε διαφορετικές πτυχές του μοντέλου. Δύο ακόμη αξιοσημείωτα τέτοια μοντέλα παρουσιάζονται εδώ, το μοντέλο \textlatin{Galves – Löcherbach} \cite{Galves2013} και το στοχαστικό μοντέλο \textlatin{Fitzhugh-Nagumo} \cite{Tuckwell1998}, που αφήνονται στον αναγνώστη να τα μελετήσει εκτενώς.

Όπως παρουσιάστηκε παραπάνω, η στοχαστικότητα στα νευρωνικά δίκτυα θα μπορούσε να είναι ένα χρήσιμο εργαλείο για την εξερεύνηση των απαρατήρητων συμπεριφορών των συστημάτων, αλλά θα μπορούσε κανείς να υποστηρίξει ότι η κωδικοποίηση και η εφαρμογή τέτοιων συστημάτων στο υλικό είναι αναποτελεσματική και πιθανώς υπολογιστικά δαπανηρή. Ωστόσο, ο στοχαστικός συντονισμός έχει να κάνει με την προσθήκη θορύβου στην είσοδο. Κατά συνέπεια, με προσεκτικό χειρισμό των ηλεκτρονικών εξαρτημάτων του υλικού, ο φυσικός θόρυβος των υλικών μπορεί να προστεθεί στην είσοδο και να δώσει την επιθυμητή θορυβώδη συμπεριφορά. Τέτοιες συσκευές, όπως οι στοχαστικοί \textlatin{memristors}, έχουν δοκιμαστεί ως προς την ικανότητά τους να ελέγχουν το θόρυβο της συσκευής και να δημιουργούν πλήρως στοχαστικές συσκευές μεταγωγής \cite{Gaba2013}, και τέτοιες συσκευές έχουν χρησιμοποιηθεί στην ανάπτυξη εξαρτημάτων υλικού με ενσωματωμένη στοχαστικότητα για την αύξηση των νευρωνικών δικτύων \cite{Maruan2015}. Επιπλέον, τα \textlatin{memristors} δεν αποτελούν μονόδρομο για την ανάπτυξη στοχαστικών ΝΔΑ. Διαφορετικά στοιχεία, όπως η δίοδος χιονοστιβάδας (\textlatin{avalanche diode}) \cite{Clayton2011}, μπορούν να χειριστούν για να παράγουν την επιθυμητή θορυβώδη είσοδο ή ακόμα και για να εκμεταλλευτούν την έξυπνη αρχιτεκτονική σε \textlatin{FPGA} και να αναπτύξουν στοχαστικά ΝΔΑ με πλήρως ντετερμινιστικά στοιχεία, όπου η στοχαστικότητα προστίθεται ψηφιακά \cite{Josep2012}.

Ένα θέμα που δείχνουν να υπερτερούν τα στοχαστικά ΝΔΑ (και γενικά τα ΤΝΔ) είναι η εύρεση γρήγορων κατά προσέγγιση λύσεων σε προβλήματα κλάσης \textlatin{NP-complete}. Τα βιολογικά συστήματα δείχνουν υψηλή ικανότητα στην εύρεση αρκετά καλών λύσεων σε προβλήματα ικανοποίησης περιορισμών. Γνωρίζοντας αυτό, έχουν αναπτυχθεί στοχαστικά ΝΔΑ για να μιμηθούν μια τέτοια συμπεριφορά με ικανοποιητικά αποτελέσματα \cite{Fonseca2017}. Μια άλλη πολλά υποσχόμενη χρήση στοχαστικών νευρωνικών δικτύων είναι η χρήση τους ως υπολογιστή δεξαμενής ή, στην περίπτωση νευρωνικών δικτύων, μηχανών υγρής κατάστασης - \textlatin{Liquid State Machine (LMS)}. Οι μηχανές υγρής κατάστασης (\textlatin{LSM}) αποτελούνται από μια μεγάλη συλλογή νευρώνων που συνδέονται τυχαία μεταξύ τους και ο καθένας λαμβάνει αιχμές εισόδου με την πάροδο του χρόνου από μια εξωτερική πηγή και άλλους νευρώνες του \textlatin{LSM}. Λόγω της αρχιτεκτονικής τους, τα \textlatin{LSM} έχουν υψηλές δυνατότητες υπολογισμού μιας μεγάλης ποικιλίας μη γραμμικών συναρτήσεων και η πρόσθετη στοχαστικότητα μπορεί να βελτιώσει ακόμη περισσότερο το πλαίσιο. Τέτοια στοχαστικά \textlatin{LSM (sLSM)} έχουν ήδη δοκιμαστεί έναντι του ντετερμινιστικού τους \textlatin{LSM} \cite{IEEE2011} και έχει συναχθεί ότι μπορούν να αποδώσουν καλύτερα από τα ντετερμινιστικά. Αυτή η τεχνική έχει ήδη χρησιμοποιηθεί με επιτυχία για την ανάπτυξη ενός ΝΔΑ για ταξινόμηση ηλεκτροκεφαλογραφημάτων (\textlatin{EEG}) \cite{Nuntalid2011}.

\bigskip
\chapter{Διαχείριση Πληροφοριών αιχμών}

Τα περισσότερα σύνολα δεδομένων για εκπαίδευση και δοκιμή ΤΝΔ που υπάρχουν σήμερα, αποτελούνται από μη χρονικά δεδομένα. Τέτοια σύνολα δεδομένων δεν μπορούν να δοθούν ως είσοδος σε ένα ασύγχρονο μοντέλο όπως ένα ΝΔΑ. Για να ξεπεραστεί αυτό το πρόβλημα, έχουν αναπτυχθεί πολλοί αλγόριθμοι κωδικοποίησης εισόδου, μερικοί από τους οποίους θα παρουσιαστούν σε αυτό το κεφάλαιο.

\section{Αναπαράσταση Πληροφοριών}

Η προστιθέμενη χρονική διάσταση που είναι ενσωματωμένη στα ΝΔΑ, δημιουργεί την ανάγκη κωδικοποίησης των δεδομένων εισόδου με τρόπους που μπορεί να επεξεργαστεί το Δίκτυο. Υπάρχουν δύο κύρια σχήματα κωδικοποίησης που αντιπροσωπεύουν τις δεδομένες πληροφορίες σε τρένα αιχμών, κωδικοποίηση ρυθμού και παλμική ή χρονική κωδικοποίηση. Η επιλογή της κωδικοποίησης δεδομένων αποτελεί σημαντική απόφαση στο σχεδιασμό του Νευρωνικού Μοντέλου, καθώς δίνει τη δυνατότητα ανάπτυξης μοντέλων με διαφορετικούς κανόνες μάθησης, συμβατά με το σύστημα κωδικοποίησης που επιλέχθηκε και διαμόρφωση μοντέλων σε ένα ευρύ φάσμα βιολογικής αληθοφάνειας και μηχανική υπολογιστικότητα.

\subsection{Κωδικοποίηση ρυθμού - \textlatin{Rate Code}}

Η κωδικοποίηση ρυθμού είναι το σχήμα κωδικοποίησης όπου οι πληροφορίες κωδικοποιούνται στον αριθμό των αιχμών που εκπέμπονται σε ένα χρονικό παράθυρο. Ο ρυθμός πυροδότησης ενός νευρώνα υπολογίζεται ως ο αριθμός των αιχμών που παράγει ο νευρώνας μέσα σε ένα δεδομένο χρονικό παράθυρο. Υπάρχουν πολλοί αλγόριθμοι κωδικοποίησης που βασίζονται στο σχήμα κωδικοποίησης ρυθμών που μετρούν τον ρυθμό πυροδότησης μεμονωμένων νευρώνων ή πληθυσμών νευρώνων κατά τη διάρκεια ενός ή πολλών χρονικών παραθύρων. Ωστόσο, ο υπολογισμός του μέσου ρυθμού πυροδότησης ενός πληθυσμού νευρώνων με παρόμοιες ιδιότητες σε ένα χρονικό διάστημα μπορεί να μην είναι πολύ χρήσιμος στην αναπαράσταση ή εξαγωγή πληροφοριών. Αυτή η μέθοδος αναπαράστασης πληροφοριών χρησιμοποιείται ευρέως λόγω της απλότητας και της χαμηλής πολυπλοκότητάς της. Ωστόσο, εστιάζοντας στον αριθμό των αιχμών σε ένα χρονικό παράθυρο και όχι στον ακριβή χρόνο που εμφανίζεται κάθε ακίδα, οι πληροφορίες που κωδικοποιούνται στον ακριβή χρόνο της αιχμής έχουν μικρή συμβολή ή ακόμη και χάνονται. Επιπλέον, ενώ υπάρχουν ορισμένες περιπτώσεις όπου ο ρυθμός πυροδότησης των νευρώνων αντιπροσωπεύει πραγματικά πληροφορίες στα βιολογικά συστήματα, όπως φαίνεται εδώ \cite{Huxter2003}, είναι κοινά αποδεκτό ότι η κωδικοποίηση ρυθμού δείχνει μικρή βιολογική αληθοφάνεια. Σε αυτήν την εργασία \cite{Richmond1987}, οι \textlatin{Richmond} και \textlatin{Optican} έδειξαν ότι τα αρχικά στρώματα νευρώνων μπορεί να συσχετίζονται σε μεγάλο βαθμό με τον ρυθμό πυροδότησης, αλλά για "βαθύτερα" στρώματα, υπήρχε μικρή συσχέτιση μεταξύ του αριθμού αιχμών και της αναπαράστασης πληροφοριών.

\subsection{Χρονική Κωδικοποίηση - \textlatin{Temporal Code}}

Η χρονική κωδικοποίηση βασίζεται στην ιδέα ότι οι πληροφορίες κωδικοποιούνται στον ακριβή χρόνο της αιχμής που εκπέμπεται ή στη χρονική διαφορά μεταξύ των αιχμών. Η χρονική κωδικοποίηση φαίνεται να περιέχει περισσότερες πληροφορίες, καθώς ο χρόνος της αιχμής μεταφέρει πληροφορίες από μόνη της, αυξάνοντας την αποδοτικότητα των νευρωνικών συνδέσεων \cite{Mainen2009}. Η πυκνότητα πληροφοριών που περιέχεται σε συστήματα χρονικής κωδικοποίησης έχει επανειλημμένα αποδειχθεί ανώτερη από τα συστήματα κωδικοποίησης ρυθμού, με ένα αξιοσημείωτο αποτέλεσμα να δηλώνεται σε αυτήν την εργασία \cite{IEEE2018}, όπου η χρήση της χρονικής κωδικοποίησης στη χαρτογράφηση ενός τυπικού ΤΝΔ σε ένα ΝΔΑ χρησιμοποιεί έως και 10 φορές λιγότερες λειτουργίες σε σύγκριση με την κωδικοποίηση ρυθμού. Η χρονική κωδικοποίηση παρέχει ένα άλλο πολύ σημαντικό χαρακτηριστικό του οποίου η πιθανή απουσία της καθιστά τη μελέτη των αλγορίθμων εκμάθησης στα Νευρωνικά Δίκτυα αιχμών δύσκολη, την  διαφοροποιησιμότητα. Σε αυτήν την έρευνα \cite{Mostafa2018}, οι συγγραφείς δείχνουν ότι η σχέση εισόδου-εξόδου του ΝΔΑ με το χρονικό σχήμα κωδικοποίησης είναι διαφοροποιήσιμη σχεδόν παντού. Χρησιμοποιώντας αυτήν την ιδιότητα, παραλλαγές του \textlatin{back-propagation} είναι διαθέσιμες ως κανόνες εκμάθησης, όπως το \textlatin{SpikeProp} \cite{Bohte2002} (βλ. Επίσης Κεφάλαιο 4.3.1). Επιπλέον, όπως αποδεικνύεται \cita{Lobov2020}, η χρονική κωδικοποίηση επιτρέπει την Χεμπιανή εκμάθηση μέσω της πλαστικότητας που εξαρτάται από τον χρόνο (\textlatin{STDP}) (βλέπε Κεφάλαιο 4.3.2), ενώ η κωδικοποίηση ρυθμών απαιτεί τη χρήση της συναρτήσεων λησμόνησης (\textlatin{forgetting functions}).

\medskip
Παρά τις διαφορές των προαναφερθέντων συστημάτων κωδικοποίησης, το ένα δεν έχει επικρατήσει ακόμη του άλλου, καθώς οι μηχανικοί εκμεταλλεύονται τα πλεονεκτήματα του καθενός εστιάζοντας είτε στην υπολογιστική αποδοτικότητα είτε στη βιολογική αληθοφάνεια. Αυτό οδήγησε το ερευνητικό πεδίο να επικεντρωθεί στην ανάπτυξη καθολικών ΝΔΑ που είναι σε θέση να εξάγουν πληροφορίες και να εκπαιδεύονται είτε με κωδικοποίηση ρυθμού είτε με χρονική κωδικοποίηση, αναπτύσσοντας αλγόριθμους που είναι συμβατοί και με τους δύο τύπους κωδικοποιήσεων, χειρίζοντας καθέναν από τους δύο τύπους σχήματος, όπως φαίνεται προηγουμένως στο \cite{Lobov2020} όπου αναπτύχθηκε μια \textlatin{forgetting function} για τη διαχείριση κωδικοποιημένων πληροφοριών και την ανάπτυξη ενός καθολικού νευρωνικού δικτύου με δυνατότητα μάθησης και με τους δύο τύπους κωδικοποιημένης εισόδου, εφαρμόζοντας μικρές αλλαγές στους αλγόριθμους εκμάθησης και προσαρμόζοντάς τους σε κάθε εκπαιδευτικό σχήμα \cite{Yin201} ή ακόμη και ανάμιξη των δύο σχημάτων σε νέες ελπιδοφόρες προσεγγίσεις, όπως φαίνεται σε αυτό το έργο \cite{Kiselev2016}, όπου ο συγγραφέας προτείνει ένα τέτοιο καινοτόμο σχήμα κωδικοποίησης με υποσχόμενα αποτελέσματα, παρά το γεγονός ότι βρίσκεται σε αρχικά στάδια ανάπτυξης.

\section{Κωδικοποίηση και Αποκωδικοποίηση αιχμών}

Τα τελευταία χρόνια, έχει συλλεχθεί μεγάλος όγκος δεδομένων και χρησιμοποιείται στον τομέα της Τεχνητής Νοημοσύνης. Αυτά τα δεδομένα αποτελούνται κυρίως από συνεχείς ή διακριτές μη χρονικές τιμές. Ως αποτέλεσμα, τέτοια σύνολα δεδομένων δεν μπορούν να χρησιμοποιηθούν ως είσοδος για Νευρωνικά Δίκτυα Αιχμών χωρίς κωδικοποίηση. Αντίστοιχα προβλήματα προκύπτουν ακόμη και με χρονικά δεδομένα όπως ο ήχος, όπου ένα αναλογικό σήμα είναι συνεχές στο χρόνο. Οι ακόλουθοι αλγόριθμοι κωδικοποίησης έχουν αναπτυχθεί για την αντιμετώπιση αυτών των προβλημάτων και τη μετατροπή τέτοιων σημάτων σε τρένα αιχμών που θα χρησιμοποιηθούν ως δεδομένα εισόδου για νευρωνικά δίκτυα αιχμών.

\subsection{Γεννήτρια Αιχμών \textlatin{Poisson}}

Η Γεννήτρια Αιχμών \textlatin{Poisson} είναι ένα ευρέως χρησιμοποιούμενο σχήμα κωδικοποίησης τιμών για τη μετατροπή αναλογικών τιμών σε τρένα αιχμών σε ένα δεδομένο χρονικό παράθυρο. Η τιμή/ένταση του σήματος υποδηλώνει τη "συχνότητα" ή τον κανονικοποιημένο αριθμό αιχμών στο δεδομένο χρονικό παράθυρο. Παρ 'όλα αυτά, αντί να καθορίζει μια συχνότητα των αιχμών στο τρένο αιχμών, δημιουργώντας μια αιχμή κάθε \(\tau\) δευτερόλεπτα, χρησιμοποιεί μια ομοιογενή διαδικασία \textlatin{poisson} \cite{Heeger2000}. Σε αυτό το σχήμα, η κανονικοποιημένη αναλογική τιμή αντιπροσωπεύει την πιθανότητα \(r\). Η κύρια ιδέα της Γεννήτρια Αιχμών \textlatin{Poisson} είναι ότι η πιθανότητα να δημιουργηθεί μια αιχμή κατά τη διάρκεια ενός χρονικού διαστήματος \(dt\) είναι περίπου \(rdt\). Μπορούμε επίσης να υπολογίσουμε την πιθανότητα η επόμενη αιχμή να συμβεί πριν από χρόνο \(\tau\) είναι \(1-e^{-r\tau}\)

\begin{equation}
    P(1\:spike\:during\:dt) \approx rdt \\
\end{equation}
\begin{equation}
    P(next\:spike\:occurs\:before\:\tau) = 1-e^{-r\tau}
\end{equation}

\medskip

Ο μέσος αριθμός αιχμών που εκπέμπονται στο διάστημα \((t1, t2)\) υπολογίζεται στην εξίσωση (4.13) και η πιθανότητα να έχουμε n αιχμές κατά το διάστημα \((t1, t2)\) υπολογίζεται στην εξίσωση (4.14). Το αποτέλεσμα που προκύπτει είναι μια κατανομή \textlatin{poisson}, η οποία δίνει το όνομά της στον αλγόριθμο κωδικοποίησης. Αυτό το αποτέλεσμα εγγυάται ότι, ενώ ο ακριβής χρόνος των αιχμών δεν θα είναι γνωστός ή ακόμη δεν θα είναι ο ίδιος για τις ίδιες τιμές εισόδου, ο αριθμός των αιχμών θα είναι σχεδόν ανάλογος με την κανονικοποιημένη πιθανότητα της αναλογικής εισόδου. Αυτό σημαίνει ότι μια υψηλότερη τιμή εισόδου θα παράγει περισσότερες αιχμές από μια χαμηλότερη σε ένα δεδομένο χρονικό παράθυρο και η αναλογία αυτών των τιμών θα είναι κοντά στην αναλογία του αριθμού αιχμών τους στο τρένο αιχμών.

\begin{equation}
    \langle n \rangle = \int_{t1}^{t2}rdt = r\Delta t
\end{equation}
\begin{equation}
    P(n\:spikes\:during\:\Delta t) = e^{-r\Delta t}\frac{(r\Delta t)^n}{n!}
\end{equation}

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{Encoding Schemes/Spike-trains-generated-by-Poisson-distribution.png}
    \caption{Πολλαπλά τρένα αιχμών παραγόμενα με την ίδια πιθανότητα \(r\) στο ίδιο χρονικό παράθυρο.}
    \label{fig:spikeprop-net-architecture}
\end{figure}

Αυτός ο αλγόριθμος κωδικοποίησης είναι αποδεδειγμένα χρήσιμος καθώς κωδικοποιεί τις πληροφορίες στον ρυθμό πυροδότησης στο παράθυρο του χρόνου, αλλά προσθέτει επίσης ένα θορυβώδες αποτέλεσμα, την τυχαιότητα του ακριβούς χρονισμού της αιχμής (εικ. 4.7). Με αυτόν τον τρόπο, προσθέτει ευρωστία στο δίκτυο, αφού αυτό μαθαίνει να εξάγει τις πληροφορίες παρά τις πιθανοτικές αιχμές και δείχνει επίσης αυξημένη βιολογική αληθοφάνεια, καθώς έχει διαπιστωθεί ότι η θορυβώδης κωδικοποίηση εισόδου χρησιμοποιείται σε βιολογικά συστήματα και στον εγκέφαλο, όπως παρουσιάστηκε και για τα αρχικά κυρίως στρώματα νευρώνων του εγκεφάλου στην εργασία \cite{Richmond1987}.

\subsection{Κωδικοποίηση Σειράς Κατάταξης - \textlatin{Rank Order Coding (ROC)}}

Η Κωδικοποίηση Σειράς Κατάταξης είναι ένας αλγόριθμος χρονικής κωδικοποίησης στον οποίο, οι πληροφορίες σήματος περιέχονται στη σειρά των νευρώνων πυροδότησης \cite{Thorpe1998}. Δεδομένου ενός συνόλου πέντε νευρώνων Α, Β, Γ, Δ και Ε, η σειρά που πυροδοτούν μεταφέρει την πληροφορία όπου η πρώτη αιχμή που εκπέμπεται θεωρείται ως η πιο σημαντική (βλ. Σχήμα 4.8). Ένα κωδικοποιημένο ερέθισμα θα μπορούσε να μοιάζει με αυτό B> A> D> E> C που υποδηλώνει ότι ο νευρώνας Β πυροδότησε πρώτος, ο νευρώνας Α δεύτερος κ.λπ. ενώ ο Β φέρει τις περισσότερες πληροφορίες που ακολουθούνται από τον Α και ούτω καθεξής. Είναι ένας εύκολα εφαρμόσιμος αλγόριθμος που δίνει τη δυνατότητα στους Ν νευρώνες να κωδικοποιούν το Ν! διαφορετικές τιμές (υπάρχουν N παραγοντικό διαφορετικές διευθετήσεις των N αντικειμένων) και ως εκ τούτου κωδικοποιούν \(log_2(N!)\) \textlatin{bits} πληροφορίας στο δεδομένο χρονικό παράθυρο, υποθέτοντας ότι κάθε νευρώνας μπορεί να εκπέμψει αιχμή μόνο μία φορά, μια δραματική αύξηση της χωρητικότητας πληροφορίας σε σύγκριση με τα \(log_2(N)\) \textlatin{bits} που παρέχουν λοιποί αλγόριθμοι κωδικοποίησης ρυθμού.

\begin{figure}[htp]
    \centering
    \includegraphics[width=6cm]{Encoding Schemes/Rank-Order-Coding-principle.png}
    \caption{Αναλογικό σήμα κωδικοποιημένο με \textlatin{ROC}. Το σήμα δειγματοληπτείται 9 φορές στο δεδομένο χρονικό παράθυρο που αντιστοιχεί σε 9 νευρώνες εισόδου. Τα δείγματα με τις υψηλότερες τιμές ταξινομούνται πρώτα, θεωρώντας ότι φέρουν τις περισσότερες πληροφορίες.}
    \label{fig:spikeprop-net-architecture}
\end{figure}

Επιπλέον, ένα σημαντικό χαρακτηριστικό που προσφέρει η \textlatin{ROC} είναι η αμεταβλητότητα. Οι αλλαγές στην ένταση του σήματος ή η αντίθεσή του δεν θα επηρεάσουν το αποτέλεσμα του κωδικοποιημένου τρένου αιχμών. Αυτό είναι ένα αξιοσημείωτο πλεονέκτημα έναντι των τυπικών συστημάτων κωδικοποίησης ρυθμού, τα οποία μπορεί να αλλοιώσουν τις πληροφορίες του σήματος κατά τη διάρκεια της κωδικοποίησης. Η ιδέα της αποκωδικοποίησης σειράς κατάταξης ακολουθεί το μοντέλο νευρώνα του \textlatin{Thrope} όπου η σειρά άφιξης της αιχμής της εισόδου επηρεάζει την αύξηση του δυναμικού της μεμβράνης.

\[V_{in} = \sum_{j\in input}mod^{order(a_j)}\pmb{w}_{j,in}\] 

Στην παραπάνω εξίσωση, το \(a_j\) δηλώνει τη σειρά που ο νευρώνας \textlatin{j} πυροδότησε και το \textlatin{mod} είναι ένας επιλεγμένος αριθμός μικρότερος από 1 που υψώνεται στο \(a_j\). Με αυτόν τον τρόπο, όσο υψηλότερη είναι η τάση (το \textlatin{j} πυροδοτείται αργότερα) τόσο λιγότερο επηρεάζει το δυναμικό της μεμβράνης του νευρώνα. Η κωδικοποίηση σειράς κατάταξης έχει δείξει πολλά υποσχόμενα αποτελέσματα σε πειράματα όπου χρησιμοποιήθηκε για την κωδικοποίηση είτε οπτικών \cite{Delorme2001} είτε ηχητικών σημάτων \cite{Loiselle2006}, αποδεικνύοντας την ικανότητά της να κωδικοποιεί τόσο χρονικά όσο και μη χρονικά δεδομένα, όπως ομιλία και εικόνες αντίστοιχα, σε χρονικά κωδικοποιημένα τρένα αιχμών.

\subsection{Κωδικοποίηση Τάξης Πληθυσμού - \textlatin{Population Order Coding (POC)}}

Η κωδικοποίηση τάξης πληθυσμού ακολουθεί την ίδια αρχή με την κωδικοποίηση σειράς κατάταξης (\textlatin{ROC}), όπου οι πληροφορίες αποθηκεύονται με τη σειρά που οι νευρώνες πυροδοτούν, αλλά κάθε τιμή εισόδου κωδικοποιείται από έναν πληθυσμό νευρώνων και η κατάταξή τους μεταφέρει τις πληροφορίες του σήματος. Στο \textlatin{POC}, η σειρά του πληθυσμού των νευρώνων κωδικοποιεί πληροφορίες σχετικά με μια τιμή εισόδου σε αντίθεση με το \textlatin{ROC}, όπου η σειρά των αιχμών κωδικοποιούσε πληροφορίες σχετικά με το σήμα εισόδου σε σχέση μεταξύ τους. Το \textlatin{POC} δίνει τη δυνατότητα κωδικοποίησης πρόσθετων πληροφοριών, όπως το μέγεθος του σήματος εισόδου. Πρέπει να σημειωθεί ότι, ενώ οι νευρώνες εισόδου \textlatin{ROC} αντιστοιχούν σε ένα σήμα εισόδου ο καθένας, ο πληθυσμός εισόδου του \textlatin{POC} κωδικοποιεί ένα μόνο σήμα εισόδου. Η κατάταξη στον \textlatin{POC} γίνεται με την ακόλουθη αρχή. Πολλαπλά αλληλεπικαλυπτόμενα δεκτικά πεδία (που εκφράζονται με Γκαουσσιανές συναρτήσεις - \textlatin{Gaussians}), που το καθένα αντιστοιχεί σε έναν νευρώνα εισόδου, καλύπτουν ολόκληρο το φάσμα των πιθανών τιμών σήματος εισόδου. Αυτή η τιμή προκαλεί την πυροδότηση των νευρώνων εισόδου, με τον χρόνο πυροδότησης τους να είναι ανάλογος με την τιμή του αντίστοιχου δεκτικού πεδίου τους για τη δεδομένη είσοδο \cite{Kasabov2018}. Το Σχήμα 4.9 δείχνει το τρένο αιχμής που κωδικοποιεί την τιμή εισόδου που εμφανίζεται στην μπλε γραμμή. Η σειρά των αιχμών που εκπέμπονται συσχετίζεται με τη συμμετοχή του σήματος εισόδου σε κάθε δεκτικό πεδίο.

\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{Encoding Schemes/Population-Order-Coding-POC-Schliebs-2009a.png}
    \caption{Τιμή εισόδου κωδικοποιημένη με Κωδικοποίηση Τάξης Πληθυσμού από πληθυσμό 5 νευρώνων με γκαουσσιανά δεκτικά πεδία. Ο χρόνος πυροδότησης κάθε νευρώνα φαίνεται στο δεύτερο διάγραμμα όπου φαίνεται ότι ο χρόνος εκπομπής αιχμών είναι ανάλογος με την τιμή της γκαουσσιανής του κάθε πεδίου.}
    \label{fig:spikeprop-net-architecture}
\end{figure}

Έχει αποδειχθεί ότι βιολογικά συστήματα, όπως συστάδες νευρώνων στον εγκέφαλο των θηλαστικών, κωδικοποιούν πληροφορίες χρησιμοποιώντας πληθυσμούς νευρώνων. Πολλές ερευνητικές εργασίες έχουν δείξει ότι ο \textlatin{POC} χρησιμοποιείται σε αρουραίους (στον σωματοαισθητικό φλοιό των αρουραίων \cite{Petersen2001}), στον ανθρώπινο εγκέφαλο (στο ακουστικό σύστημα \cite{Onken2014}, όπου οι συγγραφείς μελέτησαν το \textlatin{POC} από τα πειραματικά τους αποτελέσματα) και γενικά στην κωδικοποίηση ήχου \cite{Dean2005}, ενώ έχει χρησιμοποιηθεί επίσης σε τεχνητά ΝΔΑ, όπως για την ανάπτυξη εξελισσόμενων ΝΔΑ \cite{Soltic2010} και για χρονική ταξινόμηση \cite{Pan2019}.

\subsection{Κωδικοποίηση βάση κατωφλίου (ή Χρονική Αντίθεση) - \textlatin{Threshold-based encoding (or Temporal Contrast)}}

Μια απλή μέθοδος χρονικής κωδικοποίησης είναι η Χρονική Αντίθεση, όπου οι αλλαγές του σήματος εισόδου που υπερβαίνουν ένα κατώτατο όριο συμβολίζονται ως αιχμές. Ωστόσο, αυτός ο αλγόριθμος κωδικοποίησης απαιτεί την αποδοχή αρνητικών αιχμών. Αυτό είναι απαραίτητο, καθώς το σήμα εισόδου μπορεί να αυξηθεί με την πάροδο του χρόνου στην αρχή, όπου θα κωδικοποιηθεί ως θετικές αιχμές, αλλά να επιστρέφει στις χαμηλότερες τιμές μετά. Αυτή η πτώση θα πρέπει να κωδικοποιηθεί καθώς μπορεί να υπερβεί το όριο αλλά πρέπει να συμβολίζεται διαφορετικά από τη θετική αιχμή, συνήθως ως αρνητική αιχμή (ή σαν την ενεργοποίηση διαφορετικού νευρώνα) όπως φαίνεται στο σχήμα 4.10. Το τρένο αιχμών προσπαθεί να ακολουθήσει το σήμα εισόδου μιμούμενο τη συμπεριφορά του αλλά κβαντοποιώντας τις πιθανές τιμές. Αυτός ο αλγόριθμος είναι παρόμοιος με τη διαμόρφωση δέλτα (\textlatin{delta modulation}) \cite{Schindler1970}, ένας ευρέως γνωστός μετατροπέας αναλογικού-ψηφιακού σήματος με τον οποίο η μέθοδος της χρονικής αντίθεσης φέρει παρόμοια πλεονεκτήματα και μειονεκτήματα. Οι μόνες εκτιμήσεις παραμέτρων σε αυτόν τον αλγόριθμο κωδικοποίησης είναι η τιμή του κατωφλίου και το \textlatin{dt} δύο διαδοχικών διακριτών χρονικών βημάτων. Συνήθως θεωρείται ότι \textlatin{dt} = 0 και ότι το κωδικοποιημένο τρένο αιχμών δεν χάνει πληροφορίες λόγω αυτής της καθυστέρησης, αφήνοντας σαν μοναδική παράμετρο την τιμή κατωφλίου. Η τιμή κατωφλίου στη χρονική αντίθεση υπολογίζεται λαμβάνοντας υπόψη ολόκληρο το σήμα εισόδου και σχετίζεται με την πρώτη παράγωγό του, έτσι ώστε να βελτιστοποιηθεί ο αριθμός των αιχμών στο κωδικοποιημένο τρένο αιχμών. Αυτός ο αλγόριθμος κωδικοποίησης, μαζί με άλλους που θα παρουσιαστούν στη συνέχεια, μελετάται περαιτέρω σε αυτήν την εργασία \cite{Petro2020}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=8cm]{Encoding Schemes/Illustration-of-event-based-encoding-of-visual-signals-Shown-are-the-log-luminance.png}
    \caption{Σήμα κωδικοποιημένο στον χρόνο βάσει της χρονικής αντίθεσης. Η υπέρβαση του ορίου \(\Delta I\) προκαλεί μια θετική αιχμή, ενώ η κάτω υπέρβαση προκαλεί μια αρνητική.}
    \label{fig:spikeprop-net-architecture}
\end{figure}

\subsection{Περαιτέρω μελέτη}

Για να ξεπεραστεί η ανάγκη της πρότερης επίγνωσης ολόκληρου του σήματος όπως απαιτείται στον αλγόριθμο χρονικής αντίθεσης, έχουν αναπτυχθεί δύο ακόμη αλγόριθμοι κωδικοποίησης, η κωδικοποίηση μπροστά βήματος (\textlatin{Step-Forward}) και αυτή του κινούμενου παραθύρου (\textlatin{Moving Window}). Αυτά τα δύο σχήματα κωδικοποίησης ακολουθούν την ίδια αρχή με τη χρονική αντίθεση, καθώς και τα δύο κωδικοποιούν πληροφορίες της διαφοράς του σήματος στο χρόνο. Η κωδικοποίηση \textlatin{Step-Forward} έχει μια παράμετρο κατωφλίου \textlatin{Th} που ορίζεται ως είσοδος από τον μηχανικό και δεν χρησιμοποιεί καμία πληροφορία του σήματος. Υπολογίζει εάν η τιμή του σήματος έχει ξεπεράσει μια βασική γραμμή \textlatin{B(t) \(\pm\) Th}. Η βασική γραμμή ορίζεται στην αρχή ως η αρχική τιμή του σήματος και ενημερώνεται ως \textlatin{B(t+1) = B(t) \(\pm\) Th} ανάλογα με τη συμπεριφορά του σήματος (συν εάν το σήμα υπερβαίνει θετικά, μείον αν υπερβαίνει αρνητικά και \textlatin{B(t) = B(t+1)} εάν δεν υπερβαίνει το όριο). Οι υψηλές τιμές κατωφλίου παράγουν λιγότερο πυκνά τρένα αιχμών, ενώ οι χαμηλές προκαλούν τρένα αιχμών που αποτελούνται κυρίως από εκπεμπόμενες αιχμές, είτε θετικές είτε αρνητικές, δημιουργώντας ένα υπερφορτωμένο σήμα. Η κωδικοποίηση κινούμενου παραθύρου στέκεται μεταξύ των \textlatin{Step-Forward} και \textlatin{Temporal Contrast}. Έχει επίσης ένα όριο \textlatin{Th} που έχει οριστεί από τον χρήστη και μια βασική γραμμή Β, αλλά η βασική γραμμή υπολογίζεται ως η μέση τιμή των προηγούμενων τιμών του σήματος σε ένα χρονικό παράθυρο που έχει οριστεί από τον χρήστη, δημιουργώντας έτσι ένα φίλτρο κινητού μέσου όρου. Η αποκωδικοποίηση των προκύπτων τρένων αιχμών με χρήση \textlatin{Step-Forward} και \textlatin{Moving Window} είναι η ίδια με την αποκωδικοποίηση ενός τρένου αιχμών με χρήση \textlatin{Temporal Contrast}, όπου για κάθε θετική αιχμή στο ανακατασκευασμένο σήμα προστίθεται η τιμή κατωφλίου και για κάθε αρνητική μειώνεται κατά το κατώφλι.

Μια άλλη προσέγγιση της κωδικοποίησης σημάτων σε τρένα αιχμών είναι η χρήση φίλτρων πεπερασμένης απόκρισης ώθησης (φίλτρα \textlatin{FIR}). Στον αλγόριθμο \textlatin{Hough Spiker (HSA)} \cite{Gers1999}, ορίζεται ένα φίλτρο \(h(t)\) με χρονικά βήματα μεγέθους Μ όπου ικανοποιείται η ακόλουθη εξίσωση:

\begin{equation}
    o(t)=(x\circledast h)=\sum_{k=0}^{M}x(t-k)h(k)
\end{equation}

Στην παραπάνω συνέλιξη, \(o(t)\) είναι το αναλογικό σήμα και \(x(t)\) είναι το τρένο αιχμών. Αυτή η εξίσωση αντιπροσωπεύει την αποκωδικοποίηση του σήματος που δίνεται από το τρένο αιχμών. Για να γίνει η κωδικοποίηση, πρέπει να γίνει η αντίστροφη διαδικασία. Αυτό γίνεται συγκρίνοντας τις τιμές του φίλτρου με αυτές του σήματος και εκπέμποντας μια αιχμή και αφαιρώντας το φίλτρο από το σήμα εάν όλες οι τιμές του φίλτρου είναι ίσες ή μικρότερες από το σήμα, μετακινώντας ένα χρονικό βήμα προς τα εμπρός και επαναλαμβάνοντας. Ο αλγόριθμος \textlatin{Ben's Spike (BSA)} \cite{Schrauwen2003} χρησιμοποιεί την ίδια ιδέα, να συνελίξει το σήμα με ένα φίλτρο \textlatin{FIR} h(t). Υπολογίζει δύο σφάλματα Ε1 και Ε2 ως εξής:

\begin{equation}
    E1=\sum_{k=0}^{M}abs(s(t+k)-h(k)) \;\;\;\;\;\;\;
    E2=\sum_{k=0}^{M}abs(s(t+k))
\end{equation}

Στην εξ. 4.16 το \(abs()\) υποδεικνύει την απόλυτη τιμή και \(s(t)\) το σήμα εισόδου. Για κάθε χρονικό βήμα t, εάν το E1 είναι μικρότερο από το E2 (συν ένα κατώφλι) τότε εκπέμπεται μια αιχμή και το φίλτρο αφαιρείται από το σήμα εισόδου αλλιώς δεν συμβαίνει τίποτα. Οι αλγόριθμοι \textlatin{BSA} και \textlatin{HSA} παρέχουν το μεγάλο πλεονέκτημα ότι δεν απαιτούν την ύπαρξη αρνητικών αιχμών. Το προκύπτον κωδικοποιημένο σήμα περιέχει μόνο θετικά εκπεμπόμενες αιχμές που είναι πιο βιολογικά αληθοφανές σχήμα και μπορεί να είναι ευκολότερο να το χειριστεί ένα ΝΔΑ.

Αυτοί οι αλγόριθμοι κωδικοποίησης μελετώνται περαιτέρω στο \cite{Petro2020} (με εξαίρεση τον \textlatin{HSA}). Επιπλέον, ο \textlatin{Julien Dupeyroux} στο έργο του \cite{Dupeyroux2021}, με γνώμονα τις δυνατότητες που παρουσιάζονται στη χρήση των Νευρωνικών Δικτύων Αιχμών στη ρομποτική, έχει αναπτύξει μια βιβλιοθήκη στο \textlatin{ROS (Robotic Operating System)}, στην οποία εφάρμοσε τους αλγορίθμους που αναφέρθηκαν προηγουμένως για κωδικοποίηση της πληροφορίας σε αιχμές στα ρομποτικά συστήματα. Στο σχήμα 4.10, οι αλγόριθμοι \textlatin{Ben's Spike (BSA), Temporal Contrast} (ή \textlatin{Threshold-based representation -TBR}), \textlatin{Step-Forward (SF)} και \textlatin{Moving Window (MW)} συγκρίνονται μεταξύ τους σε χαρακτηριστικά σημαντικά για το κωδικοποιημένο σήμα.

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Encoding Schemes/encoding algo stats.png}
    \caption{Οι αλγόριθμοι BSA, TBR, SF και MW και τα χαρακτηριστικά που προσδίδουν στο προκύπτον τρένο αιχμών.}
    \label{fig:spikeprop-net-architecture}
\end{figure}

Το επιστημονικό πεδίο της κωδικοποίησης πληροφοριών σε αιχμές προσφέρει πολύ περισσότερα σχήματα κωδικοποίησης και αλγόριθμους που δεν αναφέρονται εδώ. Δεδομένου ότι η επιλογή του σχήματος και του αλγορίθμου κωδικοποίησης εισόδου επηρεάζει την απόδοση του Νευρωνικού Δικτύου, ο προγραμματιστής πρέπει να εξετάσει ποιος από τους αλγόριθμους του ταιριάζει καλύτερα. Για εκτενέστερη μελέτη παρέχονται τα ακόλουθα έργα \cite{Guo2021}, \cite{Schuman2019} προς περαιτέρω ανάγνωση.

\selectlanguage{greek}
\chapter{Μέθοδοι Μάθησης Νευρωνικών Δικτύων Αιχμών}
Έχοντας περιγράψει προηγουμένως τις μεθόδους μοντελοποίησης νευρώνων, αυτή η ενότητα αναλύει τις μεθόδους μάθησης που χρησιμοποιούνται στη μηχανική μάθηση με νευρωνικά δίκτυα.
\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Learning Methods/spikeprop/spikeprop-net-architecture.PNG}
    \caption{(A)\textlatin{Feedforward Spiking Neural Network}  (B) Σύνδεση μεταξύ νευρώνων που αποτελείται από πολλαπλά συναπτικά τερματικά (\textlatin{synaptic terminals})}
    \label{fig:spikeprop-net-architecture}
\end{figure}
\section{\textlatin{SpikeProp}}

Το \textlatin{SpikeProp}, μια μέθοδος που βασίζεται σε \textlatin{backpropagation} για εποπτευόμενη μάθηση, ήταν μία από τις πρώτες μεθόδους που χρησιμοποιήθηκαν για την εκπαίδευση των Νευρωνικών Δικτύων αιχμών \cite{Bohte2002}. Ο χρονισμός των αιχμών ενός νευρώνα κωδικοποιεί τις πληροφορίες κατά τη διάρκεια της εκπαίδευσης. Οι ερευνητές αποδεικνύουν επίσης εμπειρικά ότι δίκτυα βιολογικά εύλογων νευρώνων με αιχμές μπορούν να εκτελέσουν σύνθετη μη γραμμική ταξινόμηση σε μια γρήγορη χρονική κωδικοποίηση, καθώς και δίκτυα που κωδικοποιούνται βάση του ρυθμού των αιχμών.
\subsection{Αρχιτεκτονική Δικτύου}
Η αρχιτεκτονική αποτελείται από ένα προωθητικό δίκτυο νευρώνων με αιχμές (\textlatin{ feedforward network of spiking neurons}) , αντί για τεχνητούς νευρώνες, ακολουθούμενο από συναπτικούς τερματικούς σταθμούς με καθυστέρηση, όπως περιγράφεται στο \cite{ruf1998}, βλέπε Εικ. \ref{fig:spikeprop-net-architecture}. Οι νευρώνες ακίδας συνήθως δημιουργούν δυνατότητες δράσης (αιχμές) όταν το δυναμικό της μεμβράνης ξεπεράσει ένα όριο. Οι ερευνητές θεωρούν το δυναμικό της μεμβράνης ως εσωτερική μεταβλητή νευρωνικής κατάστασης. Η σχέση μεταξύ των αιχμών και της μεταβλητής εσωτερικής κατάστασης του νευρώνα μπορεί να περιγραφεί από οποιοδήποτε μοντέλο νευρώνα, αλλά στην εφαρμογή του \textlatin{SpikeProp} σε πειράματα, χρησιμοποιείσαν το μοντέλο απόκρισης αιχμών (\textlatin{SRM}).

Ένας νευρώνας \textit{\textlatin{j}} στο δίκτυο, λαμβάνει αιχμές εισόδου από ένα σύνολο προ-συναπτικών νευρώνων \(\Gamma\textsubscript{\textlatin{j}}\) με χρόνους πυροδότησης \(t\textsubscript{\textlatin{j}},\textlatin{i}\in\Gamma\textsubscript{\textlatin{j}}\). Η δυναμική της μεταβλητής εσωτερικής κατάστασης του νευρώνα \(x\textsubscript{\textlatin{j}}\) επηρεάζεται από τους προ-συναπτικούς νευρώνες σύμφωνα με μια συνάρτηση απόκρισης ακίδας \(\varepsilon(t)\) και ένα συναπτικό βάρος \(\textlatin{w}\textsubscript{\textlatin{j}}\).
\begin{equation}
    x\textsubscript{\textlatin{j}}(t) = \sum_{\textlatin{i}\in\Gamma\textsubscript{\textlatin{j}}} w\textsubscript{\textlatin{ij}}\varepsilon(t-t\textsubscript{i})
\end{equation}

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Learning Methods/spikeprop/spikeprop-connections-network.PNG}
    \caption{Συνδέσεις ενός \textlatin{Feedforward Spiking Neural Network} : (A) ενα συναπτικο τερματικό: το αργοπορημένο
προ-συναπτικό δυναμικό σταθμίζεται και παράγει ένα μετα-συναπτικό δυναμικό, (Β)
δύο συνδέσεις πολλαπλών συνάψεων με τη σταθμισμένη είσοδο να αθροίζεται στον επόμενο νευρώνα}
    \label{fig:spikeprop-connections-network}
\end{figure}

Η συνάρτηση απόκρισης είναι υπεύθυνη για το μετασυναπτικό δυναμικό, την ακίδα που παράγεται και το συναπτικό βάρος ρυθμίζει το ύψος αυτού του δυναμικού. Κάθε σύνδεση,σχήμα \ref{fig:spikeprop-net-architecture} (B), μπορεί να αναλυθεί περαιτέρω σε \textit{\textlatin{m}} σταθερά συναπτικά τερματικά όπου κάθε συναπτικό τερματικό έχει το δικό του βάρος και καθυστέρηση. Αυτή η καθυστέρηση ορίζεται ως η διαφορά μεταξύ του χρόνου πυροδότησης του προσυναπτικού νευρώνα, και την χρονική στιγμή που το μετασυναπτικό δυναμικό αρχίζει να αυξάνεται, Σχ.\ref{fig:spikeprop-connections-network} (A). Μια προ-συναπτική άνοδος σε ένα συναπτικό τερματικό \textit{k} ορίζεται ως μετασυναπτικό δυναμικό τυπικού ύψους με καθυστέρηση \(d^k\). Ο μη σταθμισμένος όρος ενός μόνο συναπτικού τερματικού που συμβάλλει στη μεταβλητή κατάστασης μπορεί να οριστεί ως:

\begin{equation}
    y^k\textsubscript{\textlatin{i}}(t) = \varepsilon(t-t\textsubscript{\textlatin{i}}-d^k) 
\end{equation}

Η συνάρτηση απόκρισης ακίδας  \(\varepsilon(t)\) έχει αρχική τιμή μηδέν. Ο χρόνος \(t\textsubscript{\textlatin{i}}\) είναι ο χρόνος πυροδότησης του προ-συναπτικού νευρώνα \textit{\textlatin{i}} . Η συνάρτηση απόκρισης δίνεται από: 
\begin{equation}
    \varepsilon(t)=\frac{t}{\tau}e^{1-\frac{t}{\tau}}
\end{equation}
\(\tau\) μοντελοποιεί τη σταθερά χρόνου του δυναμικού της μεμβράνης  που καθορίζει τον χρόνο ανόδου και αποσύνθεσης του μετασυναπτικού δυναμικού. Το σταθμισμένο άθροισμα των προ-συναπτικών εισφορών είναι τώρα:
\[ \sum_{k=1}^{m}w\textsubscript{\textlatin{ij}}^ky\textsubscript{\textlatin{i}}^k(t) \]
H μεταβλητή \(w\textsubscript{\textlatin{ij}}^ky\) δηλώνει το σχετικό βάρος του συναπτικού τερματικού k ,\textlatin{fig} \ref{fig:spikeprop-connections-network} . Ο χρόνος πυροδότησης \textit{\textlatin{t}}\textsubscript{\textlatin{j}} του νευρώνα \textit{\textlatin{j}} είναι ίσος με την πρώτη φορά που η μεταβλητή κατάστασης υπερβαίνει το κατώφλι \(theta\) .Το κατώφλι είναι σταθερή τιμή και παραμένει ίσο μεταξύ όλων των νευρώνων.

\subsection{\textlatin{BackPropagation} με \textlatin{SpikeProp}}

Το δίκτυο έχει τρία επίπεδα όπως φαίνεται στο Σχήμα \ref{fig:spikeprop-net-architecture},\textlatin{H}(είσοδος) , \textlatin{I}(κρυμένο) ,\textlatin{J}(έξοδος), αν και μπορούν να χρησιμοποιηθούν περισσότερα κρυμμένα επίπεδα.

Ο στόχος αυτού του αλγορίθμου είναι να μάθει ένα σύνολο χρόνων πυροδότησης,{\(t\textsubscript{\textlatin{j}}^d\)} στους νευρώνες εξόδου, {\(\textlatin{j}\in \textlatin{J}\)} για ένα δεδομένο σύνολο μοτίβων εισόδου {\(P[t\textsubscript{1}...t\textsubscript{\textlatin{h}}]\)}. Το σύνολο των μοτίβων εισόδου ορίζει ένα ενιαίο μοτίβο εισόδου που περιγράφεται με μοναδικούς χρόνους ακίδας για κάθε νευρώνα {\(h\in H\)}.

Η συνάρτηση σφάλματος που χρησιμοποιήθηκε στα πειράματα των ερευνητών ήταν η ρίζα του μέσου τετραγωνικού σφάλματος, αλλά και πάλι μπορούν να γίνουν άλλες επιλογές γι 'αυτήν. Δεδομένων επιθυμητών χρόνων αιχμών
{\(t\textsubscript{\textlatin{j}}^d\)} και τους πραγματικούς χρόνους πυροδότησης {\(t\textsubscript{\textlatin{j}}^a\)}, η συνάρτηση σφάλματος ορίζεται ως:

\selectlanguage{english}
\begin{equation}
    E=1/2\sum_{j\in J}(t\textsubscript{j}^a-t\textsubscript{j}^d)^2
\end{equation}
\selectlanguage{greek}
Κάθε συναπτικό τερματικό \textit{\textlatin{k}} θεωρείται ως ξεχωριστή σύνδεση με το βάρος \(w\textsubscript{\textlatin{ij}}^k\). Οι εξισώσεις του \textlatin{backpropagation} για τους νευρώνες εξόδου είναι:
\selectlanguage{english}
\begin{equation}
    \Delta w\textsubscript{ij}^k=-\eta y\textsubscript{i}^k (t\textsubscript{j}^a) \delta \textsubscript{j}
\end{equation}
\begin{equation}
\delta \textsubscript{j} = \frac { t\textsubscript{j}^d-t\textsubscript{j}^a}  {\sum_{i \in \Gamma \textsubscript{j} } \sum_{l} w \textsubscript{ij}^l(\partial y \textsubscript{i}^l(t \textsubscript{j}^a) / \partial t \textsubscript{j}^a)}
\end{equation}
\selectlanguage{greek}
Τώρα, πρέπει να οριστούν οι εξισώσεις για τα κρυμμένα επίπεδα.
\selectlanguage{english}
\begin{equation}
    \Delta w\textsubscript{hi}^k=-\eta y\textsubscript{h}^k (t\textsubscript{i}^a) \delta \textsubscript{i}
\end{equation}
\begin{equation}
\delta \textsubscript{i} = \frac { \sum_{j \in \Gamma^i  }\delta\textsubscript{j}\sum_{k} w \textsubscript{ij}^k(\partial y \textsubscript{i}^k(t \textsubscript{j}^a) / \partial t \textsubscript{i}^a)}
{ \sum_{h \in \Gamma \textsubscript{i} }\sum_{l} w \textsubscript{hi}^l(\partial y \textsubscript{h}^l(t \textsubscript{i}^a) / \partial t \textsubscript{i}^a)}
\end{equation}
\selectlanguage{greek}
Η ταξινόμηση εξόδου κωδικοποιήθηκε με τρόπο που ο νευρώνας που κωδικοποιούσε για την αντίστοιχη κατηγορία είχε εκχωρηθεί σε ένα χρόνο πρώτης πυροδότησης.

\subsection{Σημαντικές παρατηρήσεις και περιορισμοί}

Κατά τον πειραματισμό, ο αλγόριθμος δεν συγκλίνει εάν υπάρχουν αρνητικά και θετικά βάρη. Επιπλέον, για τη σύγκλιση της μάθησης ήταν απαραίτητο να ενσωματωθούν τόσο διεγερτικοί όσο και ανασταλτικοί νευρώνες. Για την κωδικοποίηση της εισόδου, χρησιμοποιείται μια μέθοδος για την κωδικοποίηση μεταβλητών εισόδου σε χρονικά μοτίβα ακίδας με κωδικοποίηση πληθυσμού. Ωστόσο, η κωδικοποίηση των δεδομένων εισόδου εξαρτάται από την επιλογή του πειραματιστή και δεν περιορίζεται σε μία συγκεκριμένη επιλογή από τον αλγόριθμο, οπότε η απόδοση μπορεί να ποικίλει. Παρόλο που ο αλγόριθμος φαίνεται πολλά υποσχόμενος, απαιτεί τον ίδιο αριθμό επαναλήψεων σε σύγκριση με ένα τυπικό \textlatin{MLP} . Ωστόσο, δεδομένης της ρητής χρήσης του χρονικού πεδίου για υπολογισμούς, υποστηρίζεται ότι ένα δίκτυο νευρώνων ακίδας είναι εγγενώς πιο κατάλληλο για μάθηση και αξιολόγηση χρονικών προτύπων από τα σιγμοειδή δίκτυα.
\section{Πλαστικότητα εξαρτώμενη από τον χρόνο ακίδας (\textlatin{STDP})}

Η πλαστικότητα που εξαρτάται από τον χρόνο ακίδας είναι μια βιολογική διαδικασία, ένας τύπος πλαστικότητας, που προσαρμόζει τη δύναμη των νευρωνικών συνδέσεων στον εγκέφαλο, όπως περιγράφεται στην υποενότητα 2.3.4. Πιο συγκεκριμένα, το \textlatin{STDP} μπορεί να οριστεί ως η διαδικασία που τροποποιεί τη δύναμη των συνδέσεων με βάση τον σχετικό χρονισμό των δυνατοτήτων εξόδου και εισόδου ενός νευρώνα (ή αιχμών). Από τη σκοπιά ενός μαθηματικού μπορεί να θεωρηθεί ως μια χρονικά ασύμμετρη μορφή \textlatin{hebbian} μάθησης που εξαρτάται από τους χρονικούς συσχετισμούς των νευρώνων που προκαλούνται από στενούς χρονικούς συσχετισμούς μεταξύ των αιχμών των προσυναπτικών και μετασυναπτικών νευρώνων \cite{stdp2010}.

\textlatin{STDP} με νευρωνικά δίκτυα αιχμών είναι μια μορφή μάθησης χωρίς επίβλεψη . Κατά τη διάρκεια της μάθησης, η αλλαγή \(\Delta w\textsubscript{\textlatin{j}}\) μιας σύναψης από έναν προσυναπτικό νευρώνα \textit{\textlatin{j}} εξαρτάται από τη σχετική χρονική στιγμή μεταξύ εισόδων προσυναπτικής ακίδας και μετασυναπτικών αιχμών. Το σύνολο προσυναπτικών ωρών άφιξης στη σύναψη \textit{\textlatin{j}},με αριθμό καταμέτρησης \textit{\textlatin{f}} συμβολίζεται με \(t\textsubscript{\textlatin{j}}^f\) . 
Ομοίως, οι χρόνοι πυροδότησης του μετασυναπτικού νευρώνα ονομάζονται \(t\textsubscript{\textlatin{i}}^n\). Το συνολικό βάρος αλλάζει τότε \cite{Gerstner1996} σύμφωνα με τον τύπο: 
\selectlanguage{english}
\begin{equation}
\Delta w\textsubscript{j}=\sum_{f=1}^N\sum_{n=1}^N W(t\textsubscript{i}^n-t\textsubscript{j}^f)
\end{equation}
\selectlanguage{greek}
\(W(x)\) δηλώνει το παράθυρο εκμάθησης, μια συνήθης επιλογή είναι η ακόλουθη:
\selectlanguage{english}
$$
W(x)=\begin{cases}
			A\textsubscript{+}exp(-x/\tau\textsubscript{+}), & \text{for $x$\textgreater0}\\
            -A\textsubscript{-}exp(x/\tau\textsubscript{-}), & \text{for $x$\textless0}
		 \end{cases}
$$
\selectlanguage{greek}
Οι τιμές A\textsubscript{+} και -A\textsubscript{-} μπορεί να εξαρτώνται από την τρέχουσα τιμή του \textlatin{W}(\textlatin{x}) και οι σταθερές χρόνου \(\tau\) είναι της τάξης των 10 \textlatin{ms}. Έχει αποδειχθεί ότι το \textlatin{STDP} μαθαίνει «πρότυπα πρώιμης ακίδας» όταν ένας νευρώνας παρουσιάζεται επανειλημμένα με διακριτές βολές εισόδου( \textlatin{discrete volleys of input spikes}) , συγκεντρώνοντας συναπτικά βάρη σε εισόδους που πυροδοτούνται συνεχώς νωρίς, με αποτέλεσμα η μετασυναπτική καθυστέρηση ακίδας να μειώνεται μέχρι να φτάσει σε μια ελάχιστη και σταθερή τιμή. Αυτά τα ευρήματα ισχύουν υπό ένα συνεχές καθεστώς στο οποίο οι εισροές πυροδοτούν με σταθερό ρυθμό πληθυσμού. Κατά συνέπεια, το \textlatin{STDP} μπορεί να χειριστεί ένα δύσκολο υπολογιστικό πρόβλημα:τον εντοπισμό επαναλαμβανόμενου χωροχρονικού μοτίβου ακίδας που περιέχεται σε εξίσου πυκνούς συρμούς «περισπαστών» αιχμών(\textlatin{‘distractor' spike trains}) \cite{Masquelier2008}. Το \textlatin{STDP} επιτρέπει έτσι κάποια μορφή χρονικής κωδικοποίησης, ακόμη και ελλείψει ρητής χρονικής αναφοράς. Έχει αναφερθεί ότι επαναλαμβανόμενα χωροχρονικά μοτίβα ακίδας με ακρίβεια χιλιοστού του δευτερολέπτου υπάρχουν σε ηλεκτροφυσιολογικά πειράματα \cite{Fellous2004}. Εδώ \cite{Masquelier2008} δείχνουν πώς το \textlatin{STDP} μπορεί να μάθει το δύσκολο έργο της ανίχνευσης αυτών των επαναλαμβανόμενων μοτίβων, καταδεικνύοντας για άλλη μια φορά πώς τα νευρωνικά δίκτυα αιχμών μπορούν να λύσουν δύσκολα προβλήματα αυτού του τύπου με τις κατάλληλες μεθόδους.
\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Learning Methods/stdp/1.png}
    \caption{Με κόκκινο χρώμα υποδεικνύεται ένα επαναλαμβανόμενο μοτίβο διάρκειας 50 \textlatin{ms} που αφορά 50 προσαγωγούς νευρώνες μεταξύ 100. Το κάτω πλαίσιο απεικονίζει τους μέσους ρυθμούς πυροδότησης του πληθυσμού σε "κάδους χρόνου" (\textlatin{time bins}) των 10 \textlatin{ms}. Το δεξιό πλαίσιο απεικονίζει τα μεμονωμένα ποσοστά πυροδότησης κατά μέσο όρο σε όλη την περίοδο}
    \label{fig:stdp-1}
\end{figure}
\subsection{\textlatin{STDP}: ανίχνευση επαναλαμβανόμενων μοτίβων}
Ένα παράδειγμα ενός τέτοιου επαναλαμβανόμενου μοτίβου φαίνεται στο σχήμα \ref{fig:stdp-1}. Το πρόβλημα περιπλέκεται από το γεγονός ότι ούτε ο ρυθμός πυροδότησης του πληθυσμού ούτε οι ρυθμοί πυροδότησης των νευρώνων που συμμετέχουν στο μοτίβο είναι μοναδικοί κατά τη διάρκεια των περιόδων όταν υπάρχει το μοτίβο. Αυτό το είδος της κατάστασης απαιτεί να ληφθούν υπόψη οι χρόνοι/χρονικές στιγμές των αιχμών. Οι ερευνητές δείχνουν πώς ένας νευρώνας που χρησιμοποιεί \textlatin{STDP} μπορεί να λύσει αυτό το πρόβλημα αξιοποιώντας το γεγονός ότι ένα μοτίβο είναι μια σειρά από συμπτώσεις αιχμών. Το \textlatin{STDP} είναι γνωστό ότι έχει την επίδραση της συγκέντρωσης συναπτικών βαρών σε επαναλαμβανόμενες πρώιμες εισόδους πυροδότησης, με αποτέλεσμα τη μείωση της λανθάνουσας μετασυναπτικής ακίδας όταν ένας νευρώνας παρουσιάζεται επανειλημμένα με παρόμοια βολή εισόδου. Με άλλα λόγια, αυτές οι εισροές οδηγούν συστηματικά στη διαμόρφωση της επιλεκτικότητας του νευρώνα. Αυτή η διαμόρφωση επιτεύχθηκε σε μια ποικιλία συνθηκών παρουσίας θορύβου , καθώς και σε καταστάσεις όπου οι καθυστερήσεις και τα ποσοστά πυροδότησης ή ο συγχρονισμός έδωσαν αντιφατικές πληροφορίες \cite {rossum2000} \cite {guyonneau2005} \cite {masquelier2007}. Οι ερευνητές υποδεικνύουν τον περιορισμό αυτών των μελετών που απαιτούν ρητή χρονική αναφορά και αναρωτιούνται εάν το \textlatin{STDP} μπορεί να αναγνωρίσει το επαναλαμβανόμενο μοτίβο ελλείψει χρονικής αναφοράς.

Για να δοκιμαστεί αυτό, εισήχθη ένα αυθαίρετο μοτίβο όπως στο Σχήμα \ref{fig:stdp-1} και ερευνήθηκε εάν ένα μόνο \textlatin{STDP} που λάμβανε ήταν σε θέση να το μάθει χωρίς επίβλεψη. Οι αιχμές εισόδου προσομοιώθηκαν σύμφωνα με μια διαδικασία \textlatin{Poisson} όπου οι νευρώνες πυροδοτούν στοχαστικά και ανεξάρτητα. Το αυθαίρετο μοτίβο ήταν σκόπιμα κρυμμένο από τον ρυθμό πυροδότησης των νευρώνων, οπότε θα ήταν αδύνατο να λυθεί χρησιμοποιώντας μόνο τους ρυθμούς πυροδότησης. Ενισχύοντας τις συναπτικές συνδέσεις με τους προσαγωγούς νευρώνες που συμμετείχαν στην πυροδότηση του νευρώνα (διαμορφώνοντας την εκλεκτικότητα του νευρώνα). Όταν το μοτίβο εμφανιστεί ξανά, αυξάνεται η πιθανότητα ο νευρώνας να πυροδοτηθεί ξανά. Εκτός από τη διαμόρφωση της επιλεκτικότητας του νευρώνα, επιτρέπει επίσης τη σύγκλιση με κορεσμό, όταν όλες οι αιχμές στο μοτίβο που προηγούνται της μετασυναπτικής ακίδας αντιστοιχούν ήδη σε συνάψεις με το μέγιστο επιτρεπόμενο δυναμικό και όλες είναι απαραίτητες για να φτάσουμε στο κατώφλι. Οι αιχμές έξω από το μοτίβο έχουν κατασταλλεί, ώστε να μην συμβάλλουν στο δυναμικό της μεμβράνης. Αυτό οδηγεί σε απουσία ψευδών "συναγερμών" που σημαίνει ότι μετά την εκμάθηση,ο νευρώνας πυροδοτείται  μόνο όταν υπάρχει το μοτίβο (σχήμα \ref{fig:stdp-converged}). Εάν αυτό συμβαίνει στον εγκέφαλο, οι πληροφορίες σχετικά με ένα ερέθισμα μπορούν να είναι άμεσα διαθέσιμες, αφού οι νευρώνες θα πυροδοτήσουν κατά την έναρξη του ερεθίσματος, όπως υποστηρίζεται εδώ \cite{thorpe2001}.

\subsection{\textlatin{STDP} και ταλαντώσεις}
Το \textlatin{STDP} επιτρέπει επίσης την ενσωμάτωση ταλαντώσεων καθώς φάνηκε ότι είναι σε θέση να επιλέξει μόνο εισόδους κλειδωμένες σε μια συγκεκριμένη φάση (\textlatin{phase-locked inputs}) μεταξύ ενός ευρέος πληθυσμού με τυχαίες φάσεις, μετατρέποντας τον μετασυναπτικό νευρώνα σε έναν ανιχνευτή συμπτώσεων. Μαθαίντας στο δίκτυο να ανταποκρίνεται σε ορισμένες εισόδους κλειδωμένες σε φάσεις, μπορεί να συντονίσει και να συγχρονίσει τη νευρωνική δραστηριότητα \cite{Gerstner1996}. Ένα παράδειγμα που αποδεικνύεται στην προαναφερθείσα έρευνα είναι πώς μια κουκουβάγια είναι σε θέση να συντονίσει τη νευρωνική δραστηριότητα μεταξύ των δύο αυτιών της αρκετά γρήγορα με αποτέλεσμα ο χρόνο αντίδρασης της πριν γυρίσει το κεφάλι της να είναι περίπου 100 \textlatin{ms}. Η εκμάθηση με το \textlatin{STDP} επιλέγει συνδέσεις συνάψεως με τέτοιο τρόπο ώστε οι αιχμές να φτάνουν με συνέπεια. Ένα άλλο παράδειγμα τέτοιας ανάγκης για συντονισμό με ταλαντώσεις είναι κατά τη διάρκεια της σακκαδικής κίνησης των ματιών. Για να αξιοποιηθούν αποτελεσματικά οι χωρικές πληροφορίες που περιέχονται στις διαμορφώσεις φωτεινότητας που προκύπτουν από τις κινήσεις των ματιών, η ανάλυση της νευρικής δραστηριότητας πρέπει να χρονομετρηθεί σε σχέση με την εμφάνιση των σακκαδικών κινήσεων. Για παράδειγμα, μια ακίδα από τον ίδιο νευρώνα φέρει διαφορετική πληροφοριακή τιμή εάν συμβαίνει κατά τη διάρκεια της πρώιμης εστίασης (\textlatin{early fixation}), όταν η αλλαγή εισόδου που προκαλείται από το προηγούμενη σακκαδική κίνηση ασκεί ακόμη την επιρροή του ή αργότερα, όταν οι χρονικές αλλαγές προκαλούνται μόνο από την οφθαλμική μετατόπιση (\textlatin{ocular drift})  \cite{Rucci2018 },δείτε και το σχήμα \ref{fig:eyes_saccades}.Οι συχνότητες των οπτικών ταλαντώσεων του οπτικού φλοιού μπορούν να ελεγχθούν τοπικά και μπορούν να ρυθμιστούν από και να κλειδωθούν σε εξωτερικά ερεθίσματα \cite{Ahissar2012}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Learning Methods/stdp/stdp-converged.png}
    \caption{(α) Το μοτίβο παριστάνεται ως γκρι ορθογώνιο. Σημειώστε την ομάδα των λευκών αιχμών στην αρχή: Οι περισσότερες από τις συνάψεις που αντιστοιχούν στις πρώτες αιχμές του μοτίβου έχουν ενισχυθεί από το STDP. Αξίζει να σημειωθεί ότι σχεδόν όλες οι συναπτικές συνδέσεις με προσαγωγούς που δεν ασχολούνται με το μοτίβο έχουν καταργηθεί πλήρως. (β) Στο ίδιο εύρος με το παραπάνω, το δυναμικό της μεμβράνης εμφανίζεται ως συνάρτηση του χρόνου. Η απότομη ακίδα που αντιστοιχεί στην προαναφερθείσα ομάδα είναι άμεσα ορατή\cite{Masquelier2008}.} 
    \label{fig:stdp-converged}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Learning Methods/stdp/eyes_saccades.png}
    \caption{Σακκαδικές κινήσεις:\textlatin{microsaccades or saccades}. Οφθαλμική μετατόπιση:\textlatin{ocular drift}.Φωτοϋποδοχείς στον αμφιβληστροειδή χιτώνα: \textlatin{photoreceptor cells}.}
    \label{fig:eyes_saccades}
\end{figure}

\section{Μάθηση με υποκαταστάτη παραγώγου σε Νευρωνικά Δίκτα αιχμών}

Σε αυτή την υποενότητα παρουσιάζουμε τις μεθόδους υποκατάστατης παραγώγου, καθώς οι υποκατάστατες παράγωγοι χρησιμοποιούνται ευρέως σε άλλες μεθόδους εκμάθησης τις οποίες θα δούμε αργότερα. Επιπλέον, οι ερευνητές που δημοσιεύσαν την σχετική έρευνα την αντιστοίχηση \textlatin{SNN} σε \textlatin{RNN} \cite{neftci2019}. Η διατύπωση των \textlatin{SNN} ως \textlatin{RNN} επιτρέπει τη μεταφορά και εφαρμογή υφιστάμενων μεθόδων εκπαίδευσης για \textlatin{RNN}. Αυτή η αντιστοίχηση είναι σημαντική εάν θέλουμε να εκπαιδεύσουμε μεγαλύτερο αριθμό επιπέδων . Παρουσιάζεται επίσης ο τρόπο επίλυσης του προβλήματος εκχώρησης πίστωσης (\textlatin{credit assignment problem}) με πολλά επίπεδα και επίσης τα κύρια ζητήματα με τα πολλαπλά στρώματα νευρωνικών δικτύων αιχμών που κάνουν την εκπαίδευση πολύ πιο δύσκολη από τα "παραδοσιακά" νευρωνικά δίκτυα. 

\subsection{Αντιστοίχηση των \textlatin{RNN} με \textlatin{SNN}}

Σαν ευρύτερη έννοια, τα \textlatin{RNN} είναι δίκτυα των οποίων η κατάσταση εξελίσσεται με την πάροδο του χρόνου σύμφωνα με ένα σύνολο επαναλαμβανόμενων δυναμικών εξισώσεων. Μια τέτοια δυναμική επανάληψη μπορεί να συμβεί με την παρουσία επαναλαμβανόμενων συναπτικών συνδέσεων μεταξύ των νευρώνων στο δίκτυο. Στη σημερινή έρευνα μηχανικής μάθησης αυτός είναι ο πιο συνηθισμένος ορισμός του τι είναι το \textlatin{RNN}. Αυτό συμβαίνει, για παράδειγμα, όταν χρησιμοποιούνται μοντέλα νευρώνων ή συνάψεων με εσωτερική δυναμική. Αυτές οι δυναμικές είναι εγγενώς επαναλαμβανόμενες αφού η κατάσταση του δικτύου σε ένα δεδομένο χρονικό βήμα εξαρτάται επαναλαμβανόμενα από την κατάστασή του σε προηγούμενα χρονικά βήματα. Εδώ ως συνήθως χρησιμοποιείται το μοντέλο νευρώνων \textlatin{LIF} ( για την τάση της μεμβράνης). Ωστόσο, μπορεί να προστεθεί ένας επιπλέον όρος για επαναλαμβανόμενες συνδέσεις στην εξίσωση συναπτικού ρεύματος:
\selectlanguage{english}
\begin{equation}
\label{eqn:surrogate_current_dynamics}
\frac{\mathrm{d} I_{i}^{(l)}}{\mathrm{d} t}=-\underbrace{\frac{I_{i}^{(l)}(t)}{\tau_{\mathrm{syn}}}}_{\text {exp. decay }}+\underbrace{\sum_{j} W_{i j}^{(l)} S_{j}^{(l-1)}(t)}_{\text {feed-forward }}+\underbrace{\sum_{j} V_{i j}^{(l)} S_{j}^{(l)}(t)}_{\text {recurrent }}
\end{equation}
\selectlanguage{greek}

Tο άθροισμα περνάει από όλους τους προσυναπτικούς νευρώνες $j$ και $W_{i j}^{(l)}$ είναι τα αντίστοιχα προσαγωγικά βάρη από το παρακάτω επίπεδο. Το $V_{i j}^{(l)}$ αντιστοιχεί σε ρητές επαναλαμβανόμενες συνδέσεις σε κάθε επίπεδο, αυτό αντιστοιχεί στον επιπλέον όρο για τον οποίο μιλάμε. Οι ερευνητές δηλώνουν επίσης ότι με επαναλαμβανόμενες συνδέσεις ένας μόνο νευρώνας \textlatin{LIF} μπορεί να προσομοιωθεί με δύο γραμμικές διαφορικές εξισώσεις των οποίων οι αρχικές συνθήκες αλλάζουν αμέσως κάθε φορά που εμφανίζεται μια ακίδα. Έτσι, ο όρος επαναφοράς μπορεί να εισαχθεί στην εξίσωση δυναμικού της μεμβράνης ως ένας επιπλέον όρος που μειώνει ακαριαία το δυναμικό της μεμβράνης κατά $\left(\vartheta-U_{\latintext {rest }}\right)$ κάθε φορά που ο νευρώνας πυροδοτείται :
\selectlanguage{english}
\begin{equation}
\label{eqn:surrogate_membrane_dynamics}
\frac{\mathrm{d} U_{i}^{(l)}}{\mathrm{d} t}=-\frac{1}{\tau_{\mathrm{mem}}}\left(\left(U_{i}^{(l)}-U_{\text {rest }}\right)+R I_{i}^{(l)}\right)+S_{i}^{(l)}(t)\left(U_{\text {rest }}-\vartheta\right)
\end{equation}
\selectlanguage{greek}
Οι παραπάνω εξισώσεις πρέπει να προσεγγίζονται διακριτά για προσομοίωση σε υπολογιστή. Επίσης, ο συρμός εξόδου $S_{i}^{(l)}[n]$ του νευρώνα $i$ στο επίπεδο $l$ στο χρονικό βήμα $n$ πρέπει να εκφραστεί ως μη γραμμική συνάρτηση της τάσης μεμβράνης $S_{i}^{(l)}[n] \equiv \Theta\left(U_{i}^{(l)}[n]-\vartheta\right)$ όπου $\Theta$ υποδηλώνει τη βηματική συνάρτηση \textlatin{Heaviside} και $\vartheta$ αντιστοιχεί στην οριακή τιμή πυροδότησης. Το χρονικό βήμα προσομοίωσης πρέπει επίσης να είναι μικρό για να λειτουργήσει σωστά η προσέγγιση. Η εξίσωση \ref{eqn:surrogate_current_dynamics} γίνεται:
\selectlanguage{english}
\begin{equation}
I_{i}^{(l)}[n+1]=\alpha I_{i}^{(l)}[n]+\sum_{j} W_{i j}^{(l)} S_{j}^{(l)}[n]+\sum_{j} V_{i j}^{(l)} S_{j}^{(l)}[n]
\end{equation}
\selectlanguage{greek}
Ρυθμός κορεσμού: $\alpha \equiv \exp \left(-\frac{\Delta_{t}}{\tau_{\mathrm{syn}}}\right) .$  Για πεπερασμένα και θετικά $\tau_{\mathrm{syn}}$:$0<\alpha<1$. Επίσης, $S_{j}^{(l)}[n] \in\{0,1\} .$ Η μεταβλητή $n$ χρησιμοποιείται για να δηλώσει το χρονικό βήμα. Η εξίσωση \ref{eqn:surrogate_membrane_dynamics} είναι τώρα:
\begin{equation}
U_{i}^{(l)}[n+1]=\beta U_{i}^{(l)}[n]+I_{i}^{(l)}[n]-S_{i}^{(l)}[n]
\end{equation}
$\beta \equiv \exp \left(-\frac{\Delta_{t}}{\tau_{\text {mem }}}\right)$
Με αυτές τις δύο εξισώσεις, η κατάσταση του νευρώνα $i$ μπορεί να βρεθεί από τα στιγμιαία συναπτικά ρεύματα $I_{i}$ και την τάση της μεμβράνης $U_{i}$ (Πλαίσιο. 1 $]. $ Οι υπολογισμοί που απαιτούνται για την ενημέρωση της κατάστασης του κελιού μπορεί να ξετυλιχτεί στο χρόνο όπως φαίνεται στο υπολογιστικό γράφημα στο σχήμα \ref{fig:computationgraph}.
\begin{figure}[htp]
    \centering
    \includegraphics[width=6cm]{Learning Methods/surrogate/computationgraph.jpg}
    \caption{}
    \label{fig:computationgraph}
\end{figure}
Αυτή είναι μια συνηθισμένη μέθοδος απεικόνισης που χρησιμοποιείται για επαναλαμβανόμενα νευρωνικά δίκτυα, οπότε η αντιστοίχηση των \textlatin{SNN} σε \textlatin{RNN} είναι πολύ βολική εδώ. Τα βήματα χρόνου ρέουν από αριστερά προς τα δεξιά.
\begin{itemize}
    \item  Οι αιχμές εισόδου $\mathbf{S}^{(0)}$ εισάγονται στο κάτω μέρος και διαδίδονται προς τα πάνω σε υψηλότερα επίπεδα.
    \item  Τα συναπτικά ρεύματα $\mathbf{I}$ δέχονται κορεσμό $\alpha$ σε κάθε βήμα και τροφοδοτούνται στα δυναμικά της μεμβράνης  $\mathbf{U}$. Ο κορεσμός των $\mathbf{U}$ με την πάροδο του χρόνου χαρακτηρίζονται από $\beta .$
    \item Οι συρμοί αιχμών $S$ δημιουργούνται εφαρμόζοντας μια μη γραμμικότητα ορίου στα δυναμικά της μεμβράνης $\mathrm{U}$ σε κάθε χρονικό βήμα.
    \item  Οι αιχμές επηρεάζουν αιτιακά την κατάσταση του δικτύου (κόκκινες συνδέσεις).
    
\end{itemize}
Οι ερευνητές μας ενημερώνουν επίσης για το πώς μπορούν να επικοινωνηθούν οι αιχμές μέσα στο δίκτυο.
Πρώτον, κάθε ακίδα προκαλεί επαναφορά του δυναμικού μεμβράνης του νευρώνα που εκπέμπει την ακίδα. Δεύτερον, κάθε ακίδα μπορεί να τροφοδοτηθεί στον ίδιο νευρωνικό πληθυσμό μέσω επαναλαμβανόμενων συνδέσεων $\mathbf{V}^{(1)}$. Μπορεί επίσης να τροφοδοτηθεί $\mathbf{W}^{(2)}$ σε ένα επίπεδο δικτύου πιο κάτω ή, να τροφοδοτηθεί σε ένα επίπεδο ανάγνωσης στο οποίο ορίζεται μια συνάρτηση κόστους.
\subsection{Πρόβλημα εκχώρησης πίστωσης}
Όπως σε κάθε κανόνα εκμάθησης, το πρώτο βήμα είναι να επιλέξουμε μια συνάρτηση κόστους/απώλειας που μειώνεται όταν το δίκτυο αρχίσει να μαθαίνει αυτό που θέλουμε να μάθει. Το δεύτερο βήμα είναι να επιλέξουμε πώς τα βάρη των συνδέσεων ενημερώνονται κατά τη διάρκεια της εκπαίδευσης για να μειώσουμε τη τιμή της συνάρτησης κόστους/απώλειας. Αυτό ονομάζεται πρόβλημα εκχώρησης πίστωσης. Ένας τρόπος για να λυθεί αυτό το πρόβλημα είναι να χρησιμοποιηθεί η ανάθεση χωρικής πίστωσης με \textlatin{back-propagation} η οποία έχει σημαντικό κόστος μνήμης καθώς οι παράγωγοι πρέπει να επικοινωνηθούν ξανά στο δίκτυο αφού αποθηκεύσουμε όλες τις καταστάσεις νευρώνων. Ας δούμε πώς λειτουργεί το \textlatin{back-propagation} σε επαναλαμβανόμενα νευρωνικά δίκτυα (\textlatin{RNN}):
Το \textlatin{back-propagation} στα \textlatin{RNN} μπορεί να εφαρμοστεί με "ξετύλιγμα": δημιουργείται ένα βοηθητικό δίκτυο κάνοντας αντίγραφα του δικτύου για κάθε χρονικό βήμα. Ο ίδιος κανόνας μπορεί να εφαρμοστεί στα \textlatin{SNN}. Σε αυτήν την περίπτωση η επανάληψη είναι ξετυλιγμένη (σχήμα \ref{fig:unroll} ) που σημαίνει ότι δημιουργείται ένα βοηθητικό δίκτυο δημιουργώντας αντίγραφα του δικτύου για κάθε χρονικό βήμα.
\begin{figure}[htp]
    \centering
    \includegraphics[width=6cm]{Learning Methods/surrogate/unroll.PNG}
    \caption{}
    \label{fig:unroll}
\end{figure}


Το ξετυλιγμένο δίκτυο είναι απλά ένα βαθύ δίκτυο με κοινά βάρη τροφοδοσίας $\mathbf{W}^{(l)}$ και επαναλαμβανόμενα βάρη $\mathbf{V}^{(l)}$, στα οποία ισχύει το τυπικό \textlatin{BP}:
\selectlanguage{english}
\begin{equation}
\Delta W_{i j}^{(l)} \propto \frac{\partial}{\partial W_{i j}^{(l)}} \mathcal{L}[n]=\sum_{m=0}^{t} \delta_{i}^{(l)}[m] y_{j}^{(l-1)}[m]
\end{equation}
\begin{equation}
\Delta V_{i j}^{(l)} \propto \frac{\partial}{\partial V_{i j}^{(l)}} \mathcal{L}[n]=\sum_{m=1}^{t} \delta_{i}^{(l)}[s] y_{j}^{(l)}[m-1]
\end{equation}
\begin{equation}
\delta_{i}^{(l)}[n]=\sigma^{\prime}\left(a_{i}^{(l)}[n]\right)\left(\sum_{k} \delta_{k}^{(l+1)}[n] W_{i k}^{\top, l}+\sum_{k} \delta_{k}^{(l)}[n+1] V_{i k}^{\top, l}\right)
\end{equation}
\selectlanguage{greek}
Οι υπεργραφές $l=0, \ldots, L$ δηλώνουν το επίπεδο $(0$ είναι είσοδος, $L$ είναι έξοδος). Με $\alpha_{i}^{(l)}[n]=\sum_{j} W_{i j} x_{j}$ δηλώνουμε τη συνολική είσοδο στον νευρώνα \textlatin{i}, $y_{j}$ είναι η έξοδος του νευρώνα \textlatin{j} και $\eta$ ένας μικρός ρυθμός εκπαίδευσης. Επίσης, $\sigma^{\prime}$ είναι η παράγωγος της συνάρτησης ενεργοποίησης και το $\delta_{i}^{(l)}$ είναι το σφάλμα του νευρώνα εξόδου $i$ και $T$ δηλώνει τον ανάστροφο πίνακα. Η εφαρμογή \textlatin{BP} σε ένα ξετυλιγμένο δίκτυο ονομάζεται \textlatin{BackPropagation Through Time} (\textlatin{BPTT}).

Ο δεύτερος τρόπος επίλυσης αυτού του προβλήματος είναι η χρήση μιας χρονικής ανάθεσης πίστωσης (σχήμα \ref{fig:computationgraph}),που θεωρείται πρόβλημα εκχώρησης πιστώσεων στο χρόνο. Αυτός ο τύπος χρονικής ανάθεσης μπορεί να υποδιαιρεθεί περαιτέρω στις μεθόδους "προς τα πίσω" και "προς τα εμπρός".
\begin{enumerate}
  
    \item προς τα πίσω: Μπορούμε να χρησιμοποιήσουμε το \textlatin{BPTT} όπως στην ανάθεση χωρικής πίστωσης, διαδίδουμε τα σφάλματα αντίστροφα στον χρόνο μετά από μια προώθηση.
    \item προς τα εμπρός: Όλες οι απαραίτητες πληροφορίες για τον υπολογισμό των παράγωγων διαδίδονται προς τα εμπρός \cite{williams1989}. Για παράδειγμα, η "παράγωγος προς τα εμπρός" του βάρους τροφοδοσίας $\mathbf{W}$ γίνεται:
\end{enumerate}
\selectlanguage{english}
\begin{equation*}
\Delta W_{i j}^{m} \propto \frac{\partial \mathcal{L}[n]}{\partial W_{i j}^{m}}=\sum_{k} \frac{\partial \mathcal{L}[n]}{\partial y_{k}^{(L)}[n]} P_{i j k}^{L, m}[n]
P_{i j k}^{(l, m)}[n]=\frac{\partial}{\partial W_{i j}^{m}} y_{k}^{(l)}[n]
\end{equation*}
\begin{equation*}
P_{i j k}^{(l, m)}[n]=\sigma^{\prime}\left(a_{k}^{(l)}[n]\right)\left(\sum_{j^{\prime}} V_{i j^{\prime}}^{(l)} P_{i j j^{\prime}}^{(l, m)}[n-1]+\sum_{j^{\prime}} W_{i j^{\prime}}^{(l)} P_{i j j^{\prime}}^{(l-1, m)}[n-1]+\delta_{l m} y_{i}^{(l-1)}[n-1]\right)
\end{equation*}
\selectlanguage{greek}

Οι παράγωγοι σε σχέση με τα επαναλαμβανόμενα βάρη $V_{i j}^{(l)}$ μπορούν επίσης να υπολογιστούν με παρόμοιο τρόπο. Η μέθοδος προς τα πίσω είναι πιο αποτελεσματική από άποψη υπολογισμού, αλλά απαιτεί τη διατήρηση όλων των εισόδων και των ενεργοποιήσεων για κάθε χρονικό βήμα. Έτσι, η πολυπλοκότητα χώρου για κάθε στρώμα είναι $O(N T)$, όπου $N$ είναι ο αριθμός των νευρώνων ανά επίπεδο και $T$ είναι ο αριθμός των χρονικών βημάτων. Η μέθοδος προώθησης προς τα μπροστά απαιτεί τη διατήρηση των μεταβλητών $P_{i j k}^{(l, m)}$, με αποτέλεσμα την πολυπλοκότητα $O\left(N^{3}\right)$ ανά επίπεδο. Ωστόσο, εάν εφαρμοστούν απλοποιήσεις, αυτή η πολυπλοκότητα μπορεί να μειωθεί στο O (N), όπως στον κανόνα μάθησης \textlatin{Decolle} που περιγράφουμε αργότερα. Οι απλουστεύσεις μπορούν επίσης να μειώσουν την υπολογιστική πολυπλοκότητα.
Ωστόσο, οι παραπάνω αλγόριθμοι δεν μπορούν να εφαρμοστούν άμεσα. Ένα ζήτημα είναι η μη-παραγωγισιμή και μη γραμμικότητα της ακίδας. Η παράγωγος της συνάρτησης νευρικής ενεργοποίησης $\sigma^{\prime} \equiv \frac{\partial y_{i}^{(l)}}{\partial a_{i}^{(l)}}$ είναι  πρόβλημα γιατί για έναν νευρώνα ακίδας, έχουμε $S(U(t))=\Theta(U(t)-\vartheta)$, του οποίου η παράγωγος είναι μηδενική παντού εκτός από το $U=\vartheta$, όπου είναι ασαφές . Το τυπικό \textlatin{BP} εκτός από τον ακριβό υπολογισμό, τις απαιτήσεις επικοινωνίας μνήμης, δεν είναι κατάλληλη για νευρομορφικό υλικό ούτε είναι βιολογικά ρεαλιστικό. Αυτός ο τύπος υλικού έχει απαιτήσεις τοπικότητας που απαγορεύουν τo \textlatin{BP}. Η μέθοδος προς τα εμπρός μπορεί να είναι πιο εφαρμόσιμη, ωστόσο η κλιμάκωση των παραπάνω μεθόδων δεν είναι κατάλληλη για πολλά μοντέλα \textlatin{SNN} .
Η επίλυση του πρώτου έχει πολλές προσεγγίσεις:

\begin{enumerate}
\item Χρησιμοποιώντας βιολογικά εμπνευσμένους κανόνες τοπικής μάθησης για τις κρυφές ενότητες
\item Μετατροπή συμβατικά εκπαιδευμένων νευρωνικών δικτύων "βάσει ποσοστών" σε \textlatin{SNN}.
\item Εξομάλυνση του μοντέλου δικτύου ώστε να είναι συνεχώς διαφορικό.
\item Ορισμός υποκατάστατης παραγώγου (\textlatin{Surrogate Gradient}-\textlatin{SG}) ως συνεχής χαλάρωση
\end{enumerate}

Η κύρια συμβολή αυτής της έρευνας είναι για την τελευταία προσέγγιση, χρησιμοποιώντας υποκατάστατες παράγωγων αλλά επίσης μας ενημερώνει για τις μεθόδους εξομάλυνσης. Ωστόσο, μας ενδιαφέρει μόνο ο ορισμός της προσέγγισης υποκατάστατης παραγώγου.
\subsection{Προσέγγιση υποκατάστατων παράγωγων}
Η προσέγγιση υποκατάστατων παράγωγων μπορεί να χωριστεί περαιτέρω σε δύο προσεγγίσεις:
\begin{enumerate}
    \item \textlatin{SG} που συνθέτουν μια συνεχή χαλάρωση
της μη ομαλής ,μη γραμμικής συνάρτησης ακίδας. Αυτό δεν επηρεάζει τον αλγόριθμο βελτιστοποίησης.\ref{fig:sg}
    \item \textlatin{SG} που επηρεάζουν το "\textlatin{locality}" των ίδιων των υποκείμενων αλγορίθμων βελτιστοποίησης για τη βελτίωση της υπολογιστικής ή/και πρόσβασης στη μνήμη των γενικών διαδικασιών μάθησης.
\end{enumerate}
Ένα σημαντικό πλεονέκτημα με τα \textlatin{Surrogate Gradients} είναι οτι δεν χρειάζεται να καθορίσουμε ποια μέθοδος κωδικοποίησης θα χρησιμοποιηθεί στα κρυμμένα επίπεδα. Στην πρώτη προσέγγιση οι ερευνητές  αντικαθιστούν την παράγωγο της συνάρτησης ακίδας με την παράγωγο μιας ομαλής συνάρτησης. Αυτή η προσέγγιση χρησιμοποιείται στο \textlatin{Decolle} και είναι αρκετά απλή στην εφαρμογή και εύχρηστη για εφαρμογή, καθώς μπορεί να συνδυαστεί με εργαλεία αυτόματης διαφοροποίησης. Ο αλγόριθμος \textlatin{SuperSpike} χρησιμοποιεί έναν κανόνα μάθησης τριών παραγόντων σε πραγματικό χρόνο (\textlatin{online}) χρησιμοποιώντας ένα γρήγορο σιγμοειδές για την κατασκευή ενός \textlatin{SG}. Τα \textlatin{Surrogate Gradients} χρησιμοποιούνται επίσης στο \textlatin{e-Prop} το οποίο θα εξετάσουμε μετά το \textlatin{Decolle}.
\subsection{Προβλήματα των υποκατάστατων παράγωγων}

Καθώς η χρήση των \textlatin{SG} για την εκπαίδευση των \textlatin{SNN} προχωρά σε βαθύτερες αρχιτεκτονικές, πιθανότατα να προκύψουν περισσότερα θέματα, παρόμοια με αυτά που εμφανίζονται στα \textlatin{ANN}. \textlatin{SGs} που σχηματίζονται από συναρτήσεις σιγμοειδούς ενεργοποίησης, οι οποίες έχουν προβλήματα "εξαφάνισης παραγώγων" (\textlatin{vanishing gradients}). Ένα άλλο ζήτημα είναι η πιθανή προκατάληψη (\textlatin{bias}) που εισάγουν τα \textlatin{SG} στη δυναμική της μάθησης.

\begin{figure}[htp]
    \centering
    \includegraphics[width=6cm]{Learning Methods/surrogate/SG.jpg}
    \caption{Για περισσότερες λεπτομέρειες, ανατρέξτε στη πρωτότυπη έρευνα των \textlatin{Surrogate Gradient} \cite{neft2019}. Αυτό που είναι σημαντικό εδώ είναι να δούμε πώς τα \textlatin{SG} μας επιτρέπουν να έχουμε μια συνεχή συνάρτηση.}
    \label{fig:sg}
\end{figure}
\subsection{Συμπεράσματα}
Σε αυτό το κεφάλαιο εισαγάγαμε τον αναγνώστη στα κύρια ζητήματα στην εκπαίδευση των νευρωνικών δικτύων και πώς αυτά εμποδίζουν τα \textlatin{SNN} να φτάσουν την ικανότητα των ANN. Η αντιστοίχιση σε \textlatin{RNN} επιτρέπει στα \textlatin{SNN} να είναι πιο προσιτά σε μηχανικούς μηχανικής μάθησης που δεν έχουν νευροεπιστημονικό υπόβαθρο και καθιστά ευκολότερη την κατανόηση των προβλημάτων κλιμάκωσης σε μεγάλους όγκους δεδομένων . Αυτό το κεφάλαιο δεν εισάγει κάποιο νέο αλγόριθμο εκμάθησης , αλλά μας επιτρέπει να εφαρμόσουμε \textlatin{BPTT} με τη χρήση \textlatin{SG}. Τα \textlatin{SG} είναι μια πολύ επιτυχημένη προσέγγιση στην επίλυση του ζητήματος μη διαφοροποιήσης, όπως φαίνεται από τον αριθμό των νέων αλγορίθμων \textlatin{SNN}  που τα περιλαμβάνουν. Περιλάβαμε αυτήν την υποενότητα διότι όλοι οι ακόλουθοι αλγόριθμοι στη διατριβή μας περιλαμβάνουν \textlatin{SG} και γιατί θα εκπαιδεύσουμε δίκτυα \textlatin{SNN} με \textlatin{BPTT} με χρήση \textlatin{SG}.

\section{\textlatin{SuperSpike}: Εποπτευόμενη μάθηση σε πολυστρωματικά νευρωνικά δίκτυα αιχμών}
Το \textlatin{Superspike} είναι μια προσέγγιση υποκαταστάσης παραγωγού,  τριών παραγόντων που βασίζεται στην τάση.Ενας κανόνας μάθησης ικανός να εκπαιδεύσει δίκτυα ντετερμινιστικών νευρώνων \textlatin{LIF} για να εκτελούν μη γραμμικούς υπολογισμούς σε χωροχρονικά μοτίβα αιχμών. Με τη μετάφραση του \textlatin{backpropagation} στον τομέα των αιχμών, είναι μία από τις λίγες πρωτοβουλίες για την αντιμετώπιση της δυσκολίας εκπαίδευσης των \textlatin{SNN }με κρυφές μονάδες για την επεξεργασία ακριβών χρονομετρημένων συρμών αιχμών εισόδου και εξόδου.

Η μερική παράγωγος αυτής της προσέγγισης είναι της μορφής $\partial S_{i}(t) / \partial w_{i j}$ όπου $S_{i}(t)=\sum_{k} \delta\left(t-t_{i}^{k}\right)$ είναι ο συρμός αιχμών του κρυμμένου νευρώνα $i$ και $w_{i j}$ είναι ένα κρυφό βάρος.

Σε σύγκριση με άλλες μεθόδους στη βιβλιογραφία, το \textlatin{Superspike} επιτρέπει σε δίκτυα πολλαπλών στρωμάτων ντερεμινιστικών νευρώνων \textlatin{LIF} να επιλύουν εργασίες που περιλαμβάνουν μετασχηματισμούς μοτίβου χωροχρονικών αιχμών χωρίς ανάγκη για έγχυση θορύβου, ακόμη και όταν οι κρυφές μονάδες είναι αρχικά εντελώς αθόρυβες. Αντί του μετασυναπτικού συρμού αιχμών, η μερική παράγωγος των εξόδων της κρυφής μονάδας προσεγγίζεται ως το προϊόν του φιλτραρισμένου συρμού αιχμών και μια μη γραμμική συνάρτηση της μετασυναπτικής τάσης.

\subsection{Κανόνας μάθησης}
Είναι επιθυμητό ένας μεμονωμένος νευρώνας \textlatin{LIF} να εκπέμπει δεδομένο σειρμό αιχμών εξόδου $\hat{S}_{i}$ για μια δεδομένη είσοδο. Αυτό το πρόβλημα μπορεί να θεωρηθεί πρόβλημα βελτιστοποίησης της ελαχιστοποίησης της απόστασης \textlatin{van Rossum} \cite{rossum2001} μεταξύ $\hat{S}_{i}$ και του πραγματικού σειρμού αιχμών εξόδου $S_{i}$. Αρχικά, ας περιγράψουμε τι είναι η απόσταση του \textlatin{van Rossum} . Η απόσταση \textlatin{van Rossum} στοχεύει στην επίλυση του προβλήματος της διάκρισης μεταξύ δύο σειρμών αιχμών . Ο  \textlatin{van Rossum} εισήγαγε ένα μέτρο για την απόσταση μεταξύ των δυο αυτών σειρμών.
Με στόχο ένα απλό μέτρο απόστασης, δεδομένου ενός σειρμού αιχμών με χρόνους πυροδότησης $t_{i}$
\selectlanguage{english}
\begin{equation}
f^{\text {orig }}(t)=\sum_{i}^{M} \delta\left(t-t_{i}\right)
\end{equation}
\selectlanguage{greek}
όπου υποτίθεται ότι όλα τα $t_{i}>0$. Η συνάρτηση δέλτα που σχετίζεται με κάθε ακίδα αντικαθίσταται με μια εκθετική συνάρτηση, δηλαδή μια εκθετική "ουρά" προστίθεται σε όλες τις αιχμές,
\selectlanguage{english}
\begin{equation}
f(t)=\sum_{i}^{M} H\left(t-t_{i}\right) e^{-\left(t-t_{i}\right) / t_{c}}
\end{equation}
\selectlanguage{greek}
\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{Learning Methods/superspike/vanrossum/vanrossum.jpg}
    \caption{(Πάνω) Δύο συρμοί αιχμών (το ένα αναποδογυρισμένο) συνελλίσονται με εκθετικό με σταθερά χρόνου $t_{c}$. (Κάτω) Η διαφορά στο τετράγωνο των σειρμών. Το ολοκλήρωμα της καμπύλης δίνει την επιθυμητή απόσταση
    \cite{rossum2001}
}
    \label{fig:vanrossum}
\end{figure}

$t_{c}$ είναι η χρονική σταθερά της εκθετικής συνάρτησης και $H$ είναι η βηματική συνάρτηση \textlatin{Heaviside}  $(H(x)=0$ εάν $x<0$ καί $H(x)=1$ εάν $x \geq 0) .$ . Οποιαδήποτε συνάρτηση μπορεί να χρησιμοποιηθεί για συνέλιξη με άλλη από την εκθετική, αλλά η εκθετική επιλέχθηκε λόγω της βιολογικής της εγγύτητας. Η απόσταση μεταξύ δύο συρμών $f$ και $g$ ορίζεται ως (βλέπε Εικ. \ref{fig:vanrossum})
\selectlanguage{english}
\begin{equation}
D^{2}(f, g)_{t_{c}}=\frac{1}{t_{c}} \int_{0}^{\infty}[f(t)-g(t)]^{2} d t
\end{equation}
\selectlanguage{greek}
Η απόσταση είναι η Ευκλείδεια απόσταση των δύο φιλτραρισμένων συρμών ($t_{c}$ ως ελεύθερη παράμετρος).
Για να καταλάβουμε την έννοια αυτής της απόστασης ας δούμε το ακόλουθο παράδειγμα. Λαμβάνουμε υπόψη μας τις δυο οριακές τιμές του $t_{c}.$ Για $t_{c}$ πολύ μικρότερα από το μεσοδιάστημα των αιχμών, οι επιχρισμένες συναρτήσεις $f$ και $g$ συνεισφέρουν στο ολοκλήρωμα μόνο εάν οι αιχμές δεν απέχουν περισσότερο από $t_{c}$. Αυτό μας θυμίζει τον εντοπισμό συμπτώσεων (\textlatin{"coincidence detection"}). Για του περισσότερους συρμούς αιχμών, οι συμπίπτουσες αιχμές μπορούν να αγνοηθούν στο όριο του μηδενικού $t_{c}$. έτσι, αν το $f$ περιέχει $M$ και η $g$ περιέχει $N$ αιχμές, έχουμε:
\selectlanguage{english}
\begin{equation}
\lim _{t_{c} \rightarrow 0} D^{2}(f, g)_{t_{c}}=\frac{1}{t_{c}} \int_{0}^{\infty}\left[f^{2}(t)+g^{2}(t)\right] d t=\frac{M+N}{2}
\end{equation}
\selectlanguage{greek}
Αυτή η απόσταση βασικά μετρά τις μη συμπίπτουσες αιχμές.
Ωστόσο, για μεγάλα $t_{c}$, η κύρια συνεισφορά στο ολοκλήρωμα προέρχεται από χρονικές στιγμές που η τελευταία ακίδα έχει ήδη συμβεί αλλά ο εκθέτης δεν έχει ακόμη υποστεί πλήρης κόρεσμο. Το ολοκλήρωμα επηρεάζεται κυρίως από τις χρονικές στιγμές που έχει περάσει η τελευταία ακίδα αλλά ο εκθετικός όρος δεν έχει ακόμη υποστεί πλήρης κορεσμό. Υποθέτοντας ότι το $f(g)$ περιέχει $M(N)$ αιχμές, μπορεί κανείς να κάνει την εξής προσέγγιση
\selectlanguage{english}
\begin{equation}
\lim _{t_{c} \rightarrow \infty} D^{2}(f, g)_{t_{c}}=\frac{1}{t_{c}} \int_{0}^{\infty}\left(M e^{-t / t_{\mathrm{c}}}-N e^{-t / t_{c}}\right)^{2} d t=\frac{(M-N)^{2}}{2}
\end{equation}
\selectlanguage{greek}
Σε αυτό το όριο, το $D$ μετρά τη διαφορά στο συνολικό αριθμό αιχμών. Το ανώτατο όριο του ολοκληρώματος πρέπει να θεωρηθεί ως το άπειρο, καθώς μας ενδιαφέρει η ουρά της τελευταίας ακίδας που υπέστη πλήρης κορεσμό και όχι ο χρόνος της τελευταίας ακίδας.

Η απόσταση παρεμβάλλεται έτσι μεταξύ των δύο άκρων της ανίχνευσης σύμπτωσης και της μέτρησης της διαφοράς στο συνολικό αριθμό αιχμών. Μετά το φιλτράρισμα, οι συρμοί αιχμών αφήνουν ένα ποσό $N+M$ όρων.
Η αλλαγή των μεταβλητών ολοκλήρωσης οδηγεί σε μια εναλλακτική έκφραση για την απόσταση,
\selectlanguage{english}
\begin{equation}
D^{2}(f, g)=\frac{1}{2} \int_{-\infty}^{\infty} C_{f-g, f-g}(t) e^{-|t| t_{c}} d t
\end{equation}
\selectlanguage{greek}

όπου $C_{f-g, f-g}(t)$ είναι η αυτοσυσχέτιση της διαφοράς των αρχικών συρμών, $f^{\latintext {orig }}(t)-g^{\latintext {orig }}(t) .$ Αυτό δείχνει ότι η απόσταση μπορεί να αναπαρασταθεί ως σταθμισμένο ολοκλήρωμα επί της αυτοσυσχέτισης, με τη στάθμιση να ποικίλλει με $t_{c}$. Αποδεικνύει επίσης ότι η απόσταση δεν επηρεάζεται από την αντιστροφή του χρόνου, δηλαδή, η απόσταση θα ήταν η ίδια εάν οι εκθετικές ουρές ήταν προσαρτημένες στις αιχμές στις αντίθετες πλευρές. Όσον αφορά την επιλογή του εκθετικού, η συνέλιξη σε υψηλότερη -τάξης νευρώνα για μικρές και μεσαίες περιόδους μπορεί να διαβαστεί ως μετασυναπτικά δυναμικά. Μεγαλύτερο $t_{c}$ φαίνεται να εξυπηρετείται καλύτερα από βραδύτερα ρεύματα που προκαλούν δεύτεροι αγγελιοφόροι ή παρουσία ασβεστίου .
Τώρα ας συνεχίσουμε με το πρόβλημα βελτιστοποίησης της ελαχιστοποίησης της απόστασης \textlatin{van Rossum} \cite {rossum2001} μεταξύ $\hat{S}_{i}$ και του πραγματικού συρμού εξόδου $S_{i}$:
\begin{equation}
L=\frac{1}{2} \int_{-\infty}^{t} d s\left[\left(\alpha * \hat{S}_{i}-\alpha * S_{i}\right)(s)\right]^{2}
\end{equation}

όπου $\alpha$ είναι ένας κανονικοποιημένος ομαλός πυρήνας χρονικής συνέλιξης. Επειδή μπορούν να υπολογιστούν εύκολα σε πραγματικό χρόνο και να εφαρμοστούν ως ηλεκτρικά ή χημικά ίχνη στη νευροβιολογία, χρησιμοποιούνται διπλοί εκθετικοί αιτιώδεις πυρήνες. Υπολογισμός της παραγώγου της παραπάνω εξίσωσης ως προς τα συναπτικά βάρη $w_{i j}$:
\selectlanguage{english}
\begin{equation}
\frac{\partial L}{\partial w_{i j}}=-\int_{-\infty}^{t} d s\left[\left(\alpha * \hat{S}_{i}-\alpha * S_{i}\right)(s)\right]\left(\alpha * \frac{\partial S_{i}}{\partial w_{i j}}\right)(s)
\end{equation}
\selectlanguage{greek}
στο οποίο η παράγωγος $\partial S_{i} / \partial w_{i j}$ είναι η παράγωγος του συρμού αιχμών. Αυτή η παράγωγος για τα περισσότερα μοντέλα νευρώνων είναι μηδενικό εκτός από τους χρόνους αιχμών κατά τους οποίους δεν ορίζεται. Αυτό είναι ένα τεράστιο πρόβλημα για το \textlatin{backpropagation}. Συνήθως οι αλγόριθμοι εκπαίδευσης αποφεύγουν αυτό το πρόβλημα είτε εκτελώντας βελτιστοποίηση απευθείας στο δυναμικό της μεμβράνης $U_{i}$ είτε εισάγοντας θόρυβο που κάνει την πιθανότητα του συρμού $\left\langle S_{i}(t)\right\rangle$ ομαλή συνάρτηση του δυναμικού της μεμβράνης. Το \textlatin{Superspike} συνδυάζει αυτές τις δύο προσεγγίσεις αντικαθιστώντας τον συρμό αιχμών $S_{i}(t)$ με μια συνεχή βοηθητική (\textlatin{auxiliary}) συνάρτηση $\sigma\left(U_{i}(t)\right)$  του δυναμικού της μεμβράνης. Για λόγους επιδόσεων, επιλέγουμε $\sigma(U)$ ως την αρνητική πλευρά ενός γρήγορου σιγμοειδούς. Αυτή η "βοηθητική" συνάρτηση "αντικαθιστά το παράγωγο του συρμού αιχμών ως εξής:
\selectlanguage{english}
\begin{equation}
\frac{\partial S_{i}}{\partial w_{i j}} \quad \rightarrow \quad \sigma^{\prime}\left(U_{i}\right) \frac{\partial U_{i}}{\partial w_{i j}}
\end{equation}
\selectlanguage{greek}
Για να υπολογίσoυμε την παράγωγο $\partial U_{i} / \partial w_{i j}$ στην παραπάνω έκφραση, για μοντέλα \textlatin{LIF} το δυναμικό μεμβράνης  $U_{i}(t)$ ως μοντέλο απόκρισης αιχμής:
\selectlanguage{english}
\begin{equation}
U_{i}(t)=\sum_{j} w_{i j}\left(\epsilon * S_{j}(t)\right)+\left(\eta * S_{i}(t)\right)
\end{equation}
\selectlanguage{greek}

Ο πυρήνας αιτιώδους μεμβράνης $\epsilon$ αντιστοιχεί στο σχήμα του μετασυναπτικού δυναμικού ( \textlatin{postsynaptic potential}-\textlatin{PSP}) και το $\eta$ περιγράφει τη δυναμική των αιχμών και την επαναφορά του δυναμικού μεμβράνης. Λόγω του δεύτερου όρου, το $U_{i}$ εξαρτάται από το παρελθόν του μέσω της εξόδου του συρμού $S_{i}$.

Η εξάρτηση από το παρελθόν του δεν μας επιτρέπει να υπολογίσουμε την παράγωγο $\frac{{\partial U}_{i}}{\partial w_{i j}}$ απευθείας. Ωστόσο, διασφαλίζοντας χαμηλούς ρυθμούς πυροδότησης (με την προσθήκη ομοιοστατικών μηχανισμών που ρυθμίζουν τα επίπεδα νευρωνικής δραστηριότητας) μπορούμε να αγνοήσουμε τον δεύτερο όρο.

Αγνοώντας τον δεύτερο όρο, η εξίσωση γίνεται η φιλτραρισμένη προσυναπτική δραστηριότητα $\frac{\partial U_{i}}{\partial w_{i j}} \approx\left(\epsilon * S_{j}(t)\right)$. Βιολογικά, αυτό μπορεί να ερμηνευτεί ως η συγκέντρωση νευροδιαβιβαστών στη σύναψη.

Αντικαθιστώντας αυτήν την προσέγγιση πίσω στην Εξίσωση. (4.26), ο κανόνας μάθησης \textlatin{gradient descent} για έναν μόνο νευρώνα γίνεται:
\selectlanguage{english}
\begin{equation}
\label{eqn:superspike}
\frac{\partial w_{i j}}{\partial t}=r \int_{-\infty}^{t} d s \underbrace{e_{i}(s)}_{\text {Error signal }} \underbrace{\alpha * \underbrace{\sigma^{\prime}\left(U_{i}(s)\right)}_{\text {Post }} \underbrace{\left(\epsilon * S_{j}\right)(s)}_{\text {Pre }})}_{\equiv \lambda_{i j}(s)}
\end{equation}
\selectlanguage{greek}
Ο ρυθμός μάθησης συμβολίζεται με $r$ και για το σήμα σφάλματος εξόδου έχουμε $e_{i}(s) \equiv \alpha *\left(\hat{S}_{i}-S_{i}\right)$ και το \textlatin{eligibility trace} (προσωρινή
καταγραφή της εμφάνισης ενός συμβάντος) είναι $\lambda_{i j} .$ Στην πράξη, η έκφραση υπολογίζεται σε μικρές παρτίδες και προτείνεται χρήση ενός ρυθμού εκμάθησης ανά παράμετρο $r_{i j}$ για να επιταχυνθεί η μάθηση.

Η τελευταία εξίσωση αντιστοιχεί στον κανόνα μάθησης \textlatin{SuperSpike} για τον νευρώνα εξόδου $i$.
Επαναπροσδιορίζοντας το σήμα σφάλματος $e_{i}$ ως σήμα ανατροφοδότησης, μπορεί να χρησιμοποιηθεί επίσης ο ίδιος κανόνας για κρυφούς νευρώνες. Σημαντικές ιδιότητες:
\begin{itemize}
  \item Περιλαμβάνει έναν όρο \textlatin{Hebbian}  που συνδυάζει την προσυναπτική με την μετασυναπτική δραστηριότητα
  \item Ο κανόνας μάθησης βασίζεται στην τάση
  \item Είναι ένας μη γραμμικός \textlatin{Hebbian} κανόνας λόγω της εμφάνισης του $\sigma^{\prime}\left(U_{i}\right)$
  \item Η αιτιώδης συνέλιξη με το $\alpha$ λειτουργεί ως \textlatin{eligibility trace} για την επίλυση του προβλήματος της απομακρυσμένης ανταμοιβής (\textlatin{distal reward problem}) λόγω των σημάτων σφάλματος που έρχονται μετά από την εμφάνιση ενός σφάλματος στην εκπαίδευση \cite{izike2007}
  \item Είναι ένας κανόνας τριών παραγόντων στον οποίο το σήμα σφάλματος παίζει το ρόλο ενός τρίτου παράγοντα \cite{fremaux2016} . Το σήμα σφάλματος είναι συγκεκριμένο για τον μετασυναπτικό νευρώνα.
\end{itemize}

Το "πρόβλημα μακρινής ανταμοιβής" είναι ένα αιτιολογικό αίνιγμα που εμφανίζεται όταν οι ανταμοιβές φτάνουν δευτερόλεπτα μετά από γεγονότα που προκαλούν ανταμοιβή. Πώς αναγνωρίζει ο εγκέφαλος ποια μοτίβα πυροδότησης των οποίων οι νευρώνες είναι υπεύθυνοι για την ανταμοιβή εάν 1) τα μοτίβα δεν είναι πλέον εκεί όταν φτάσει η ανταμοιβή και 2) όλοι οι νευρώνες και οι συνάψεις είναι ενεργές κατά την περίοδο αναμονής της ανταμοιβής;

\subsection{Μοντέλο Νευρώνα}

Το μοντέλο νευρώνων \textlatin{LIF} με συναπτική είσοδο ρεύματος \textlatin{"current-based input"} χρησιμοποιείται στην μορφή ολοκληρώματος. Για την προσομοίωση της δυναμικής της μεμβράνης, η τάση $U_{i}$ του νευρώνα $i$ μπορεί να υπολογιστεί ως εξής:
\selectlanguage{english}
\begin{equation}
\label{eqn:lifvoltage}
\tau^{\mathrm{mem}} \frac{d U_{i}}{d t}=\left(U^{\text {rest }}-U_{i}\right)+I_{i}^{\mathrm{syn}}(t)
\end{equation}
\selectlanguage{greek}
στο οποίο το συναπτικό ρεύμα εισόδου $I_{i}^{\latintext {syn }}(t)$ αλλάζει σύμφωνα με την ακόλουθη εξίσωση:
\begin{equation}
\selectlanguage{english}
\frac{d}{d t} I_{i}^{\mathrm{syn}}(t)=-\frac{I_{i}^{\mathrm{syn}}(t)}{\tau^{\mathrm{syn}}}+\sum_{j \in \mathrm{pre}} w_{i j} S_{j}(t)
\end{equation}
\selectlanguage{greek}
Η τιμή των $I_{i}^{\mathrm{syn}}(t)$ πηδά κατά $w_{i j}$ τη στιγμή της άφιξης από προσυναπτικούς νευρώνες $S_{j}(t)=\sum_{k} \delta\left(t-t_{j}^{k}\right)$ .To $\delta$ υποδηλώνει την συνάρτηση \textlatin{Dirac} $\delta$ και τα $t_{j}^{k}(k=1,2, \cdots)$ είναι οι χρόνοι πυροδότησης νευρώνα $j .$  Για να δείτε τις τιμές των σταθερών που χρησιμοποιήθηκαν στα πειράματα θα πρέπει να κατατρεξετε στην δημοσίευση του  \textlatin{Superspike} καθώς δεν μας ενδιαφέρουν οι λεπτομέρειες εφαρμογής εδώ αλλά περισσότερο η θεωρητική βάση της μεθόδου εκμάθησης αυτής.

\begin{itemize}
    \item Ένα δυναμικό δράσης (\textlatin{action potential}) ενεργοποιείται όταν η τάση μεμβράνης του νευρώνα $i$ φτάσει σε μια τιμή πάνω από μια τιμή κατωφλίου $\vartheta$ .
    \item Μετά από μια ακίδα, η τάση $U_{i}$ παραμένει σταθερή στα $U_{i}^{\latintext {rest }}$ για $\tau^{\mathrm{ref}}$ για να εξομοίωσουμε μια πυρίμαχη περίοδο ("\textlatin{refractory period}").
    \item  Μετά τη παραγωγή αιχμών, οι αιχμές διαδίδονται σε άλλους νευρώνες με αξονική καθυστέρηση.
\end{itemize}


\subsection{Μοντέλο πλαστικότητας}

Ο κανόνας μάθησης μπορεί να ερμηνευτεί ως ένας μη γραμμικός \textlatin{ Hebbian} κανόνας τριών παραγόντων. Ο μη γραμμικός \textlatin{ Hebbian} όρος ανιχνεύει συμπτώσεις μεταξύ προσυναπτικής δραστηριότητας και μετασυναπτικής εκπόλωσης. Αυτές οι χωροχρονικές συμπτώσεις στη μεμονωμένη σύναψη  $w_{i j}$ αποθηκεύονται στη συνέχεια παροδικά από τη χρονική συνέλιξη με τον αιτιώδη πυρήνα $\alpha$. Αυτό το βήμα μπορεί να ερμηνευτεί ως ένα συναπτικό \textlatin{eligibility trace}.

Όλες οι απαραίτητες ποσότητες υπολογίζονται σε πραγματικό χρόνο χωρίς την ανάγκη να διαδοθούν σήματα σφάλματος προς τα πίσω με την πάροδο του χρόνου. Έτσι, το \textlatin{Superspike} μπορεί να ερμηνευθεί ως μια υλοποίηση επαναλαμβανόμενης μάθησης σε πραγματικό χρόνο (\textlatin{ real-time recurrent learning }-\textlatin{RTRL}) \cite{williams1989} για νευρωνικά δίκτα αιχμών.

Στο μοντέλο, όλη η πολυπλοκότητα της νευρικής ανατροφοδότησης της μάθησης απορροφάται στο σήμα ανά νευρώνα $e_{i}(t) .$  Επειδή δεν είναι σαφές εάν και πώς αποστέλλεται τέτοια εσφαλμένη ανατροφοδότηση σε μεμονωμένους νευρώνες στη βιολογία, οι συγγραφείς δοκίμασαν πολλούς τρόπους τους οποίους δεν θα περιγράψουμε σε βάθος. Η προαναφερθείσα εξίσωση του κανόνα μάθησης \textlatin{Superspike} ενσωματώνεται σε πεπερασμένα χρονικά διαστήματα πριν από την ενημέρωση των βαρών. Ο πλήρης κανόνας εκμάθησης μπορεί να γραφτεί ως εξής:

\selectlanguage{english}
\begin{equation}
\label{eqn:integratedsuperspike}
\Delta w_{i j}^{k}=r_{i j} \int_{t_{k}}^{t_{k+1}} \underbrace{e_{i}(s)}_{\text {Error signal }} \alpha *(\underbrace{\sigma^{\prime}\left(U_{i}(s)\right)}_{\text {Post }} \underbrace{\left(\epsilon * S_{j}\right)(s)}_{\text {Pre }}) d s
\end{equation}
\selectlanguage{greek}
 
O υπολογισμός της εξίσωσης του κανόνα μάθησης \textlatin{Superspike} μπορεί να περιγραφεί ως εξής: 1) αξιολόγηση προσυναπτικών ιχνών΄(\textlatin{presynaptic traces}), 2) υπολογισμός της σύμπτωσης \textlatin{Hebbian} και υπολογισμός των \textlatin{eligibility traces}, 3) υπολογισμός και διάδοση σημάτων σφάλματος και 4) ολοκλήρωση της δοσμένης εξίσωσης \textlatin{Superspike} και αναπροσαρμογής βάρους.
\subsection{Σήματα μάθησης του \textlatin{SuperSpike}}
\subsubsection{Προσυναπτικά ίχνη}
Επειδή το  $\epsilon$ είναι ένα διπλό εκθετικό φίλτρο, η χρονική συνέλιξη στην έκφραση των προσυναπτικών ιχνών (Εξ.\ref{eqn:integratedsuperspike}), μπορεί να αξιολογηθεί αποτελεσματικά σε πραγματικό χρόνο με εκθετικό φιλτράρισμα δύο φορές. Πρώτον, η ολοκλήρωση του μοναδικού εκθετικού ίχνους είναι:
\selectlanguage{english}
\begin{equation}
\frac{d z_{j}}{d t}=-\frac{z_{j}}{\tau_{\text {rise }}}+S_{j}(t)
\end{equation}
\selectlanguage{greek}
σε κάθε χρονικό βήμα το οποίο στη συνέχεια τροφοδοτείται σε μια δεύτερη εκθετική συστοιχία φίλτρων
\selectlanguage{english}
\begin{equation}
\tau_{\text {decay }} \frac{d \tilde{z}_{j}}{d t}=-\tilde{z}_{j}+z_{j}
\end{equation}
\selectlanguage{greek}
με την συνάρτηση $\tilde{z}_{j}(t) \equiv\left(\epsilon * S_{j}\right)(t)$ να προσαρμόζει το σχήμα ενός μετασυναπτικού δυναμικού.
\subsubsection{Ανίχνευση σύμπτωσης \textlatin{Hebbian}}
Για τον υπολογισμό του όρου \textlatin{Hebbian}, το \textlatin{SG} $\sigma^{\prime}\left(U_{i}\right)$ υπολογίζεται σε κάθε χρονικό βήμα.

Το εξωτερικό γινόμενο μεταξύ των αργοπορημένων προσυναπτικών ιχνών $\tilde{z}_{j}(t-\Delta)$ και των υποκατάστατων μερικών παραγώγων $\sigma^{\prime}\left(U_{i}\right)(t-\Delta)$  υπολογίζεται σε κάθε χρονικό βήμα.
\subsubsection{Συναπτικά \textlatin{eligibility traces} }
Για να υλοποιήσουμε το συναπτικό \textlatin{eligibility trace} όπως δίνεται από το χρονικό φίλτρο $\alpha$, οι τιμές του όρου \textlatin{Hebbian} φιλτράρονται με δύο εκθετικά φίλτρα z\textsubscript{\textlatin{j}}. Aυτά τα ίχνη πρέπει τώρα να υπολογιστούν για κάθε σύναψη $w_{i j}$  που κάνει την πολυπλοκότητα του αλγορίθμου $O\left(n^{2}\right)$ με $n$ να είναι ο αριθμός των νευρώνων. Για να λειτουργεί σωστά το \textlatin{Superspike}, είναι σημαντικό αυτά τα \textlatin{traces} να είναι αρκετά μεγάλα ώστε να συμπίπτουν με οποιοδήποτε σχετικό σήμα σφάλματος $e_{i}(t)$. Η διάρκεια του \textlatin{trace} στο μοντέλο δίνεται από το σχήμα του πυρήνα του φίλτρου που χρησιμοποιείται για τον υπολογισμό της απόστασης του \textlatin{van Rossum}.

\subsubsection{Σήματα σφάλματος}
Τα σήματα σφάλματος εξόδου συνδέονται με μονάδες εξόδου που έχουν συγκεκριμένο σήμα -στόχο. Τα στοιχεία τους καθορίζονται από την υποκείμενη συνάρτηση κόστους.

Στο επίπεδο των σφαλμάτων εξόδου μπορούν να χρησιμοποιηθούν δύο τρόποι για να μάθουμε την έξοδο. Για να μάθουμε ακριβείς αιχμές εξόδου, τα σήματα σφάλματος εξόδου δόθηκαν ακριβώς από $e_{i}=\alpha *\left(\hat{S}_{i}-S_{i}\right)$  για μονάδα εξόδου $i$. Όπως φαίνεται από την εξίσωση, το σήμα σφάλματος $e_{i}$ είναι μηδενικό μόνο εάν ο στόχος και ο συρμός εξόδου ταιριάζουν ακριβώς με τη χρονική ακρίβεια της προσομοίωσής μας. Όλες οι τιμές συνάρτησης κόστους μπορούν να υπολογιστούν ως μέση τετραγωνική ρίζα.

Ο δεύτερος τρόπος είναι να ταξινομήσουμε τα μοτίβα αιχμών εισόδου, εισάγωντας κάποια χαλάρωση στον υπολογισμό του σήματος σφάλματος. Η άμεση αρνητική ανατροφοδότηση σφάλματος δίνεται από $e_{i}=-\alpha * S_{i}^{\latintext {err }}$ για κάθε περιττή επιπλέον αύξηση $S_{i}^{\latintext {err }}$. Ένα θετικό σήμα ανατροφοδότησης δίνεται απο τον τύπο $e_{i}=\alpha * S_{i}^{\latintext {miss }}$ όταν ένα ερέθισμα απέτυχε να εκπέμψει μια ακίδα εξόδου όταν έπρεπε.

\subsubsection{Σήματα ανατροφοδότησης}
Τα σήματα ανατροφοδότησης, προέρχονται από σήματα σφάλματος εξόδου τροφοδοτώντας τα πίσω στις κρυφές μονάδες.

Μπορούν να χρησιμοποιηθούν διαφορετικές στρατηγικές εκχώρησης πιστώσεων για κρυφές μονάδες. Η συμμετρική, τυχαία και ομοιόμορφη ανάδραση είναι μερικά παραδείγματα σημάτων ανάδρασης.

Συμμετρική ανατροφοδότηση: υπολογίζεται ως το σταθμισμένο άθροισμα $e_{i}=\sum_{k} w_{k i} e_{k}$  των μεταγενέστερων σημάτων σφάλματος χρησιμοποιώντας τα πραγματικά βάρη προώθησης $w_{i k} .$  

Τυχαία ανατροφοδότηση: Υπολογίζεται ως η τυχαία προβολή $e_{i}=\sum_{k} b_{k i} e_{k}$ με τυχαίους συντελεστές $b_{k i}$ που προέρχονται από μια κανονική κατανομή με μηδενικό μέσο όρο και διακύμανση μονάδας ,

Ομοιόμορφη ανατροφοδότηση: όλοι οι συντελεστές στάθμισης απλώς ορίστηκαν σε ένα $e_{i}=\sum_{k} e_{k}$ που αντιστοιχεί πιο κοντά σε έναν κοινό τρίτο συντελεστή που κατανέμεται σε όλους τους νευρώνες, (διάχυτο νευροδιαμορφωτικό σήμα). Για την υλοποιήση των ενημερώσεων βάρους σε προσομοιώση διαβάστε την ενότητα 4.4.2 του \textlatin{Superspike}. 


\subsubsection{Πολυεπίπεδη Εκπαίδευση και Περιορισμοί}

Ο τύπος \ref{eqn:integratedsuperspike} απαιτεί επέκταση σε κρυμμένα επίπεδα. Μπορεί να χρησιμοποιηθεί ο ίδιος κανόνας εκμάθησης για κρυφές μονάδες, με την τροποποίηση ότι το $e_{i}(t)$ γίνεται μια περίπλοκη συνάρτηση που εξαρτάται από τα βάρη και τη μελλοντική δραστηριότητα όλων των νευρώνων. Αυτή η μη τοπικότητα στο χώρο και στο χρόνο παρουσιάζει σοβαρά προβλήματα, τόσο από άποψη βιολογικής αληθοφάνειας όσο και από τεχνικής υλοποιήσης. Τεχνικά, αυτός ο υπολογισμός απαιτεί είτε \textlatin{back-propagation} στο χρόνο μέσω του πυρήνα του μετασυναπτικού δυναμικού είτε υπολογισμό όλων των σχετικών ποσοτήτων στο \textlatin{online} όπως στην περίπτωση του \textlatin{RTRL}.
\subsection{Συμπέρασμα}
Οι συγγραφείς προτείνουν ότι αντί για ένα μόνο κοινό σήμα ανατροφοδότησης, απαιτείται ενα υψηλότερης διαστατικότητας νευροδιαμορφωτικό ή ηλεκτρικό σήμα ανατροφοδότησης για μάθηση με πιθανώς κάποια γνώση της πορείας τροφοδοσίας. Προτείνουμε ότι οι ταλαντώσεις μπορούν να εξυπηρετήσουν αυτόν τον σκοπό της ευφυούς νευροδιαμόρφωσης. Ο εγκέφαλος χρησιμοποιεί μια μέθοδο διαμερισματοποίησης υπολογισμών, η οποία μπορεί να του επιτρέψει να εκτελέσει πιο πολύπλοκες και δύσκολες εργασίες, αλλά απαιτεί επίσης την ανάγκη κεντρικού ελέγχου για την ενσωμάτωση δεδομένων από διάφορες περιοχές του εγκεφάλου. Η απόκλιση προβολής νευροδιαμορφωτή μεγάλης εμβέλειας (\textlatin{Long-range neuromodulator projection divergence}) φαίνεται να είναι κατάλληλη για τον συντονισμό της επικοινωνίας μεταξύ των περιοχών του εγκεφάλου \cite{ito2008}. Η νευρική μετάδοση χαρακτηρίζεται από ταλαντωτική δραστηριότητα του εγκεφάλου. Έτσι, η ικανότητα των νευροδιαμορφωτών να τροποποιούν τη μετάδοση σήματος με τρόπο που εξαρτάται από τη συχνότητα παρέχει έναν βαθύτερο βαθμό ελέγχου και θα μπορούσε να αποτελέσει αντικείμενο έρευνας για νευρωνικά δίκτυα αιχμών.

\section{\textlatin{Synaptic Plasticity Dynamics for Deep Continuous Local Learning}-\textlatin{Decolle}}
H μέθοδος \textlatin{Synaptic Plasticity Dynamics for Deep Continuous Local Learning} \cite{kaiser2020} επιτρέπει εκμάθηση σε πραγματικό χρόνο χρησιμοποιώντας τοπικές συναρτήσεις σφάλματος. Αυτή η ιδιότητα καθιστά αυτόν τον κανόνα εκμάθησης κατάλληλο για νευρομορφικό υλικό, καθώς δεν απαιτεί επιβάρυνση μνήμης. Οι κανόνες της συναπτικής πλαστικότητας λαμβάνονται συστηματικά από συναρτήσεις κόστους που καθορίζονται από τον χρήστη και νευρωνικές δυναμικές χρησιμοποιώντας μεθόδους αυτόματης διαφοροποίησης . Το \textlatin{BPTT} δεν είναι βιολογικά ρεαλιστικό επειδή στο \textlatin{Decolle} οι νευρώνες κάνουν τους υπολογισμούς τους τοπικά. Επίσης, η συνεχής χρονική δυναμική των \textlatin{SNN} δημιουργεί ένα πρόβλημα χρονικής ανάθεσης πίστωσης \textlatin{temporal credit assignment problem}.
\subsection{Λειτουργία του Κανόνα Μάθησης}
Το \textlatin{Decolle} μπορεί να υπολογίσει τις παραγώγους τοπικά χρησιμοποιώντας τοπικά \textlatin{readouts } ανά στρώμα \cite{neftci2017}. Η χρονική δυναμική αντιμετωπίζεται από την καθιερωμένη ισοδυναμία \textlatin{SNN} και \textlatin{RNN} όπως αναφέρεται στην ενότητα \textlatin{Surrogate Gradient Learning }\cite{neft2019}. Ο κανόνας της πλαστικότητας είναι τοπικός χρονικά επειδή το \textlatin{Decolle} είναι διαμορφωμένο με τέτοιο τρόπο ώστε οι πληροφορίες που απαιτούνται για τον υπολογισμό της παραγώγου να προωθούνται προς τα μπροστά. Αυτή η ιδέα δεν είναι εντελώς νέα, αλλά παρόμοιες μέθοδοι απαιτούν μεταβλητές ειδικής κατάστασης για κάθε σύναψη, κλιμακώνοντας έτσι τουλάχιστον τετραγωνικά με τον αριθμό των νευρώνων, όπως το \textlatin{Superspike} με τοπικούς κανόνες μάθησης. Ο \textlatin{Decolle} κλιμακώνεται γραμμικά με τον αριθμό των νευρώνων, ο οποίος απαιτεί τάξεις μεγέθους λιγότερη μνήμη. Χρησιμοποιώντας \textlatin{readouts}, μια συνάρτηση τοπικού κόστους χρονικά και χωρικά. Οι συγγραφείς επισημαίνουν την ομοιότητα με τους μηχανισμούς ανάγνωσης (\textlatin{readout mechanisms}) που χρησιμοποιούνται στα \textlatin{ Liquid State Machines} \cite{markram2002}. Οι νευρώνες ανάγνωσης σε \textlatin{ Liquid State Machines} μπορούν να μάθουν να εξάγουν πληροφορίες από ορισμένα μικροκυκλώματα και να αναφέρουν αυτές τις πληροφορίες σε άλλα κυκλώματα. Τα \textlatin{readouts} στο \textlatin{Decolle} πραγματοποιούνται σε έναν σταθερό τυχαίο συνδυασμό εξόδων νευρώνων. Οι συγγραφείς το θεωρούν επίσης μια προσέγγιση συνθετικής παραγώγου (\textlatin{synthetic gradient })με τυχαία εκκίνηση του τοπικού \textlatin{readout}.

\subsection{Πλεονεκτήματα της μεθόδου}
Το \textlatin{Decolle}, όπως και το \textlatin{SuperSpike}, χρησιμοποιεί υποκατάστατες παραγώγους για να ενημερώσει τα βάρη, αλλά σε αντίθεση με το \textlatin{SuperSpike}, η συνάρτηση κόστους είναι τοπική σε χρόνο και χώρο, απαιτώντας μόνο ένα ίχνος ανά νευρώνα εισόδου. Αυτό επιτρέπει στη μέθοδο να κλιμακωθεί γραμμικά στο χώρο. Επιπλέον, ο υπολογισμός των παράγωγων στο \textlatin{Decolle} μπορεί να επαναχρησιμοποιήσει τις μεταβλητές που υπολογίζονται για τη δυναμική προώθησης προς τα εμπρός, με αποτέλεσμα να μην υπάρχει επιπλέον επιβάρυνση μνήμης κατά τη μάθηση. Η τοπική ανάγνωση του \textlatin{Decolle} λειτουργεί ως ένα επίπεδο αποκωδικοποιητή και μαθαίνει τα εσωτερικά βάρη με τα βάρη ανάγνωσης να είναι τυχαία και σταθερά. Η εσωτερική εκπαίδευση με βάρη επιτρέπει στο δίκτυο να μαθαίνει παραστάσεις που διευκολύνουν τα επόμενα επίπεδα να ταξινομήσουν εισόδους \cite{neftci2017}. Αυτό είναι κάπως παρόμοιο με τα \textlatin{reservoir networks}, αλλά τα βάρη ανάγνωσης εκεί είναι εκπαιδεύομενα, το \textlatin{Decolle} το αποφεύγει αυτό για να μειώσει το κόστος υπολογισμού ενώ ταυτόχρονα καταφέρνει να επιτύχει μεγάλη ακρίβεια ταξινόμησης.Αυτό δεν σημαίνει οτι δεν μπορούμε να εκπαιδεύσουμε νευρωνικα δίκτυα αιχμών τύπου \textlatin{reservoir} δικτύων. Για παράδειγμα, ενα νευρωνικό δίκτυο \textlatin{Convolutional Spiking} που βασίζεται σε \textlatin{Reservoir} δίκτυα χρησιμοποιήθηκε για την εκπαίδευση ενός σύνολο δεδομένων δυναμικής όρασης με μεγάλη ακρίβεια \cite{george2020}.

Επισημαίνουμε ξανά ότι τα \textlatin{SNN} είναι επαναλαμβανόμενα (\textlatin{recurrent}) ακόμη και όταν όλες οι συνδέσεις είναι προωθημένες προς τα εμπρός, επειδή οι νευρώνες διατηρούν μια κατάσταση που διαδίδεται προς τα εμπρός σε κάθε χρονικό βήμα. Επειδή η πλήρης ακολουθία και οι καταστάσεις δραστηριότητας που προκύπτουν αποθηκεύονται για τον υπολογισμό παραγώγων, οι τεχνικές που βασίζονται σε \textlatin{BPTT} μπορούν να υπολογίσουν καλά τις παραγώγους, αλλά με κόστος στη μνήμη. Επιπλέον, στα \textlatin{SNN} πρέπει να χρησιμοποιήσουμε πολύ μεγάλο αριθμό χρονικών βημάτων για να καταγράψουμε τη χρονική δυναμική της εισόδου (συνιστάται χρόνος προσομοίωσης 1 \textlatin{ms} ή λιγότερο). Αυτό προφανώς σημαίνει ότι πρέπει να χρησιμοποιήσουμε ένα μεγάλο παράθυρο περικοπής ανάλογα με τη συχνότητα των σημαντικών συμβάντων στα δεδομένα εισόδου (1000 χρονικά βήματα εάν αυτά,τα συμβάντα, συμβαίνουν κάθε 1 δευτερόλεπτο). Το μέγεθος του \textlatin{SNN} που εκπαιδεύεται από το \textlatin{BPTT} περιορίζεται σημαντικά από τη διαθέσιμη μνήμη \textlatin{GPU} \cite{ochard2018}. Το \textlatin{Decolle} απαιτεί μια τάξη \textlatin{T} λιγότερους πόρους μνήμης σε σύγκριση με το \textlatin{BPTT}, όπου \textlatin{T} είναι το μήκος της ακολουθίας.



\subsection{Μοντέλο νευρώνα και σύναψης}
Τα μοντέλα νευρώνων και συνάψεων που χρησιμοποιούνται στα πειράματα του \textlatin{Decolle} ακολουθούν \textlatin{leaky, current-based integrate-and-fire dynamics} με σχετικό \textlatin{refractory} μηχανισμό. Έχουμε ορίσει αυτό το μοντέλο νευρώνα προηγουμένως, αλλά το ορίζουμε ξανά για να μας επιτρέψει να εξηγήσουμε εύκολα τις εξισώσεις που χρησιμοποιούνται για μάθηση. Η συμπεριφορά του δυναμικού μεμβράνης $U_{i}$ ενός νευρώνα $i$ περιγράφεται από τις ακόλουθες διαφορικές εξισώσεις:

\selectlanguage{english}
\begin{equation}
\begin{aligned}
U_{i}(t) &=V_{i}(t)-\rho R_{i}(t)+b_{i} \\
\tau_{m e m} \frac{\mathrm{d}}{\mathrm{d} t} V_{i}(t) &=-V_{i}(t)+I_{i}(t) \\
\tau_{r e f} \frac{\mathrm{d}}{\mathrm{d} t} R_{i}(t) &=-R_{i}(t)+S_{i}(t)
\end{aligned}
\end{equation}

\selectlanguage{greek}
$S_{i}(t)$ έχει δυαδική τιμή $(0$ ή 1$)$ που αντιπροσωπεύει εάν ο νευρώνας $i$ αυξήθηκε τη στιγμή $t$. Το δυναμικό της μεμβράνης χωρίζεται σε δύο μεταβλητές \textlatin{U} και \textlatin{V} . Είναι βιολογικά εμπνευσμένο και αντιπροσωπεύει ένα μοντέλο δύο διαμερισμάτων, ένα δενδριτικό (\textlatin{V}) και ένα σωματικό (\textlatin{U}) διαμέρισμα. Όταν το δυναμικό της μεμβράνης φτάσει σε ένα κατώφλι δημιουργείται μια ακίδα $S_{i}(t)=\Theta\left(U_{i}(t)\right)$, όπου $\Theta(x)=0$ εάν $x<0$, διαφορετικά το 1 είναι η βηματική συνάρτηση. Το σταθερό $b_{i}$  αντιπροσωπεύει την εγγενή διέγερση του νευρώνα. Ο \textlatin{refractory} μηχανισμός περιγράφεται με $R_{i}$. Βασικά ο νευρώνας αναστέλλεται μετά τη βολή, με σταθερό βάρος $\rho$ αλλά μπορεί ακόμα να εκπέμψει μια ακίδα δεδομένης μιας αρκετά ισχυρής εισόδου. Αυτή η συμπεριφορά διέπεται από την τελευταία διαφορική εξίσωση παραπάνω. Συνήθως στα μοντέλα \textlatin{Integrate-and-Fire} ο νευρώνας δεν μπορεί να εκπέμψει μια ακίδα αμέσως μετά ακόμη και με ισχυρή είσοδο. Οι παράγοντες $\tau_{r e f}$ και $\tau_{m e m}$ είναι χρονικές σταθερές της μεμβράνης και της δυναμικής του \textlatin{refractory} μηχανισμού. $I_{i}$ περιγράφει το συνολικό συναπτικό ρεύμα του νευρώνα $i$, εκφρασμένο ως:

\selectlanguage{english}
\begin{equation}
\tau_{s y n} \frac{\mathrm{d}}{\mathrm{d} t} I_{i}(t)=-I_{i}(t)+\sum_{j \in \mathrm{pre}} W_{i j} S_{j}(t)
\end{equation}
\selectlanguage{greek}
$W_{i j}$ είναι τα συναπτικά βάρη μεταξύ του προ-συναπτικού νευρώνα $j$ και του μετα-συναπτικού νευρώνα $i$. $V_{i}$ και $I_{i}$ είναι γραμμικά ως προς τα βάρη $W_{i j}$, επομένως ξαναγράφουμε το $ V_ {i} $ ως:
\selectlanguage{english}
\begin{equation}
\begin{aligned}
V_{i}(t) &=\sum_{j \in \text { pre }} W_{i j} P_{i j}(t) \\
\tau_{m e m} \frac{\mathrm{d}}{\mathrm{d} t} P_{i j}(t) &=-P_{i j}(t)+Q_{i j}(t) \\
\tau_{s y n} \frac{\mathrm{d}}{\mathrm{d} t} Q_{i j}(t) &=-Q_{i j}(t)+S_{j}(t)
\end{aligned}
\end{equation}
\selectlanguage{greek}
Η κατάσταση $P$  περιγράφει τα ίχνη της μεμβράνης και $Q$  περιγράφει τα ίχνη της συνάψεως. Για κάθε εισερχόμενη άνοδο, το ίχνος $Q$ υφίσταται άλμα ύψους 1 και διαφορετικά υποκύπτει σε κορεσμό εκθετικά με χρονική σταθερά $\tau_{\mathrm{syn}}$. Τα μετασυναπτικά δυναμικά του νευρώνα $i$ που προκαλούνται από τον νευρώνα εισόδου $j$ προκύπτουν από τη στάθμιση του ίχνους $Q_{i j}$ με το συναπτικό βάρος $W_{i j}$. Τα $P_{i j}$ και $Q_{i j}$ οδηγούνται μόνο από το $S_{j}$, και έτσι ο δείκτης $i$ αγνοείται. Ως αποτέλεσμα, οι μεταβλητές $P$ και $Q$ είναι τόσες όσες και οι προ-συναπτικοί νευρώνες, ανεξάρτητα από τον αριθμό των συνάψεων.
Ωστόσο, για την προσομοίωση σε υπολογιστή χρειαζόμαστε τις εξισώσεις σε διακριτό χρόνο, το χρονικό βήμα προσομοίωσης συμβολίζεται με $\Delta t$. Το υπεργράφημα $l$ αντιπροσωπεύει το επίπεδο στο οποίο ανήκει ο νευρώνας. Οι παραπάνω εξισώσεις ξαναγράφονται ως εξής:
\selectlanguage{english}
\begin{equation}
\begin{aligned}
U_{i}^{l}[t]=\sum_{j} W_{i j}^{l} p_{j}^{l}[t]-\rho R_{i}^{l}[t]+b_{i}^{l} \\
S_{i}^{l}[t]=\Theta\left(U_{i}^{l}[t]\right) \\
P_{j}^{l}[t+\Delta t]=\alpha P_{j}^{l}[t]+(1-\alpha) Q_{j}^{l}[t] \\
Q_{j}^{l}[t+\Delta t]=\beta Q_{j}^{l}[t]+(1-\beta) S_{j}^{l-1}[t] \\
R_{i}^{l}[t+\Delta t]=\gamma R_{i}^{l}[t]+(1-\gamma) S_{i}^{l}[t]
\end{aligned}
\end{equation}
\selectlanguage{greek}
Οι σταθερές $\alpha=\exp \left(-\frac{\Delta t}{\tau_{\latintext {mem }}}\right), \gamma=\exp \left(-\frac{\Delta t}{\tau_{\latintext {ref }}}\right)$, και $\beta=\exp \left(-\frac{\Delta t}{\tau_{s y n}}\right)$ αντικατοπτρίζουν τη δυναμική κορεσμού του δυναμικού της μεμβράνης $U$, της \textlatin{refractory} κατάστασης $R$ και της συναπτικής κατάστασης $Q$ κατά τη διάρκεια ενός χρονικού βήματος $\Delta t$.
\subsubsection{Βαθειά Μάθηση με Δίκτυα αιχμών}
Θεωρείται μια συνάρτηση συνολικού κόστους: $\mathcal{L}\left(S^{N}\right)$. Ορίζεται στις αιχμές $S^{N} $ του ανώτατου επιπέδου και στόχους $\hat{Y}$, οι παράγωγοι σε σχέση με τα βάρη στο στρώμα $l$ είναι:
\selectlanguage{english}
\begin{equation}
\frac{\partial \mathcal{L}\left(S^{N}\right)}{\partial W_{i j}^{l}}=\frac{\partial \mathcal{L}\left(S^{N}\right)}{\partial S_{i}^{l}} \frac{\partial S_{i}^{l}}{\partial U_{i}^{l}} \frac{\partial U_{i}^{l}}{\partial W_{i j}^{l}}
\end{equation}
\selectlanguage{greek}
Ο συντελεστής $\frac{\partial \mathcal{L}\left(S^{N}\right)}{\partial S_{i}^{l}}$ περιγράφει τα σφάλματα που προκύπτουν απο \textlatin{backpropagation}. Τα σφάλματα αυτά αντιπροσωπεύουν τον τρόπο με τον οποίο η έξοδος του νευρώνα $i$ στο επίπεδο $l$ τροποποιεί την απώλεια σε όλο το δίκτυο. Αυτό το πρόβλημα είναι γνωστό ως πρόβλημα εκχώρησης πίστωσης. Ωστόσο, συνήθως περιλαμβάνει μη τοπικούς όρους, τους νευρώνες δραστηριότητας, τα λάθη τους και πώς συμπεριφέρθηκαν στο παρελθόν. Λαμβάνοντας υπόψη τη μη-τοπικότητα, ένας κρυμμένος νευρώνας δεν μπορεί να προβλέψει πώς η τροποποίηση της συμπεριφοράς του θα επηρεάσει το κόστος του ανώτερου επιπέδου. Οι προσεγγίσεις στα \textlatin{backpropagation} σφάλματα στα \textlatin{SNN} μπορούν να επιτρέψουν την τοπική εκμάθηση, για παράδειγμα στην \cite{lillicrap2016}. Ωστόσο, η αποτελεσματική διατήρηση της ιστορίας της δυναμικής του δικτύου παραμένει ένα ανοιχτό πρόβλημα. Αυτό που παραμένει ως ανοιχτό πρόβλημα είναι πώς να αποθηκεύσουμε αποτελεσματικά τις προηγούμενες δυναμικές και αυτό είναι ένα από τα κύρια επιτεύγματα της \textlatin{Decolle}: μειωμένη απαίτηση μνήμης \textlatin{GPU} συγκριτικά με τη χρήση μεθόδων \textlatin{BPTT} .

Το \textlatin{Decolle} επικεντρώνεται σε έναν τύπο βαθιάς τοπικής μάθησης κατά τον οποίο τυχαίες αναγνώσεις (\textlatin{random readouts}) προσαρτώνται σε βαθιά στρώματα και καθορίζονται βοηθητικές συναρτήσεις κόστους μέσω των \textlatin{readouts} \ref{fig:readout}.
\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Learning Methods/decolle/READOUT.jpg}
    \caption{Μηχανισμός \textlatin{Readout}  . 
    \label{fig:readout}}
\end{figure}

Για νευρώνες στα βαθιά στρώματα, αυτές οι συναρτήσεις βοηθητικού κόστους παρέχουν μια πηγή σφάλματος. Πολλαπλασιάζοντας τις νευρικές ενεργοποιήσεις με μια τυχαία και σταθερή μήτρα, λαμβάνεται η τυχαία ανάγνωση. Εκπαίδευση με βοηθητικά τοπικά σφάλματα που μειώνουν το κόστος τοπικά επιτρέπουν στο δίκτυο στο σύνολό του να επιτύχει χαμηλό κόστος συνολικά. Όπως συζητήθηκε στο \cite{mostafa2017}, η μείωση της απώλειας ταξινόμησης μιας τοπικής ανάγνωσης ασκεί πίεση στα βαθιά στρώματα να αποκτήσουν σημαντικά χαρακτηριστικά, επιτρέποντας στους τυχαίους τοπικούς ταξινομητές να είναι αρκετά ακριβείς ώστε να μειωθεί η απώλεια σε όλο το δίκτυο. Επιπλέον, κάθε στρώμα βασίζεται στα χαρακτηριστικά του προηγούμενου στρώματος για να μάθει χαρακτηριστικά για τον τοπικό τυχαίο ταξινομητή του που είναι πιο "ξεμπλεγμένα" σε σχέση με τις κατηγορίες από το προηγούμενο επίπεδο. Σε αυτό το άρθρο, επικεντρωνόμαστε στο γεγονός ότι, υπό την προϋπόθεση των τοπικών λειτουργιών απώλειας, η υποκατάστατη μάθηση σε νευρωνικά δίκτυα αιχμών βαθιάς μάθησης γίνεται ιδιαίτερα αποτελεσματική. Αυτό έχει ως αποτέλεσμα μια αποδοτική προσέγγιση υποκατάστατης παραγώγου για αυτά τα δίκτυα.
\subsection{Κανόνας Μάθησης}
Η τυχαία ανάγνωση που επισυνάπτεται σε κάθε ένα από τα στρώματα των $N$ επιπέδων νευρώνων ακίδας δίνεται από:
\selectlanguage{english}
\begin{equation}
Y_{i}^{l}=\sum_{j} G_{i j}^{l} S_{j}^{l}
\end{equation}
\selectlanguage{greek}
όπου τα $G_{i j}^{l}$ είναι σταθεροί, τυχαίοι πίνακες (ένας για κάθε επίπεδο $l$) και $\Theta$  είναι μια συνάρτηση ενεργοποίησης. Η καθολική συνάρτηση απώλειας ορίζεται ως το άθροισμα των συναρτήσεων απώλειας κατά επίπεδο που ορίζονται στα τυχαία \textlatin{readouts}, δηλαδή $\mathcal{L}=\sum_{l=1}^{N} L^{l}\left(Y^{l}\right)$. Για την επιβολή της τοπικότητας, το \textlatin{Decolle} θέτει στο μηδέν όλες τις μη τοπικές παραγώγους, δηλαδή $\frac{\partial L^{l}}{\partial W_{i j}^{m}}=0$ εάν $m \neq l$ . Οι ενημερώσεις βάρους σε κάθε επίπεδο είναι:
\selectlanguage{english}
\begin{equation}
\Delta W_{i j}^{l}=-\eta \frac{\partial L^{l}}{\partial W_{i j}^{l}}=-\eta \frac{\partial L^{l}}{\partial S_{i}^{l}} \frac{\partial S_{i}^{l}}{\partial W_{i j}^{l}}
\end{equation}
\selectlanguage{greek}
όπου $\eta$ είναι ο ρυθμός εκπαίδευσης. Αν υποθέσουμε ότι η συνάρτηση απώλειας εξαρτάται μόνο από μεταβλητές στο ίδιο χρονικό βήμα, ο όρος $\frac{\partial L^{l}}{\partial S_{i}^{\prime}}$, μπορεί να υπολογιστεί χρησιμοποιώντας τον κανόνα της αλυσίδας . Η εφαρμογή του κανόνα της αλυσίδας στις παραγώγους δεύτερης τάξης μας δίνει:
\selectlanguage{english}
\begin{equation}
\frac{\partial S_{i}^{l}}{\partial W_{i j}^{l}}=\frac{\partial \Theta\left(U_{i}^{l}\right)}{\partial U_{i}^{l}} \frac{\partial U_{i}^{l}}{\partial W_{i j}^{l}}
\end{equation}
\selectlanguage{greek}
Λόγω της αραιής, δυαδικής ενεργοποίησης των νευρώνων με αιχμές, αυτή η έκφραση εξαφανίζεται παντού εκτός από το 0, όπου είναι άπειρο. Για την επίλυση αυτού του προβλήματος,χρησιμοποίουμε την προσέγγιση των \textlatin{SG}:
\selectlanguage{english}
\begin{equation}
\frac{\partial S_{i}^{l}}{\partial W_{i j}^{l}}=\sigma^{\prime}\left(U_{i}^{l}\right) \frac{\partial U_{i}^{l}}{\partial W_{i j}^{l}}
\end{equation}
\selectlanguage{greek}
όπου $\sigma^{\prime}\left(U_{i}^{l}\right)$ είναι το \textlatin{SG} της μη διαφοροποιήσιμης συνάρτησης  $\Theta\left(U_{i}^{l}\right)$.
\selectlanguage{english}
\begin{equation}
\frac{\partial U_{i}^{l}}{\partial W_{i j}^{l}}=P_{j}^{l}-\rho \frac{\partial R_{i}^{l}}{\partial W_{i j}^{l}}
\end{equation}
\selectlanguage{greek}
Οι όροι που περιλαμβάνουν $R_{i}^{l}$ είναι δύσκολο να υπολογιστούν επειδή εξαρτώνται από το ιστορικό του νευρώνα. Όπως και στο \textlatin{Superspike}, οι όροι που περιλαμβάνουν $R_{i}^{l}$ αγνοούνται. Η κανονικοποίηση εφαρμόζεται για να ευνοήσει τα χαμηλά ποσοστά πυροδότησης, γεγονός που προκαλεί τον όρο $R_{i}^{l}$ να έχει αμελητέα επιρροή. Ο κανόνας που περιγράφει τον τρόπο ενημέρωσης των συναπτικών βαρών είναι:
\selectlanguage{english}
\begin{equation}
\Delta W_{i j}^{l}=-\eta \frac{\partial L^{l}}{\partial S_{i}^{l}} \sigma^{\prime}\left(U_{i}^{l}\right) P_{j}^{l}
\end{equation}
\selectlanguage{greek}
Η απώλεια του επιπέδου $l$ αν χρησιμοποιήσουμε την συνάρτηση σφάλματος \textlatin{mean square error}, δίνεται ως εξής:
\selectlanguage{english}
\begin{equation}
L^{l}=\frac{1}{2} \sum_{i}\left(Y_{i}^{l}-\hat{Y}_{i}^{l}\right)^{2}
\end{equation}
\selectlanguage{greek}
οπότε έχουμε:
\selectlanguage{english}
\begin{equation}
\begin{aligned}
\Delta W_{i j}^{l} &=-\eta \operatorname{error}_{i}^{l} \sigma^{\prime}\left(U_{i}^{l}\right) P_{j}^{l} \\
\operatorname{error}_{i}^{l} &=\sum_{k} G_{k i}^{l}\left(Y_{k}^{l}-\hat{Y}_{k}^{l}\right)
\end{aligned}
\end{equation}
\selectlanguage{greek}
όπου $\hat{Y}^{l}$  είναι το διάνυσμα ψευδο-στόχου για το επίπεδο $l$.
\selectlanguage{english}
\begin{equation}
\mathcal{L}_{g}=\sum_{l} L^{l}+\lambda_{1}\left\langle\left[U_{i}^{l}+0.01\right]^{+}\right\rangle_{i}+\lambda_{2}\left[0.1-\left\langle U_{i}^{l}\right\rangle_{i}\right]^{+}
\end{equation}
\selectlanguage{greek}
\subsection{Υπολογιστική Πολυπλοκότητα}
Οι μεταβλητές \textlatin{P} και \textlatin{U} που απαιτούνται για τη μάθηση είναι τοπικές και είναι διαθέσιμες από τη  προωθούμενη δυναμική. Επειδή τα σφάλματα υπολογίζονται τοπικά σε κάθε επίπεδο, δεν χρειάζεται να αποθηκεύσουμε επιπλέον ενδιάμεσες μεταβλητές. Το υπολογιστικό κόστος της ενημέρωσης βάρους είναι μία προσθήκη και δύο πολλαπλασιασμοί ανά σύνδεση. Αυτό καθιστά το \textlatin{DECOLLE} σημαντικά αποδοτικότερο σε σύγκριση με το \textlatin{BPTT} για την εκπαίδευση \textlatin{SNN} που κλιμακώνεται χωρικά ως O(NT), T είναι ο αριθμός των χρονικών βημάτων. Το \textlatin{Decolle} χρησιμοποιεί εργαλεία αυτόματης διαφοροποίησης, το μόνο που απαιτείται είναι \textlatin{backpropagation} σε ένα υπογράφημα που αντιστοιχεί σε ένα επίπεδο στο ίδιο χρονικό βήμα. Επειδή οι πληροφορίες που απαιτούνται για τον υπολογισμό των παραγώγων μεταφέρονται στο χρόνο και οι τοπικές συναρτήσεις απώλειας παρέχουν τις παραγώγους που απαιτούνται για κάθε επίπεδο. Οι ακριβείς λεπτομέρειες εφαρμογής της αυτόματης διαφοροποίησης δίνονται στις υποσημειώσεις του \textlatin{Decolle}.

\subsection{Ο προβληματισμός μας}
Σε σύγκριση με το \textlatin{BPTT}, το \textlatin{Decolle } είναι μια πολύ χρήσιμη προσέγγιση επειδή έχει τάξεις μεγέθους μικρότερη πολυπλοκότητα χώρου. Το \textlatin{Decolle} είναι απλώς μια προσωρινή λύση για την εκπαίδευση συνόλων δεδομένων σε νευρομορφικό υλικό μέχρι να καταλάβουμε τα πιο έξυπνα και περίπλοκα διαμορφωτικά σήματα που χρησιμοποιεί ο εγκέφαλός μας. Υπάρχει επίσης το ζήτημα της απώλειας σημαντικών πληροφοριών/χαρακτηριστικών σε μεγαλύτερες ακολουθίες δεδομένων εισόδου, όπου ο \textlatin{e-prop } φαίνεται να είναι σε θέση να χειριστεί καλύτερα τέτοιου είδους δεδομένα .Επιπλέον, σε σύγκριση με τον βιολογικό εγκέφαλο , το παρόν νευρομορφικό υλικό είναι αρκετά πρωτόγονο. Δεδομένης της λιγοστής κατανόησής μας για το πώς τα διαμορφωτικά σήματα εκμεταλλεύονται την περίπλοκη τρισδιάστατη δομή του εγκεφάλου και τον τεράστιο αριθμό νευρώνων και συνάψεων, οι υπάρχουσες προσεγγίσεις γιa εκμάθηση νευρωνικών δικτύων είναι επίσης πολύ πρωτόγονες. Οι μέθοδοι μάθησης θα πρέπει να ακολουθούν την κατανόησή μας για τον βιολογικό εγκέφαλο, ο οποίος επηρεάζεται από τη διαθέσιμη τεχνολογία απεικόνισης του εγκεφάλου και της καταγραφής της εγκεφαλικής δραστηριότητας.

\section{Αλγόριθμος Διάδοσης Επιλεξιμότητας \textlatin{(e-Prop)}}

Τα δίκτυα προώθησης, μολονότι παρουσιάζουν αξιοσημείωτα αποτελέσματα, δεν φαίνεται να ακολουθούν το βιολογικό μοντέλο, καθώς φαίνεται ότι ο εγκέφαλος αποτελείται από πολλά επαναλαμβανόμενα συνδεδεμένα κύτταρα. Τα τεχνητά επαναλαμβανόμενα δίκτυα \textlatin{(RNNs)} έχουν επιδείξει αξιοσημείωτα αποτελέσματα σε χρονικά δεδομένα, ενώ τα επαναλαμβανόμενα νευρικά δίκτυα αιχμών αιχμαλωτίζουν τις εκπληκτικές δυνατότητες επεξεργασίας πληροφοριών του εγκεφάλου. Ωστόσο, η έλλειψη κατανόησης της διαδικασίας μάθησης στους βιολογικούς εγκεφάλους οδηγεί στη χρήση μη βιολογικά εύλογων αλγορίθμων εκπαίδευσης. Για να αντισταθμιστεί αυτό το γεγονός, υποστηριζόμενη από νευροεπιστημονικά ευρήματα, αναπτύχθηκε η μέθοδος της Διάδοσης Επιλεξιμότητας - \textlatin{Eligibility Propagation (e-Prop)} \cite{Bellec2020}, ένας αλγόριθμος για την εκπαίδευση επαναλαμβανόμενων νευρωνικών δικτύων αιχμών \textlatin{(RSNNs)}. Ο αλγόριθμος δίνει στο δίκτυο τη δυνατότητα διαδικτυακής εκπαίδευσης ενώ προσεγγίζει την απόδοση του αλγορίθμου \textlatin{Back Propagation Through Time (BPTT)}, η πιο γνωστή μέθοδος για εκπαίδευση \textlatin{RNN}.

\subsection{Σχέδιο Μάθησης}
Προκειμένου να εκπαιδευτεί ένα Νευρωνικό Δίκτυο, οι περισσότερες μέθοδοι μάθησης επικεντρώνονται στην ελαχιστοποίηση μιας συνάρτησης σφάλματος Ε. Η μέθοδος του \textlatin{backpropagation} χρησιμοποιείται ευρέως σε νευρωνικά δίκτυα προώθησης, όπου οι κλίσεις αυτής της συνάρτησης απώλειας διαδίδονται προς τα πίσω στο δίκτυο υπολογίζοντας για κάθε βάρος (σύναψη του νευρώνα) τη συμβολή του στη συνάρτηση σφάλματος. Τα επαναλαμβανόμενα μοντέλα μπορούν ακόμα να χρησιμοποιήσουν αυτήν τη μέθοδο, ξεδιπλώνοντας τον εαυτό τους στο χρόνο και σε κάθε βήμα δημιουργείται ένα αντίγραφο του δικτύου και η συνάρτηση σφάλματος διαδίδεται προς τα πίσω στο χρόνο \textlatin{(Back Propagation Through Time - BPTT)}. Αυτή η μέθοδος μπορεί και έχει χρησιμοποιηθεί για την εκπαίδευση επαναλαμβανόμενων νευρωνικών δικτύων αιχμών στο παρελθόν με επιτυχία. Αλλά είναι εύκολα κατανοητό ότι απαιτεί την αποθήκευση των ενδιάμεσων καταστάσεων των νευρώνων σε κάθε χρονικό βήμα, ενώ η ίδια η διαδικασία μάθησης πραγματοποιείται εκτός σύνδεσης. Αυτοί οι δύο παράγοντες υποδηλώνουν ότι ο εγκέφαλος πιθανότατα δεν χρησιμοποιεί την μέθοδο \textlatin{BPTT} για μαθησιακές εργασίες.

\begin{figure}[htp]
    \centering
    \includegraphics[width=12cm]{Learning Methods/e-prop/A-recurrent-neural-network-and-the-unfolding-in-time-of-the-computation-involved-in-its.png}
    \caption{Το ξεδίπλωμα ενός επαναλαμβανόμενου νευρωνικού δικτύου \textlatin{(RNN)} για εκπαίδευση με \textlatin{backpropagation}. Κάθε κατάσταση \textlatin{s} αποθηκεύεται σε κάθε χρονικό βήμα με την αντίστοιχη είσοδο και έξοδο.
    \label{fig:readout}}
\end{figure}

Έχει διαπιστωθεί, ωστόσο, ότι οι νευρώνες φαίνεται να θυμούνται αμυδρά ορισμένα γεγονότα (σειρά συμβάντων), π.χ. με τη μορφή ιόντων ασβεστίου, τα οποία είναι γνωστό ότι προκαλούν συναπτική πλαστικότητα. Aυτά τα ίχνη συχνά ονομάζονται ίχνη επιλεξιμότητας. Νευροδιαβιβαστές έχουν αποδειχθεί ότι λειτουργούν ως σήματα μάθησης σε πληθυσμούς νευρώνων σχετικά με αποτελέσματα συμπεριφοράς. Αυτό οδήγησε στην πεποίθηση ότι οι νευρώνες εκπαιδεύονται χωρίς την ανάγκη της αντίστροφης διάδοσης (\textlatin{backpropagation}), αλλά χρησιμοποιούν τα ίχνη επιλεξιμότητας του νευρώνα για να συγκλίνουν στο τοπικό βέλτιστο. Ο αλγόριθμος \textlatin{e-prop} είναι ένας νέος αλγόριθμος που βασίζεται σε αυτήν την ιδέα.

Το πρόβλημα που πρέπει να αντιμετωπιστεί είναι, όπως και στα περισσότερα νευρωνικά δίκτυα, να ελαχιστοποιήσουμε μια συνάρτηση σφάλματος \(E\) πειράζοντας τα βάρη του επαναλαμβανόμενου δικτύου \(W\). Η φύση της συνάρτησης σφάλματος μπορεί να διαφέρει ανάλογα με την εργασία, για παράδειγμα μπορεί να υπολογίζει την απόκλιση της εξόδου του δικτύου από μια βέλτιστη έξοδο (για εργασίες όπως ταξινόμηση ή παλινδρόμηση). Η τιμή της κλίσης της συνάρτησης σφάλματος σε μια δεδομένη σύναψη (βάρος) \(\frac{dE}{dW_{ji}}\) δίνει την αναλογική αλλαγή στην τιμή της συνάψεως για την μέγιστη δυνατή μείωση της συνάρτησης σφάλματος που σχετίζεται με την αλλαγή αυτής της σύναψης. Σε επαναλαμβανόμενα ΝΔΑ, η αιχμή ενός νευρώνα \textlatin{j} σε μια δεδομένη χρονική στιγμή \textlatin{t} συμβολίζεται ως δυαδική μεταβλητή \(z^t_j\). Ως αποτέλεσμα, το \(z^t_j\) δεν μπορεί να παραγωγιθεί. Οι \textlatin{Huh Dongsung} και \textlatin{Sejnowski Terrence J.} στην εργασία τους \cite{Huh2018} πρότειναν μια ψευδοπαράγωγο για αιχμές έτσι ώστε να αποφευχθεί το πρόβλημα της μη παραγωγίσιμης φύσης της αιχμής. Αυτός ο χειρισμός δίνει τη δυνατότητα παραγωγισιμότητας της αιχμής του νευρώνα \textlatin{j} \(z^t_j\) στις συνάψεις του νευρώνα. Αυτή η τοπική παράγωγος συλλέγει πληροφορίες σχετικά με την κλίση της συνάρτησης σφάλματος συναρτήσει της συνάψεως για χρόνο \textlatin{t}, ανάλογη με το ίχνος επιλεξιμότητας των βιολογικών νευρώνων. Με αυτόν τον τρόπο, το ίχνος επιλεξιμότητας \(e^t_ji\) ορίζεται στην εξίσωση 4.63.

\begin{equation}
    e_{j i}^{t} \stackrel{\text { def }}{=}\left[\frac{d z_{j}^{t}}{d W_{j i}}\right]_{\text {local }}
\end{equation}

Το ίχνος επιλεξιμότητας σχετίζεται με το κρυφό επίπεδο $\mathbf{h}_{j}^{t}$ σε χρόνο \textlatin{t}. Παρόλο που περιέχει τη μέγιστη δυνατή πληροφορία που υπολογίζεται τοπικά για τη σχετική συμμετοχή της συνάψεως στη συνάρτηση σφάλματος, η τιμή του εξαρτάται και μπορεί να ενημερωθεί χρησιμοποιώντας το ίχνος επιλεξιμότητας της συνάψεως στο τελευταίο βήμα και την κατάσταση του κρυμμένου στρώματος νευρώνων. Αυτός ο βασικός παράγοντας καθιστά δυνατή τη διαδικτυακή μάθηση, καθώς δεν απαιτεί διάδοση σε μακρινές προηγούμενες καταστάσεις για να εκπαιδεύσει το δίκτυο (να ενημερώσει τη σύναψη). Όπως είναι κατανοητό, αυτό το μοντέλο φαίνεται να μιμείται τη μάθηση του βιολογικού νευρώνα συγκριτικά με τις προηγούμενες προτεινόμενες μεθόδους, όπως ο \textlatin{BPTT}, αλλά εξακολουθεί να μην αλλάζει τις αργά μεταβαλλόμενες μεταβλητές των νευρώνων που επηρεάζουν τη διαδικασία της πλαστικότητας και ως αποτέλεσμα τη μάθηση. Ωστόσο, στο ίδιο άρθρο \cite{Bellec2020} παρουσιάζεται ένα μαθηματικό μοντέλο που λαμβάνει υπόψη τέτοιες συμπεριφορές και θα παρουσιαστεί αργότερα σε αυτό το κεφάλαιο.

Αυτός ο χειρισμός της αιχμής \(z^t_j\) και η παραγώγισή της δίνει τη δυνατότητα υπολογισμού της παραγώγου της συνάρτησης σφάλματος σε μια σύναψη ως άθροισμα όλων των χρονικών βημάτων του γινομένου της παραγώγου της συνάρτησης σφάλματος συναρτήσει κάθε αιχμή με την παράγωγο της αιχμής ενός δεδομένου νευρώνα στις συνάψεις του, δηλαδή το ίχνος επιλεξιμότητας, σύμφωνα με τον κανόνα της αλυσίδας, όπως παρουσιάζεται στην εξίσωση 4.64.

Το δεύτερο γινόμενο, όπως αναφέρθηκε προηγουμένως, είναι το ίχνος επιλεξιμότητας και αντικαθιστώντας το στην εξ. 4.63 φτάνουμε στην αναπαράσταση της κλίσης της συνάρτησης σφάλματος λόγω της συνάψεως όπως φαίνεται παρακάτω:

\begin{equation}
    \frac{d E}{d W_{j i}}=\sum_{t} \frac{d E}{d z_{j}^{t}} \cdot\left[\frac{d z_{j}^{t}}{d W_{j i}}\right]_{\text {local }}
    \;\;\; \Longleftrightarrow \;\;\;
    \frac{d E}{d W_{j i}}=\sum_{t} \frac{d E}{d z_{j}^{t}} \cdot e_{ji}^t
\end{equation}

Στην εξίσωση 4.65 φαίνεται ότι ο όρος του ίχνους επιλεξιμότητας δεν εξαρτάται από τη συνάρτηση σφάλματος \(E\) ενώ μόνο το πρώτο γινόμενο του αθροίσματος εξαρτάται. Ο όρος \(\frac{dE}{dz_j^t}\) ορίζεται ως το μαθησιακό σήμα (εξ. 4.65) και στον βιολογικό νευρώνα αντιπροσωπεύει τα μαθησιακά σήματα όπως αυτά που προαναφέρθηκαν π.χ. νευροδιαβιβαστές, προσθέτοντας στη βιολογική αληθοφάνεια του μοντέλου. Ως αποτέλεσμα, η εξίσωση 4.64 μπορεί τώρα να γραφτεί αντικαθιστώντας τον πρώτο όρο προϊόντος με την εξ. 4.65 με αποτέλεσμα την εξίσωση 4.66 που ακολουθεί.

\begin{equation}
    L_{j}^{t} \stackrel{\text { def }}{=} \frac{d E}{d z_{j}^{\prime}}
\end{equation}

\begin{equation}
    \frac{d E}{d W_{j i}}=\sum_{t} L_{j}^{t} e_{j i}^{t}
\end{equation}

Ο όρος του μαθησιακού σήματος, που ορίζεται παραπάνω, παίζει τον μοναδικό ρόλο της σύνδεσης της συνάρτησης σφάλματος με την εκπομπή αιχμής ενός νευρώνα.

Συνοψίζοντας το σχήμα εκμάθησης \textlatin{e-prop}, σε αντίθεση με τη διαδικασία ξεδίπλωσης του επαναλαμβανόμενου νευρωνικού δικτύου και τον υπολογισμό των παραγώγων της συνάρτησης σφάλματος για κάθε αντίγραφο, όπως γίνεται στο σχήμα \textlatin{BPTT}, το \textlatin{e-prop} χρησιμοποιεί ίχνη επιλεξιμότητας συνάψεων που περιέχουν πληροφορίες για προηγούμενες ενεργοποιήσεις των νευρώνων και χρησιμοποιώντας σήματα μάθησης, που συσχετίζονται με την εκπομπή αιχμής και την ίδια τη συνάρτηση σφάλματος, υπολογίζουν με διαδικτυακό τρόπο την επιθυμητή κλίση. Στο σχήμα 4.23 δίνεται μια γραφική εξήγηση της διαφοράς μεταξύ των αλγορίθμων μάθησης \textlatin{BPTT} και \textlatin{e-prop}. Ενώ ο \textlatin{BPTT} απαιτεί την αποθήκευση, τον υπολογισμό και τη μεταφορά πληροφοριών για όλες τις προηγούμενες καταστάσεις του δικτύου, προκειμένου το δίκτυο να μάθει (χρησιμοποιώντας μια διαδικασία μάθησης εκτός σύνδεσης), ο \textlatin{e-prop} απαιτεί μόνο τα προηγούμενα ίχνη επιλεξιμότητας κάθε συνάψεως (βάρος) και το σήμα εκμάθησης \(L_j^t\) για τον υπολογισμό της ιδανικής αλλαγής βάρους κάθε συναψη (διαδικτυακή διαδικασία) για την ελαχιστοποίηση της απόστασης από τον στόχο της (δηλαδή ελαχιστοποίηση της συνάρτησης σφάλματος).

\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{Learning Methods/e-prop/e-prop vs bptt.png}
    \caption{\footnotesize Η διαφορά μεταξύ της εκπαίδευσης των \textlatin{BPTT} και \textlatin{e-prop}. \textbf{Α.} Η αρχιτεκτονική του \textlatin{RSNN}. Τα \(x, h, z, y\) αντιπροσωπεύουν την είσοδο, την κρυφή κατάσταση, τις αιχμές και την έξοδο του δικτύου ενώ ο εκθέτης \(t\) αντιπροσωπεύει το χρονικό βήμα κάθε μεταβλητής. \textbf{Β.} Η ξεδιπλωμένη έκδοση του δικτύου εμφανίζεται για δύο χρονικά βήματα, \textlatin{t} και \textlatin{t-1}. Οι επαναλαμβανόμενες συνάψεις ξεδιπλώνονται τώρα και συνδέουν τα δύο νέα δίκτυα με την πάροδο του χρόνου. \textbf{Γ.} Ο \textlatin{BPTT} απαιτεί την διάδοση της κλίσης σφαλμάτων αντίστροφα στο χρόνο. \textbf{Δ.} Η διαδικτυακή εκμάθηση μέσω \textlatin{e-prop} απαιτεί μόνο τα προηγούμενα ίχνη συνάψεων και σημάτων μάθησης που υπολογίζονται χρησιμοποιώντας τη συνάρτηση σφάλματος στο επιθυμητό χρονικό βήμα.
    \label{fig:readout}}
\end{figure}

Σε ένα δίκτυο προώθησης, όπου οι αιχμές και εξερχόμενες καταστάσεις συγχρονίζονται, το σήμα μάθησης καταγράφει την πλήρη επιρροή της αιχμής \(z_j^t\) στη συνάρτηση σφάλματος. Αλλά σε επαναλαμβανόμενα δίκτυα, η εκπομπή της αιχμής μπορεί να μην επηρεάσει την έξοδο και ως αποτέλεσμα τη συνάρτηση απώλειας στο ίδιο χρονικό βήμα αλλά αργότερα στο χρόνο. Αυτό οδηγεί στο σήμα μάθησης όπως παρουσιάζεται παραπάνω να μην συλλαμβάνει την ακριβή κλίση της συνάρτησης σφάλματος συναρτήσει της εκπεμπόμενης αιχμής. Για την αντιμετώπιση του προβλήματος που προκύπτει, οι συγγραφείς προτείνουν τη χρήση μιας προσέγγισης του σήματος μάθησης (χρησιμοποιώντας τον τελεστή της μερικής παραγώγου - \(\frac{\partial E}{\partial z_{j}^{t}}\) για τη διάκριση των δύο όρων). Η αιτιολόγηση της χρήσης αυτού του ελλιπούς μαθησιακού σήματος είναι ότι, ενώ δεν λαμβάνει υπόψη τη μελλοντική επιρροή της αιχμής, ο συνδυασμός του με το ίχνος επιλεξιμότητας δίνει την ικανότητα στον νευρώνα να δει στο παρελθόν, καθιστώντας τον ικανό να μάθει χρονικές εργασίες. Στο σχήμα 4.24 φαίνονται οι υπολογιστικές εξαρτήσεις μεταξύ των στοιχείων του δικτύου. Αποδεικνύεται ότι το ίχνος επιλεξιμότητας εξαρτάται από προηγούμενα και παρόντα γεγονότα, ενώ το σήμα εκμάθησης απαιτεί τη γνώση μελλοντικών γεγονότων. Εμφανίζεται επίσης η εξάρτηση από την προαναφερθείσα προσέγγιση σήματος μάθησης, η οποία εξαρτάται μόνο από τη συνάρτηση απώλειας κατά τον χρόνο υπολογισμού. Με αυτόν τον τρόπο είναι σαφές ότι το δίκτυο είναι ικανό για διαδικτυακή εκπαίδευση.

\begin{figure}[htp]
    \centering
    \includegraphics[width=10cm]{Learning Methods/e-prop/computational flow.png}
    \caption{\footnotesize Υπολογιστική ροή σημάτων στον \textlatin{e-prop}. \textbf{Α. } Το δίκτυο ξεδιπλώνεται στο βάθος χρόνου και οι εξαρτήσεις μεταξύ εισόδου \(\textbf{x}^t\), κρυφής κατάστασης \(\textbf{h}^t\), εξόδου νευρώνων \(\textbf{z}^t\) και συνάρτηση σφάλματος \(\textbf{E}\) σχεδιάζεται με βέλη. \textbf{Β. } Υπολογιστική ροή ιχνών επιλεξιμότητας. \textbf{Γ. } Υπολογιστική ροή του ιδανικού σήματος μάθησης \(L^t\). Οι εξαρτήσεις εμφανίζονται σε τρέχουσες και μελλοντικές καταστάσεις. \textbf {Δ. } Η υπολογιστική ροή του προσεγγιστικού σήματος εκμάθησης \(\frac{\partial E}{\partial z_ {j}^{t}}\). Όπως φαίνεται, αυτό εξαρτάται μόνο από τα παροντικά γεγονότα και υπολογίζεται σε χρονικό βήμα \textlatin{t}.
    \label{fig:readout}}
\end{figure}

Στον κανόνα εκμάθησης \textlatin{e-prop}, το σήμα εκμάθησης \(L_j^t\) υπολογίζεται ως η απόσταση ή απόκλιση της εξόδου του δικτύου \(y^t_k\) ως προς την επιθυμητή έξοδο \(y^{*, t}_κ\). Πιο συγκεκριμένα, κάθε νευρώνας εξόδου του δικτύου διαδίδει την απόκλιση από την επιθυμητή έξοδο στον νευρώνα \textlatin{j} μέσω μιας σταθμισμένης σύναψης \(B_{jk}\). Επομένως, η τιμή του μαθησιακού σήματος υπολογίζεται στην ακόλουθη εξίσωση:

\begin{equation}
    L_{j}^{t}=\sum_{k} B_{j k}\left(y_{k}^{t}-y_{k}^{*, t}\right)_{\text {deviation of output } k}
\end{equation}

Η επιλογή του βάρους \(B_{jk}\) αφήνεται στον μηχανικό του δικτύου, ενώ οι συγγραφείς προτείνουν τρεις πιθανές επιλογές. Τον Συμμετρικό (\textlatin{Symmetric}) \textlatin{e-prop} όπου το βάρος \(B_{jk}\) είναι ίσο με το βάρος της σύναψης \textlatin{j} με την έξοδο \textlatin{k}, \(W_{kj}^{out}\), τον Τυχαίο (\textlatin{Random}) \textlatin{e-prop} όπου τα βάρη \(B_{jk}\) επιλέγονται τυχαία και παραμένουν σταθερά (αυτό συνεπάγεται συνδέσεις μέσω \(B_{jk}\) για νευρώνες του κρυμμένου στρώματος με το επίπεδο εξόδου, όπου, στο δίκτυο, δεν είναι συνδεδεμένες) και τον Προσαρμοστικό (\textlatin{Adaptive}) \textlatin{e-prop} που είναι σαν το τυχαίο \textlatin{e-prop} αλλά τα βάρη \(B_{jk}\) εξελίσσονται μέσω απλών κανόνων πλαστικότητας. Οι δύο τελευταίες προτάσεις φαίνεται να είναι πιο βιολογικά αληθοφανείς σε σύγκριση με την πρώτη, όπως αναφέρουν οι συγγραφείς.

\subsection{Μαθηματική Βάση}

Θεωρούμε ένα επαναλαμβανόμενα συνδεδεμένο νευρωνικό δίκτυο με είσοδο \textlatin{\textbf{x}\textsuperscript{t}}, κρυφή κατάσταση \textlatin{\textbf{h}\textsuperscript{t}} και έξοδο νευρώνων \textlatin{\textbf{z}\textsuperscript{t}}. Σε επαναλαμβανόμενα δίκτυα η κρυφή κατάσταση τη στιγμή \textlatin{t} εξαρτάται από την προηγούμενη κρυφή κατάσταση, την είσοδο τη χρονική στιγμή \textlatin{t} και την έξοδο των νευρώνων στο προηγούμενο χρονικό βήμα. Μπορεί λοιπόν να αναπαρασταθεί ως η ακόλουθη συνάρτηση M: $\mathbf{h}_{j}^{t} = M\left(\mathbf{h}_{j}^{t-1}, \mathbf{z}^{t-1}, \mathbf{x}^{t}, \mathbf{W}_{j}\right)$. Η συνάρτηση \(f\) ορίζεται ως $z_{j}^{t} = f\left(\mathbf{h}_{j}^{t}\right)$ που περιγράφει την ενημέρωση της κατάστασης ενός νευρώνα \(j\). Όπως αναφέρθηκε προηγουμένως, σε επαναλαμβανόμενα δίκτυα, συμβάντα που συμβαίνουν στο χρονικό βήμα \textlatin{t} μπορεί να επηρεάσουν τη συνάρτηση σφάλματος μέσω της επίδρασης τους στις μελλοντικές καταστάσεις των κρυφών καταστάσεων των νευρώνων. Αυτό προκαλεί την ανάγκη ορισμού δύο τύπων παραγώγων, που συμβολίζονται ως \(\frac{dA}{dB^t}\) και \(\frac{\partial A}{\partial B^t}\), όπου το πρώτο υποδηλώνει την εξάρτηση του \(A\) από το \(B^t\) άμεσα και έμμεσα, μέσω της επιρροής του \(B^t\) στις μεταβλητές \(B^{t+\tau}\) σε μελλοντικά χρονικά βήματα, και το δεύτερο την εξάρτηση του \(A\) από \(B^t\) απευθείας στο χρόνο \textlatin{t}.

Σε ένα επαναλαμβανόμενο νευρωνικό δίκτυο, ο υπολογισμός της κλίσης του \(E\) συναρτήσει του \(W_{ji}\) σε χρόνο t περιγράφεται από την εξίσωση.

\begin{equation}
    \frac{d E}{d W_{ji}}=\frac{d E}{d h_{j}^{t}} \frac{\partial h_j^{t}}{\partial W_{ji}^{t}}
\end{equation}

Ξεδιπλώνοντας το δίκτυο, όπως στον \textlatin{BPTT}, οι αθροισμένες παράγωγοι στο χρόνο δίνουν την ακόλουθη εξίσωση, η οποία περιγράφει την αντίστροφη διάδοση στον χρόνο των απωλειών. Η εξίσωση Το 4.69 χρησιμοποιείται συνήθως σε μεθόδους \textlatin{BPTT} όπου κάθε στρώμα \(l\) του ξεδιπλωμένου δικτύου είναι ισοδύναμο με την περίπτωση του χρόνου t του \textlatin{RSNN} που παρουσιάζεται εδώ.

\begin{equation}
    \frac{d E}{d W_{j i}}=\sum_{t^{\prime}} \frac{d E}{d \mathbf{h}_{j}^{t^\prime}} \cdot \frac{\partial \mathbf{h}_{j}^{t^\prime}}{\partial W_{j i}}
\end{equation}

Από το σχ. 4.24, εφαρμόζοντας τον κανόνα της αλυσίδας του υπολογιστικού γραφήματος, η παράγωγος \(\frac{d E}{d h_{j}^{t}}\) μπορεί να εκφραστεί ως παράγωγος του επόμενου χρονικού βήματος όπως φαίνεται στις εξισώσεις 4.70 και 4.71. Στην εξίσωση 4.70, η παράγωγος \(\frac{d E}{d z_{j}^{t}}\) αντικαθίσταται με το σήμα εκμάθησης \(L_j^t\) από το eq. 4,65.

\begin{align}
    \frac{d E}{d \mathbf{h}_{j}^{t}}=\frac{d E}{d z_{j}^{t}} \frac{\partial z_{j}^{t}}{\partial \mathbf{h}_{j}^{t}}+\frac{d E}{d \mathbf{h}_{j}^{t+1}} \frac{\partial \mathbf{h}_{j}^{t+1}}{\partial \mathbf{h}_{j}^{t}} \\
    =L_j^t\frac{\partial z_{j}^{t}}{\partial \mathbf{h}_{j}^{t}}+\frac{d E}{d \mathbf{h}_{j}^{t+1}} \frac{\partial \mathbf{h}_{j}^{t+1}}{\partial \mathbf{h}_{j}^{t}}
\end{align}

Αντικαθιστώντας τον πρώτο όρο του γινομένου της εξίσωσης 4.69 από την εξίσωση 4.71 παίρνουμε την εξίσωση 4.72. Εφαρμόζοντας την ίδια μέθοδο και αντικαθιστώντας τις νέες παραγώγους της συνάρτησης σφάλματος στις κρυφές καταστάσεις σε μελλοντικά χρονικά βήματα, παίρνουμε την εξίσωση 4.73

\begin{equation}
    \frac{d E}{d W_{j i}}=\sum_{t}\left(L_{j}^{t} \frac{\partial z_{j}^{t}}{\partial \mathbf{h}_{j}^{t}}+\frac{d E}{d \mathbf{h}_{j}^{t+1}} \frac{\partial \mathbf{h}_{j}^{t+1}}{\partial \mathbf{h}_{j}^{t}}\right) \cdot \frac{\partial \mathbf{h}_{j}^{t}}{\partial W_{j i}} 
\end{equation}

\begin{equation}
    =\sum_{t}\left(L_{j}^{t} \frac{\partial z_{j}^{t}}{\partial \mathbf{h}_{j}^{t}}+\left(L_{j}^{t+1} \frac{\partial z_{j}^{t+1}}{\partial \mathbf{h}_{j}^{t+1}}+(\cdots) \frac{\partial \mathbf{h}_{j}^{t+2}}{\partial \mathbf{h}_{j}^{t+1}}\right) \frac{\partial \mathbf{h}_{j}^{t+1}}{\partial \mathbf{h}_{j}^{t}}\right) \cdot \frac{\partial \mathbf{h}_{j}^{t}}{\partial W_{j i}}
\end{equation}

Στην παραπάνω έκφραση, η επέκταση της παρένθεσης οδηγεί σε ένα διπλό άθροισμα όπου κάθε σήμα εκμάθησης \(L_j^t\) πολλαπλασιάζεται με κάποιον παράγοντα ο οποίος, όπως θα δειχθεί, είναι ίσος με το ίχνος επιλεξιμότητας. Πρέπει να σημειωθεί ότι ο όρος \(\frac{\partial \mathbf{h}_{j}^{t+1}}{\partial \mathbf{h}_{j}^{t}}\) αντιπροσωπεύει γεγονότα στον νευρώνα \(j\) που συμβαίνουν μέχρι το χρόνο \textlatin{t} χωρίς την επίδραση οποιουδήποτε μελλοντικού συμβάντος ή σφάλματος. Συνολικά, όπως φαίνεται στην εξίσωση 4.75 προκύπτει ως αποτέλεσμα ένα γινόμενο που ορίζεται από γεγονότα του παρελθόντος το οποίο και οδηγεί στην δημιουργία της ιδέας του \textlatin{e-prop} και του ίχνους επιλεξιμότητας.

\begin{equation}
    \frac{d E}{d W_{j i}}=\sum_{t^\prime}\sum_{t\geq t^\prime}L_{j}^{t} \frac{\partial z_{j}^{t}}{\partial \mathbf{h}_{j}^{t}}\frac{\partial \mathbf{h}_{j}^{t}}{\partial \mathbf{h}_{j}^{t-1}} \cdots \frac{\partial \mathbf{h}_{j}^{t^\prime + 1}}{\partial \mathbf{h}_{j}^{t^\prime}} \frac{\partial \mathbf{h}_{j}^{t^\prime}}{\partial W_{j i}} 
\end{equation}

\begin{equation}
    =\sum_{t}L_{j}^{t} \frac{\partial z_{j}^{t}}{\partial \mathbf{h}_{j}^{t}}\sum_{t\geq t^\prime}\frac{\partial \mathbf{h}_{j}^{t}}{\partial \mathbf{h}_{j}^{t-1}} \cdots \frac{\partial \mathbf{h}_{j}^{t^\prime + 1}}{\partial \mathbf{h}_{j}^{t^\prime}} \frac{\partial \mathbf{h}_{j}^{t^\prime}}{\partial W_{j i}} 
\end{equation}

\medskip
Το εσωτερικό άθροισμα του δεξιού τμήματος της εξίσωσης ορίζεται ως το διάνυσμα επιλεξιμότητας \(\mathbf{\epsilon_{ji}^t}\) το οποίο ικανοποιεί την αναδρομική σχέση 4.77.

\begin{equation}
    \mathbf{\epsilon_{ji}^t} \stackrel{\text { def }}{=} \sum_{t\geq t^\prime}\frac{\partial \mathbf{h}_{j}^{t}}{\partial \mathbf{h}_{j}^{t-1}} \cdots \frac{\partial \mathbf{h}_{j}^{t^\prime + 1}}{\partial \mathbf{h}_{j}^{t^\prime}} \frac{\partial \mathbf{h}_{j}^{t^\prime}}{\partial W_{j i}}
\end{equation}

\begin{equation}
    \epsilon_{j i}^{t}=\frac{\partial \mathbf{h}_{j}^{t}}{\partial \mathbf{h}_{j}^{t-1}} \cdot \epsilon_{j i}^{t-1}+\frac{\partial \mathbf{h}_{j}^{t}}{\partial W_{j i}}
\end{equation}

Εξ ορισμού του ίχνους επιλεξιμότητας από την εξίσωση 4.63, με εφαρμογή του κανόνα της αλυσίδας οδηγούμαστε στην εξίσωση 4.78 όπου, με την αντικατάσταση του αθροίσματος με το διάνυσμα επιλεξιμότητας, δίνεται η μέθοδος υπολογισμού του ίχνους επιλεξιμότητας. Η εξίσωση 4.77 προσδίδει τη διαδικτυακή εκμάθηση της μεθόδου \textlatin{e-prop} στον υπολογισμό του ίχνους επιλεξιμότητας μέσω της εξίσωσης 4.79.

\begin{equation}
    e_{j i}^{t} \stackrel{\text { def }}{=}\left[\frac{d z_{j}^{t}}{d W_{j i}}\right]_{\text {local }} = \frac{\partial z_j^t}{\partial \mathbf{h}_j^t}\sum_{t\geq t^\prime}\frac{\partial \mathbf{h}_{j}^{t}}{\partial \mathbf{h}_{j}^{t-1}} \cdots \frac{\partial \mathbf{h}_{j}^{t^\prime + 1}}{\partial \mathbf{h}_{j}^{t^\prime}} \frac{\partial \mathbf{h}_{j}^{t^\prime}}{\partial W_{j i}}
\end{equation}

\medskip
\begin{equation}
    e_{j i}^{t} = \frac{\partial z_j^t}{\partial \mathbf{h}_j^t} \cdot \epsilon_{j i}^{t}
\end{equation}

Έτσι, αντικαθιστώντας το ίχνος επιλεξιμότητας από την εξίσωση. 4.78 στην εξίσωση 4.75, ολοκληρώνεται η απόδειξη της εξίσωσης 4.66.

\subsection{Ίχνος Επιλεξιμότητας με βάση την ανταμοιβή}

Μια σημαντική μέθοδος μάθησης στα νευρωνικά δίκτυα είναι η ενισχυτική μάθηση - \textlatin{Reinforcement Learning (RL)}. Η βαθιά ενισχυτική μάθηση εκμεταλλεύεται έξυπνα τη δύναμη του \textlatin{BPTT} για να επιτύχει εντυπωσιακά αποτελέσματα. Ως αποτέλεσμα, η μέθοδος του \textlatin{e-prop} μπορεί να εφαρμοστεί για \textlatin{RL}. Οι συντάκτες του \cite{Bellec2020} προτείνουν μια βιολογικά αληθοφανή μέθοδο ενισχυτικής μάθησης χρησιμοποιώντας το σχήμα του \textlatin{e-prop}, η οποία βασίζεται στο συνδυασμό πολιτικής κλίσης και της μεθόδου δράστη-κριτικού. Μια αξιοσημείωτη διαφορά αυτής της μεθόδου (\textlatin{e-prop} βάσει ανταμοιβής) σε σύγκριση με τον \textlatin{e-prop} που συζητήθηκε προηγουμένως είναι ότι στην προηγούμενη περίπτωση το σήμα μάθησης εξαρτάται από την απόκλιση ενός εξωτερικού σήματος, ενώ στην περίπτωση αυτή, το σήμα εκμάθησης μεταφέρει πληροφορίες σχετικά με το πώς η επιλεγμένη ενέργεια αποκλίνει από τη δράση που προτείνει το δίκτυο.

Για να υπολογιστεί η κατάλληλη αλλαγή βάρους, πρέπει να οριστεί μια σημειογραφία ενός φίλτρου μνήμης που ξεθωριάζει. Ο τελεστής \(\mathcal{F}_\alpha\) δηλώνει ένα φίλτρο χαμηλής διέλευσης \textlatin{(Low Pass Filter)} που ικανοποιεί την ακόλουθη εξίσωση.

\begin{equation}
    \mathcal{F}_{\alpha}\left(x^{t}\right)=\alpha \mathcal{F}_{\alpha}\left(x^{t-1}\right)+x^{t}
\end{equation}

με την αρχική συνθήκη \(\mathcal{F}_{\alpha}\left(x^{0}\right) = x^{0}\). Μια απλοποιημένη σημειωγραφία για την εφαρμογή του φίλτρου χαμηλής διέλευσης στο ίχνος επιλεξιμότητας \(e_{ji}^t\) δίνεται ως $\mathcal{F}_{\kappa}\left(e_{j}\right)^{t} = \bar{e}_{ii}^{t}$, όπου \(\kappa\) αντιπροσωπεύει τη διαρροή του νευρώνα μεταξύ διαδοχικών χρονικών βημάτων. Ο προτεινόμενος κανόνας πλαστικότητας βασισμένος στον \textlatin{e-prop}, ο οποίος ενσωματώνει τη διαδικτυακή μάθηση βασίζεται στην εφαρμογή ενός φίλτρου χαμηλής διέλευσης $\mathcal{F}_{\gamma}$, το οποίο αντιπροσωπεύει τη μνήμη που ξεθωριάζει, στο γινόμενο \(L_j^t\bar{e}_{ji}^t\). Το αποτέλεσμα πολλαπλασιάζεται με το σφάλμα πρόβλεψης $\delta^{t} = r^{t}+\gamma V^{t+1} -V^{t}$, με $r^t$ να είναι η ανταμοιβή στον χρόνο \textlatin{t}. Ο υπολογισμός της μεταβολής βάρους τη στιγμή \textlatin{t} δίνεται από το υπολογισμένο αποτέλεσμα πολλαπλασιασμένο με έναν παράγοντα λησμόνησης (\textlatin{forgetting factor}) $\eta$ και έτσι, η ζητούμενη έκφραση παρουσιάζεται στην εξίσωση 4.81.

\begin{equation}
    \Delta W_{j i}^{t}=-\eta \delta^{t} \mathcal{F}_{\gamma}\left(L_{j}^{t} e_{j i}^{t}\right)
\end{equation}

Αυτή η νέα προσέγγιση ενημέρωσης των βαρών προσφέρει καλύτερα αποτελέσματα από προηγούμενες προσπάθειες με χρήση του ίχνους επιλεξιμότητας. Η προσθήκη του μαθησιακού σήματος και του φίλτρου χαμηλής διέλευσης στον υπολογισμό οδηγούν σε λιγότερο θορυβώδεις εκτιμήσεις κλίσης και, όπως έχουν αποδείξει οι συγγραφείς, δείχνουν παρόμοια συμπεριφορά με τον \textlatin{BPTT} που χρησιμοποιείται στη βαθιά ενισχυτική μάθηση.

Οι συγγραφείς δοκίμασαν αυτήν τη μέθοδο του \textlatin{e-prop} που βασίζεται σε ανταμοιβή σε δύο παιχνίδια \textlatin{Atari}, το \textlatin{Pong}, ένα παιχνίδι που χρησιμοποιείται συχνά ως βάση για την \textlatin{RL} και το \textlatin{Fishing Derby}, ένα παιχνίδι με μεγάλη ανάγκη για χρονική επεξεργασία. Κατάφεραν να εκπαιδεύσουν ένα δίκτυο με διαδικτυακό τρόπο χωρίς να χρειάζεται εμπειρία επαναληπτικής αναπαραγωγής με τέλεια μνήμη. Χρησιμοποίησαν αυστηρά την διαδικτυακή εκπαίδευση με κάθε καρέ ως είσοδο και χρησιμοποίησαν αυξανόμενο μήκος επεισοδίων με προσαρμοστικό ρυθμό μάθησης. Με αυτόν τον τρόπο, σε πρώιμα σύντομα επεισόδια, το δίκτυο θα μπορούσε να μάθει χρήσιμες δεξιότητες ενώ, σε μεγαλύτερα επεισόδια, να ρυθμίσει τέλεια τις δεξιότητες αυτές. Τα δίκτυα έδειξαν ανταγωνιστικά αποτελέσματα, με την απόδοση του ανεπτυγμένου \textlatin{Fishing Derby LSNN} δικτύου να είναι παρόμοια με αυτή των διακριτών αλγορίθμων μάθησης εκτός σύνδεσης που εφαρμόζονται σε δίκτυα \textlatin{LSTM}.

\subsection{Περαιτέρω Μελέτη}

Η ιδέα του αλγορίθμου \textlatin{e-prop} έχει προσελκύσει το ενδιαφέρον της ερευνητικής κοινότητας. Οι \textlatin{Zixuan Zhao et.al.} στην εργασία τους \cite{Brasoveanu2020} παρουσιάζουν μια υλοποιημένη βιβλιοθήκη με το όνομα \textlatin{Neko} για εκπαίδευση Νευρωνικών Δικτύων Αιχμών σε γλώσσα \textlatin{python}, όπου έχει υλοποιηθεί η μέθοδος \textlatin{e-prop}. Επιπλέον, έχουν κωδικοποιήσει τον προσαρμοστικό νευρώνα \textlatin{LIF (ALIF)} για το δίκτυο όπως παρουσιάζεται στο έργο των \textlatin{Guillaume Bellec et.al.} \cite{Bellec2020}, δίνοντας τη δυνατότητα στους ερευνητές να μελετήσουν με ευκολία αυτήν τη μέθοδο εκπαίδευσης.

Όπως προαναφέρθηκε, ο \textlatin{e-prop} παρέχει ένα βιολογικά εύλογο μοντέλο εκπαίδευσης νευρωνικών δικτύων. Ωστόσο, οι \textlatin{Manuel Traub et.al.} στην εργασία τους \cite{Traub2020} έδειξαν ότι το προτεινόμενο μοντέλο που παρουσιάστηκε παραπάνω δεν λαμβάνει υπόψη το \textlatin{STDP}, μια διαδικασία που έχει περιγραφεί στο κεφάλαιο 4.3.2 και χρησιμοποιείται από τον εγκέφαλο για νευρική πλαστικότητα (κεφάλαιο 2.3). Αποδεικνύουν ότι τα μοντέλα νευρώνων που χρησιμοποιήθηκαν (μοντέλα νευρώνων \textlatin{LIF} και \textlatin{A-LIF}) δεν αντικατοπτρίζουν τη συμπεριφορά του \textlatin{STDP} στις παραγώγους βασισμένες στο ίχνος επιλεξιμότητας. Δείχνουν ότι πιο σύνθετα μοντέλα όπως το μοντέλο νευρώνα \textlatin{Izhikevich} φέρουν τις απαραίτητες πληροφορίες για να εμφανίζεται η συμπεριφορά του \textlatin{STDP} στον \textlatin{e-prop}. Επιπλέον, προτείνουν ένα τροποποιημένο μοντέλο νευρώνων \textlatin{LIF} το οποίο ονομάζουν \textlatin{STDP-LIF} νευρώνα που δρα με τον κατάλληλο τρόπο για να υπάρχει το \textlatin{STDP} μέσω της μαθησιακής διαδικασίας. Η διακριτή διαφορική εξίσωση για τον νευρώνα \textlatin{STDP-LIF} είναι

\begin{equation}
    u^{t+1}_j = \alpha u^t_j+I^t_j-z_j^t\alpha u^t_j-z_j^{(t-\delta t_{ref})}\alpha u^t_j
\end{equation}

όπου η \(u^t_j\) είναι η τάση μεμβράνης του νευρώνα \textlatin{j} τη στιγμή \textlatin{t}, το \(z_j^t\) αντιπροσωπεύει την παρουσία της αιχμής του νευρώνα \textlatin{j} τη στιγμή \textlatin{t} και το \(\delta t_{ref}\) είναι η πυρίμαχη περίοδος του νευρώνα. Με αυτόν τον τρόπο, η επίδραση της αιχμής του νευρώνα είναι παρούσα στην κρυφή κατάσταση καθώς είναι μέρος της εξίσωσης της παραγώγου στην κρυφή κατάσταση. Αυτή η παρουσία είναι υπεύθυνη για την \textlatin{STDP} συμπεριφορά του δικτύου, όπως αποδεικνύουν οι συγγραφείς στο έργο τους. Η εφαρμογή του κανόνα \textlatin{STDP} προσδίδει την ικανότητα στο δικτύου για τον ακριβή χρονισμό του δικτύου, όπως έδειξαν οι συγγραφείς αφού οι νευρώνες \textlatin{STDP-LIF} απέδωσαν καλύτερα σε σύγκριση με τους νευρώνες \textlatin{LIF} σε πριάματα της προαναφερθείσα εργασία, εκπαιδευόμενοι με τον αλγόριθμο \textlatin{e-prop}, έχοντας ως είσοδο ένα \textlatin{Poisson} κατανεμημένο τρένο αιχμών συχνότητας 25 \textlatin{Hz}.

Σε συνέχεια της μελέτης του αλγορίθμου εκπαίδευσης \textlatin{e-prop}, δημοσιεύτηκε αυτή η μελέτη \cite{Veen2021} όπου συγκρίνονται τα μοντέλα νευρώνων που παρουσιάστηκαν μέχρι τώρα. Ο συγγραφέας υλοποιεί το μοντέλο νευρώνα \textlatin{STDP-A-LIF} το οποίο, όπως φαίνεται, αποδίδει καλύτερα από το μοντέλο \textlatin{A-LIF}. Ωστόσο, το μοντέλο νευρώνα \textlatin{Izhikevich} ήταν ασταθές και είχε χειρότερη απόδοση από τις εναλλακτικές λύσεις, δείχνοντας ότι η ύπαρξη συμπεριφοράς \textlatin{STDP} στον \textlatin{e-prop} δεν εγγυάται καλύτερα αποτελέσματα από μόνη της, αν και η προσθήκη της σε μοντέλα που δεν φέρουν \textlatin{STDP} συμπεριφορά φαίνεται να βελτιώνει τα αποτελέσματα. Επιπλέον, αυτή η μελέτη δείχνει ότι τα πολλαπλά στρώματα επαναλαμβανόμενων δικτύων αυξάνουν την αστάθεια της σύγκλισης του δικτύου και αποδίδουν χειρότερα από τα δίκτυα ενός στρώματος.

Ένα άλλο σημαντικό χαρακτηριστικό που στοχεύουν τα νευρωνικά δίκτυα είναι η εκμάθηση με μία βολή. Σε σύγκριση με τις τυπικές μεθόδους μάθησης, όπου η διαδικασία εκμάθησης απαιτεί εκατοντάδες ή χιλιάδες επαναλήψεις της εργασίας, η μάθηση με μία βολή στοχεύει στην εκμάθηση πληροφοριών σχετικά με την εργασία σε μία ή λίγες μόνο επαναλήψεις. Όπως δείχνει αυτή η εργασία \cite{Scherr2020}, ο \textlatin{e-prop} δεν μπορεί να επιτύχει αυτό το αποτέλεσμα. Παρ 'όλα αυτά, οι συγγραφείς προτείνουν μια μέθοδο για τα νευρωνικά δίκτυα αιχμών με τη χρήση της μεθόδου \textlatin{e-prop} να πετύχουν την εκμάθηση με μία λήψη, προσθέτοντας στη βιολογική αληθοφάνεια του δικτύου, μιμούμενη τη λειτουργία του εγκεφάλου. Στην προτεινόμενη μέθοδο, χρησιμοποιείται ένα δεύτερο βελτιστοποιημένο \textlatin{RSNN} το οποίο θα εκπέμπει τα κατάλληλα σήματα μάθησης. Με αυτόν τον τρόπο, αντιμετωπίζεται και επιλύεται το πρόβλημα των ελλειπών μελλοντικών πληροφοριών στο σήμα μάθησης. Αυτή η μέθοδος, που ονομάζεται φυσικό \textlatin{e-prop ( natural e-prop)}, δοκιμάστηκε και τα παραγόμενα αποτελέσματα δείχνουν βελτίωση σε σχέση με τις προηγούμενες εκδόσεις του \textlatin{e-prop}.

\selectlanguage{greek}
\chapter{Αισθητήρες Δυναμικής Όρασης}

\begin{wrapfigure}{r}{0.25\textwidth} %this figure will be at the right
    \centering
     \includegraphics[width=0.25\textwidth]{DVS/sensor.PNG}
    \caption{Αρχιτεκτονική ενός αισθητήρα \textlatin{DVS} της εταιρίας \textlatin{iniLabs} }
    \label{fig:dvs-sensor}
\end{wrapfigure}.

Οι αισθητήρες δυναμικής όρασης (\textlatin{Dynamic Vision Sensors}-\textlatin{DVS}) ή οι κάμερες συμβάντων είναι αισθητήρες βιολογικής έμπνευσης που λειτουργούν με κάπως διαφορετικό τρόπο από τις συμβατικές κάμερες. Υπολογίζουν τις αλλαγές φωτεινότητας ανά \textlatin{pixel} ασύγχρονα αντί να συλλέγουν εικόνες σε καθορισμένο χρόνο. Οι δυναμικοί αισθητήρες όρασης παρέχουν μεγάλα πλεονεκτήματα σε σύγκριση με τις τυπικές κάμερες. Αυτά περιλαμβάνουν υψηλότερη χρονική ανάλυση, πολύ υψηλό δυναμικό εύρος (140 \textlatin{dB} έναντι 60 \textlatin{dB}). Επίσης, καταναλώνουν πολύ λιγότερη ενέργεια και παρέχουν υψηλό εύρος ζώνης εικονοστοιχείων, το οποίο έχει ως αποτέλεσμα τη λιγότερη θολούρα κίνησης. Αυτά τα πλεονεκτήματα σε σχέση με τις τυπικές κάμερες μπορούν να βοηθήσουν τη ρομποτική που απαιτεί κάμερες χαμηλής καθυστέρησης, υψηλής ταχύτητας και αποδοτικότητας για να λειτουργήσει καλά και αποτελεσματικά ενα ρομπότ. Ο \textlatin{DVS} έχει επίσης τη δυνατότητα να μειώσει το κόστος εκπαίδευσης των βαθύ νευρωνικών δικτύων \textlatin{Computer Vision} προσφέροντας ισάξιας ή καλύτερης ακρίβειας. Ωστόσο, επειδή οι κάμερες συμβάντων διαφέρουν από τις συνηθισμένες κάμερες στο ότι μετρούν τις παραλλαγές φωτεινότητας ανά \textlatin{pixel} (που ονομάζονται "συμβάντα") ασύγχρονα και δεν εγγράφουν "απόλυτης" φωτεινότητας με σταθερό ρυθμό, χρειάζονται νέες μέθοδοι για την ανάλυση της εξόδου τους.

Μερικές εφαρμογές αυτών των αισθητήρων αποτελούν:
\begin{itemize}
    \item Ιχνηλάτιση αντικειμένων
    \item Επιτήρηση και παρακολούθηση
    \item Αναγνώριση αντικειμένου/χειρονομίας
    \item Εκτίμηση βάθους 
    \item Υπολογισμός \textlatin{Optical Flow}
    \item \textlatin{SLAM}
    \item \textlatin{Deblurring} εικόνας
\end{itemize}

\section{Αρχές λειτουργίας}

Πώς ακριβώς υπολογίζουν οι κάμερες συμβάντων τις αλλαγές φωτεινότητας ανά \textlatin{ pixel}; Κάθε συμβάν εξόδου ή ακίδα αντιπροσωπεύει μια αλλαγή φωτεινότητας (Λογαριθμική ένταση) που εξαρτάται από ένα όριο, σε μια συγκεκριμένη στιγμή. Κάθε εικονοστοιχείο πρέπει να θυμάται τη δική του ένταση καταγραφής κάθε φορά που στέλνει μια ακίδα και είναι έτοιμο να στείλει ξανά εάν γίνει αντιληπτή μια ορισμένη αλλαγή στη φωτεινότητα. Αυτό είναι διαφορετικό από τις παραδοσιακές κάμερες όπου οι πληροφορίες σχετικά με το περιβάλλον καταγράφονται σε καρέ ανά δευτερόλεπτο (σε κύκλους) αντί συνεχής παρακολούθησης του περιβάλλοντος. Ένα συμβάν εξόδου περιέχει την ($x$,$y$) τοποθεσία, την ώρα $t$ και την πολικότητα 1-\textlatin{bit} $p$ (αύξηση φωτεινότητας "ON" ή μείωση "\textlatin{OFF}, δείτε επίσης \ref{fig:dvs-overview}b,\ref{fig:dvs-overview}e, \ref{fig:dvs-overview} \cite{davis}).

\begin{wrapfigure}{r}{0.5\textwidth} %this figure will be at the right
    \centering
     \includegraphics[width=0.5\textwidth]{DVS/dvs-overview.PNG}
    \caption{\textlatin{DAVIS}}
    \label{fig:dvs-overview}
\end{wrapfigure}
Τα συμβάντα εξάγονται από την κάμερα χρησιμοποιώντας ένα κοινόχρηστο ψηφιακό δίαυλο εξόδου, συνήθως χρησιμοποιώντας ανάγνωση διεύθυνσης-συμβάντος (\textlatin{AER}) \cite{boahen2004} \cite{liu2015}. Ο όγκος των σημείων δεδομένων που παράγονται από αυτούς τους αισθητήρες εξαρτάται από τον όγκο των πληροφοριών που αλλάζουν στο χρόνο, στο περιβάλλον. Αυτό επιτυγχάνεται με την προσαρμογή του ρυθμού δειγματοληψίας στον ρυθμό μεταβολής της λογαριθμικής έντασης του σήματος . Η χρονική ανάλυση είναι της τάξης των μικροδευτερολέπτων, ενώ η καθυστέρηση είναι λίγο πιο αργή στη σειρά των \textlatin{ms}. Αυτό επιτρέπει στους αισθητήρες να αντιδρούν γρήγορα σε οπτικά ερεθίσματα και καθιστά τους αισθητήρες κατάλληλους για αυτόνομη οδήγηση. Επιπλέον, τα συμβάντα αλλαγής φωτεινότητας των \textlatin{DVS} παραμένουν αναλλοιώτα απο τον φωτισμό της σκηνής.

\section{Διαθέσιμες Συσκευές}

Εκτός από την αρχική κάμερα \textlatin{DVS} \cite{Lichtsteiner2008}, υπήρξαν πρόσφατες εξελίξεις \cite{posch2014} , \cite{liu2015}, \cite{indiveri2015}, \cite{delbruck2010} .
Ένας από τους πιο ευρέως χρησιμοποιούμενους αισθητήρες είναι ο \textlatin{Dynamic and Active Pixel Vision Sensor} \cite{davis} \ref{fig:dvs-sensor}.Ο \textlatin{DAVIS } συνδυάζει έναν συμβατικό ενεργό αισθητήρα pixel (\textlatin{active pixel sensor}-\textlatin{APS})  \cite{fossum1997}  στο ίδιο \textlatin{pixel} με ενός \textlatin{DVS}.
\subsubsection{Προκλήσεις}
\begin{itemize}
    \item Η έξοδος των καμερών συμβάντων διαφέρει σημαντικά από τις συμβατικές κάμερες: τα γεγονότα είναι ασύγχρονα και χωρικά αραιά, ενώ οι εικόνες είναι σύγχρονες και πυκνές. Ως αποτέλεσμα, δεν εφαρμόζονται αλγόριθμοι όρασης βασισμένοι σε ανάλυση πλαισίων.
    \item Κάθε συμβάν περιέχει πληροφορίες σχετικά με τον τρόπο αλλαγής της φωτεινότητας. Ωστόσο, οι αλλαγές φωτεινότητας δεν σχετίζονται μόνο με την τρέχουσα φωτεινότητα της σκηνής, αλλά με την τρέχουσα και την προηγούμενη σχετική κίνηση μεταξύ της σκηνής και της κάμερας.
    \item Θόρυβος: Λόγω του εγγενή θορύβου λήψης στα φωτόνια και του θορύβου κυκλωμάτων τρανζίστορ, όλοι οι αισθητήρες όρασης είναι θορυβώδεις.
\end{itemize}
\section{Παραγωγή Συμβάντων(\textlatin{Event Generation})}

Ένας αισθητήρας συμβάντων \cite{Lichtsteiner2008} έχει ανεξάρτητα εικονοστοιχεία που ανταποκρίνονται
σε αλλαγές της λογαριθμικής έντασης της φωτεινότητας $L=\textlatin{log}(\textlatin{I})$ . Ένα συμβάν $e\textsubscript{k}=(\textlatin{x}\textsubscript{\textlatin{k}},t\textsubscript{\textlatin{k}},p\textsubscript{k}$) ενεργοποιείται στο \textlatin{pixel} $\textlatin{x}\textsubscript{\textlatin{k}}=(\textlatin{x}\textsubscript{\textlatin{k}},y\textsubscript{\textlatin{k}})^T$ στη χρονική στιγμή $t_k$ όταν η φωτεινότητα αυξάνει ένα ορισμένο χρονικό όριο αντίθεσης $C$ από το τελευταίο συμβάν.

\begin{equation}
    \Delta L(\textlatin{x}\textsubscript{\textlatin{k}},\textlatin{t}\textsubscript{\textlatin{k}}) = L(\textlatin{x}\textsubscript{\textlatin{k}},t\textsubscript{\textlatin{k}})-(\textlatin{x}\textsubscript{\textlatin{k}},t\textsubscript{\textlatin{k}}-\Delta t\textsubscript{\textlatin{k}})
\end{equation}
\selectlanguage{english}
\begin{equation}
   \Delta L(x\textsubscript{k},t\textsubscript{k}) = p\textsubscript{k}C 
\end{equation}
\selectlanguage{greek}
Όπου \(C > 0, \Delta t\textsubscript{\textlatin{k}}\) είναι το χρονικό διάστημα που μεσολαβεί από το τελευταίο συμβάν στο ίδιο \textlatin{pixel} και η πολικότητα \(p\textsubscript{\textlatin{k}} \in {+1,-1}\) είναι το πρόσημο της αλλαγής φωτεινότητας\cite{Lichtsteiner2008}. Η Ευαισθησία αντίθεσης $C$ καθορίζεται από τα ρεύματα πόλωσης του \textlatin{pixel} \cite{nozaki2017} \cite{Gallego2020}, τα οποία επίσης ορίζουν την ταχύτητα και το κατώφλι του ανιχνευτή αλλαγής στο \ref{fig:dvs-overview}. Τα τυπικά κατώφλια \textlatin{DVS} ορίζονται στο 10-50 τοις εκατό της αλλαγής φωτισμού. Υπάρχουν πιο προηγμένες μέθοδοι δημιουργίας γεγονότων, αλλά δεν αποτελούν το αντικείμενο αυτής της διατριβής. Ορισμένες από αυτές τις μεθόδους περιλαμβάνουν τον καθορισμό ενός ορίου στο μέγεθος της αλλαγής φωτεινότητας από το τελευταίο συμβάν και χρησιμοποιούνται ως βάση αλγορίθμων που βασίζονται σε φυσικά γεγονότα \cite{Gallego2019}. Τα συμβάντα μπορούν επίσης να ρυθμιστούν ώστε να δημιουργούνται απο κινούμενες ακμές. Τα πιο πολύπλοκα μοντέλα περιλαμβάνουν τη λήψη θορύβου αισθητήρα και την αναντιστοιχία τρανζίστορ καθιστώντας αυτά τα μοντέλα στοχαστικά.

\section{Επεξεργασία συμβάντων}
Η επεξεργασία συμβάντων στοχεύει στην εξαγωγή χρήσιμων πληροφοριών από τα δεδομένα, αλλά εξαρτάται επίσης από τον τύπο της εφαρμογής που μας ενδιαφέρει. Προσεγγίσεις για την επεξεργασία συμβάντων που λειτουργούν κατά συμβάν σε συμβάν, με την κατάσταση του συστήματος (εκτιμώμενοι άγνωστοι) να αλλάζει με κάθε άφιξη ενός μεμονωμένου συμβάντος,έχουν τη μικρότερη δυνατή καθυστέρηση,όμως δεν παρέχουν αρκετές πληροφορίες για εκτίμηση κίνησης και έτσι οι μέθοδοι που λειτουργούν σε γεγονότα σε ομάδες ή πακέτα είναι πιο κατάλληλες. Ωστόσο, αυτά οδηγούν σε αυξημένη καθυστέρηση, αλλά εξακολουθούν να παρέχουν μια  ενημέρωση κατάστασης σε κάθε άφιξη ενός συμβάντος. Δεδομένου ότι τα γεγονότα υποβάλλονται σε επεξεργασία σε ένα βελτιστοποιημένο πλαίσιο, μια άλλη διάκριση είναι ο τύπος της συνάρτησης αντικειμένου ή απώλειας που χρησιμοποιείται: γεωμετρική έναντι χρονικής έναντι φωτομετρικής (π.χ., συνάρτηση της πολικότητας ή της δραστηριότητας γεγονότων).

\section{Μέθοδοι αναπαράστασης συμβάντων}
Αυτές οι μέθοδοι μπορούν να θεωρηθούν ως το στάδιο προεπεξεργασίας πριν από την εισαγωγή των δεδομένων στο επιθυμητό νευρωνικό δίκτυο. Η επιλογή ωστόσο επηρεάζεται επίσης από τον τύπο νευρωνικού δικτύου που χρησιμοποιείται.
\begin{figure}[htp] %this figure will be at the right
    \centering
     \includegraphics[width=16cm]{DVS/representation-methods.PNG}
    \caption{Αναπαράσταση συμβάντων}
    \label{fig:representation-methods}
\end{figure}


(\textlatin{a}) Γεγονότα στο χωροχρόνο (η θετική πολικότητα είναι σε μπλε χρώμα ενώ η αρνητική πολικότητα στο κόκκινο).

(\textlatin{b}) Τα συμβάντα συσσωρεύονται σε μια δισδιάστατη εικόνα. Αυτό επιτρέπει την εφαρμογή συμβατικών αλγορίθμων όρασης υπολογιστή. Ωστόσο, αυτή η μέθοδος έρχεται σε αντίθεση με τον στόχο μας: να καταγράφουμε γεγονότα στη χρονική διάσταση,οπότε χάνουμε πολύτιμες πληροφορίες .

(\textlatin{c}) Η επιφάνεια χρόνου είναι ένας δισδιάστατος χάρτης όπου κάθε εικονοστοιχείο αποθηκεύει μια μοναδική τιμή χρόνου. Αυτό ονομάζεται \textlatin{Motion History Images} στην όραση υπολογιστή \cite{ahad2012}. Ένα παράδειγμα αποτελεί το \cite{lagorge2017} .Τα γεγονότα μπορούν να μετατραπούν σε εικόνα που αντιπροσωπεύει το πρόσφατο ιστορικό κίνησης που εκθέτει τις πλούσιες χρονικές πληροφορίες των καταγεγραμμένων δεδομένων. Όμως,Η αποτελεσματικότητά τους υποβαθμίζεται σε σκηνές με υφή.

(\textlatin{d}) Παρόμοια έννοια του (\textlatin{c}) αλλά τώρα η 3η διάσταση επιτρέπει την αποθήκευση ολόκληρης της ακολουθίας των γεγονότων (μόνο τα αρνητικά γεγονότα φαίνονται στο σχήμα).

(\textlatin{e}) Εικόνα συμβάντος με αντιστάθμιση κίνησης \cite{Gallego2020}. Αναπαράσταση που βασίζεται τόσο σε γεγονότα όσο και σε υποθέσεις κίνησης. Η θεωρία πίσω από την αντιστάθμιση κίνησης είναι ότι καθώς μια ακμή κινείται πάνω από το επίπεδο της εικόνας, δημιουργεί γεγονότα στα εικονοστοιχεία από τα οποία διέρχεται. η κίνηση της ακίδας μπορεί να υπολογιστεί με στρέβλωση των γεγονότων σε χρόνο αναφοράς και βελτιστοποίηση της ευθυγράμμισης τους, με αποτέλεσμα μια ευκρινή εικόνα (δηλ., ιστόγραμμα) στρεβλωμένων γεγονότων. Αυτό είναι κυρίως χρήσιμο εάν κάποιος θέλει να ιχνιλατεί αντικείμενα σε ένα βίντεο.

(\textlatin{f}) Ανακατασκευή ένταση εικόνας\cite{rebecq2019}.


\chapter{Υπολογιστικά Πειράματα}

Στο επόμενο κεφάλαιο, χρησιμοποιούμε την αναπτυγμένη θεωρία σχετικά με τα Νευρωνικά Δίκτυα Αιχμών για να συγκρίνουμε την απόδοση τέτοιων δικτύων σε σύνολα δεδομένων από αισθητήρες δυναμικής όρασης. Η επιλεγμένη εργασία που τα ΝΔΑ καλείται να επιλύσει ανήκει στην κατηγορία της όρασης, όπου τα δίκτυα που θα αναπτυχθούν θα πρέπει να εκπαιδευτούν ώστε να αναγνωρίζουν και να ταξινομούν χειρονομίες που έχουν εγγραφεί με έναν αισθητήρα δυναμικής όρασης (\textlatin{DVS} - κεφάλαιο 5). Η επιλογή των δεδομένων \textlatin{DVS} ήταν προτιμότερη, καθώς οι αισθητήρες δυναμικής όρασης παρουσιάζουν πολλαπλά πλεονεκτήματα έναντι των αισθητήρων \textlatin{RGB}, κυρίως στην κατανάλωση ενέργειας και ταχύτητα επεξεργασίας και έχουν υψηλές προοπτικές για μελλοντικές χρήσεις.
% Χρειάζεται επεξεργασία

\section{Σύνολο Δεδομένων}

Για να επωφεληθούμε από την χρονική πληροφορία, επιλέξαμε ένα σύνολο δεδομένων που έχει χρονική συνιστώσα από μόνο του. Σε αντίθεση με τα μη χρονικά σύνολα δεδομένων, όπως οι ακίνητες εικόνες, που απαιτούν είτε σχήματα κωδικοποίησης όπως αυτά που περιγράφονται στο κεφάλαιο 4.2 για τη μετατροπή των δεδομένων σε τρένα αιχμών, είτε την κατασκευή ενός συνόλου δεδομένων \textlatin{DVS} των εικόνων μετακινώντας τα μπροστά από έναν αισθητήρα δυναμικής όρασης (π.χ. το σύνολο δεδομένων \textlatin{DVS MNIST} \cite{Gotarredona2015}), ένα σύνολο δεδομένων που εξαρτάται από το χρόνο, όπως το βίντεο, θα μπορούσε να εκμεταλλευτεί πλήρως τις δυνατότητες που παρέχουν τα νευρικά δίκτυα αιχμών.

Έχοντας αυτό υπόψη, αποφασίσαμε να δοκιμάσουμε να αναπτύξουμς ΝΔΑ και να τα εκπαιδεύσουμε στο σύνολο δεδομένων \textlatin{DVSGesture} \cite{Amir2017}. Οι χειρονομίες καταγράφηκαν από την κάμερα \textlatin{iniLabs DVS128}, η οποία είναι αισθητήρας δυναμικής όρασης μεγέθους εισόδου 128x128 εικονοστοιχεία. Αυτός ο αισθητήρας παράγει μια αιχμή σε ένα από τα 128x128 εικονοστοιχεία όταν η τιμή του εικονοστοιχείου αλλάζει πάνω από ένα όριο που καθορίζεται από τον χρήστη. Η αιχμή κωδικοποιείται ως ένα γεγονός όπου αποθηκεύεται η χρονική σήμανση της αιχμής και οι χωρικές συντεταγμένες της. Το σύνολο δεδομένων αποτελείται από 11 κλάσεις, κάθε μια από τις οποίες αντιπροσωπεύει μια χειρονομία. 29 άτομα συμμετείχαν στην δημιουργία του συνόλου δεδομένων και έκαναν κάθε μία από τις χειρονομίες σε τουλάχιστον 3 διαφορετικές συνθήκες φωτισμού δημιουργώντας ένα μεγάλο σύνοδο δεδομένων αποτελούμενο από 1341 αντικείμενα - χειρονομίες οι οποίες σημειώθηκαν με έναν αριθμό από το 0 έως το 10, ο οποίος υποδηλώνει την κλάση στην οποία ανήκουν. Οι 11 χειρονομίες είναι οι ακόλουθες:
\begin{enumerate}
    \setcounter{enumi}{0}
    \item Χειροκρότημα
    \item Χαιρετισμός αριστερού χεριού
    \item Χαιρετισμός δεξιού χεριού
    \item Περιστροφή δεξιού χεριού δεξιόστροφα
    \item Περιστροφή δεξιού χεριού αριστερόστροφα
    \item Περιστροφή αριστερού χεριού δεξιόστροφα
    \item Περιστροφή αριστερού χεριού αριστερόστροφα
    \item Βαρελάκια
    \item Τύμπανα στον αέρα
    \item Κιθάρα στον αέρα
    \item Άλλο
\end{enumerate}

\medskip
Στο σχ. 6.1 βλέπουμε τα γεγονότα που παράγονται από τον Αισθητήρα δυναμικής όρασης σε 2 δευτερόλεπτα ενός παραδείγματος κάθε τάξης, εκτός από την κλάση "Άλλο".

\begin{figure}[htp] %this figure will be at the right
    \centering
     \includegraphics[width=13cm]{Experiments/Two-second-snippets-of-10-classes-in-DVS128-Gesture-Dataset.png}
    \caption{Ένα παράδειγμα γεγονότων που παράγονται από τον αισθητήρα σε 2 δευτερόλεπτα εγγραφής. Τα γεγονότα συσσωρεύονται στη χρονική διάσταση με το κίτρινο χρώμα να αντιπροσωπεύει τις πρώτες αιχμές ενώ το μωβ τις τελευταίες.}
    \label{fig:representation-methods}
\end{figure}

Για να φορτώσουμε το σύνολο δεδομένων στην επιθυμητή μορφή, χρησιμοποιούμε τη βιβλιοθήκη \textlatin{Tonic} της \textlatin{python (https://github.com/neuromorphs/tonic)}. Η βιβλιοθήκη χωρίζει το σύνολο δεδομένων σε σύνολο εκπαίδευσης και σύνολο δοκιμών, που αποτελούνται από τις χειρονομίες 18 και 11 ατόμων αντίστοιχα σε όλες τις συνθήκες φωτισμού. Αυτά τα σύνολα είναι αντικείμενα υλοποιημένων κλάσεων \textlatin{Tonic} και κάθε χειρονομία είναι ένας τανυστής που περιέχει όλα τα συμβάντα στην ακόλουθη μορφή: Χρονική σήμανση του συμβάντος - άξονας Χ - άξονας Υ - πολικότητα του συμβάντος. Η πολικότητα αντιπροσωπεύεται ως δυαδική τιμή όπου το 0 αντιστοιχεί σε "αρνητική" αιχμή και το 1 σε θετική αιχμή. Η πολικότητα δείχνει εάν η ακίδα δημιουργήθηκε λόγω αύξησης ή μείωσης της τιμής του αντίστοιχου εικονοστοιχείου. Η προσθήκη των πληροφοριών πολικότητας μετασχηματίζει ελαφρώς το σύνολο δεδομένων. Στο σχ. 6.2 τα πράσινα εικονοστοιχεία υποδεικνύουν τη θετική ακίδα ενώ τα κόκκινα την αρνητική ακίδα, προσθέτοντας συνολικά στην πληροφορία εισόδου, καθώς το δίκτυο θα μπορούσε δυνητικά να χρησιμοποιήσει τη πληροφορία της πολικότητας για να εξαγάγει πληροφορίες στη χρονική διάσταση.

\begin{figure}[htp] %this figure will be at the right
    \centering
     \includegraphics[width=13cm]{Experiments/handwave.jpg}
    \caption{Δείγματα χαιρετισμού δεξιού χεριού τα οποία απέχουν 0,5 δευτερόλεπτα το κάθε ένα. Κάθε μη μαύρο εικονοστοιχείο αντιπροσωπεύει μια αιχμή που παράγεται από τον αισθητήρα. Τα πράσινα χρώματα υποδηλώνουν αύξηση της τιμής του εικονοστοιχείου πάνω από το όριο, ενώ τα κόκκινα δείχνουν μείωση της τιμής του εικονοστοιχείου στη δεδομένη περίπτωση.}
    \label{fig:representation-methods}
\end{figure}

Η χρήση της βιβλιοθήκης \textlatin{Tonic} παρέχει επίσης σημαντικούς μετασχηματισμούς στο σύνολο δεδομένων. Οι πιο αξιοσημείωτοι από τους μετασχηματισμούς που εφαρμόστηκαν και στα πειράματα είναι οι μετασχηματισμοί \textlatin{Denoising} και \textlatin{Spatiotemporal Downsample} δηλαδή υποδειγρατοληψία στις χρονικές ή/και χωρικές συντεταγμένες. Το \textlatin{Denoising} χρησιμοποιείται για να φιλτράρει γεγονότα που πιθανώς προκαλούνται από θόρυβο. Αυτός ο μετασχηματισμός απορρίπτει γεγονότα που δεν έχουν χωρικό γείτονα εντός καθορισμένης από τον χρήστη χρονικής διάρκειας, διαγράφοντας τα από την είσοδο. Η μείωση της δειγματοληψίας μπορεί να είναι ο πιο σημαντικός μετασχηματισμός. Η χωρική υποδειγματοληψεία μπορεί να χρησιμοποιηθεί για τη μείωση του μεγέθους της εισόδου αφού τα αρχικά δεδομένα απαιτούν \(128*128 = 16384\) νευρώνες εισόδου και αρκετά μεγάλο χώρο μνήμης κατά την επεξεργασία. Η χρονική μείωση της δειγματοληψίας έχει ως αποτέλεσμα οι χρονικές σημάνσεις των συμβάντων να πολλαπλασιάζονται με τιμή μικρότερη από 1, με αποτέλεσμα τα γεγονότα να τροφοδοτούνται πιο γρήγορα στο δίκτυο και έτσι οι προηγούμενες πληροφορίες να μην χάνονται στο χρόνο. Θα πρέπει να σημειωθεί ότι η υποδειγματοληψία του χρόνου δεν απορρίπτει κανένα συμβάν.

Η βιβλιοθήκη \textlatin{Tonic} υλοποιεί επίσης μετασχηματισμούς για να αντιμετωπίσει την ευρωστία του δικτύου, μπορεί να πειράξει δηλαδή τα δεδομένα προκειμένου το δίκτυο να εστιάσει και να μάθει γενικές λεπτομέρειες του συνόλου δεδομένων. Τέτοιοι μετασχηματισμοί περιλαμβάνουν μια τυχαία απόρριψη γεγονότων, αναστροφή χωρικών και χρονικών διαστάσεων, αντιστροφή του χρόνου, ανατροπή των πολικότητας, τρεμοπαίξιμο γεγονότα χωροχρονικά και πολλά άλλα, τα οποία μπορεί να είναι πολύ λίγα σε πειράματα.

\section{Εκπαιδεύοντας το ΝΔΑ}

Έχοντας έτοιμο το σύνολο δεδομένων, αποφασίσαμε να εστιάσουμε την προσοχή μας στις πιο καινοτόμες μεθόδους εκπαίδευσης νευρωνικών δικτύων αιχμών. Ως αποτέλεσμα, επιλέξαμε τρεις μεθόδους εκπαίδευσης, \textlatin{Back Propagation Through Time (BPTT)}, μια μέθοδο που έχει χρησιμοποιηθεί στο παρελθόν για ΝΔΑ, ως σημείο αναφοράς, το \textlatin{Decolle} (κεφ. 4.3.5), ένας κανόνας εκμάθησης κατάλληλος για ΝΔΑ και \textlatin{e-prop} (κεφ. 4.3.7), μέθοδος που χρησιμοποιείται κυρίως σε επαναλαμβανόμενα ΝΔΑ. Η επιλογή αυτών των 2 μεθόδων μεταξύ αυτών που παρουσιάζονται σε αυτήν τη διατριβή ή έχουν παρουσιαστεί στον τομέα της έρευνας γενικά, βασίζεται στην καινοτομία των μεθόδων και στην πρόσφατη ανάπτυξή τους, έχοντας δημοσιευτεί το 2020 και το 2019 αντίστοιχα. Επιπλέον, η μέθοδος \textlatin{e-prop}, ενώ έχει δοκιμαστεί στην επίλυση χρονικών αναθέσεων, δεν έχει αντιμετωπίσει ακόμη τόσο πολύπλοκα, όσον αφορά τη χρονική διάσταση, σύνολα δεδομένων. Το \textlatin{Decolle} έχει δοκιμαστεί στο παρελθόν στο ίδιο σύνολο δεδομένων, αλλά σε αυτήν την εργασία, πειραματιζόμαστε με το μέγεθος του δικτύου και άλλες υπερπαραμέτρους προκειμένου να συλλέξουμε περισσότερες πληροφορίες σχετικά με τον κανόνα εκμάθησης και να το συγκρίνουμε με τους ανταγωνιστές του.

\section{Εκπαίδευση με τον αλγόριθμο \textlatin{BPTT}}

Ο αλγόριθμος \textlatin{Back Propagation Through Time (BPTT)} είναι ένα συνηθισμένο σχήμα εκμάθησης για επαναλαμβανόμενα νευρωνικά δίκτυα. Λόγω της χρονικής διάστασης των δεδομένων των νευρωνικών δικτύων, οι αλγόριθμοι \textlatin{BPTT}, όπως αυτός που αναπτύχθηκε εδώ \cite{Wu2018}, χρησιμοποιούνται επίσης στην εκπαίδευση ΝΔΑ, είτε προωθητικών είτε επαναλαμβανόμενων δικτύων. Ως αποτέλεσμα, αποφασίσαμε να εκπαιδεύσουμε ένα ΝΔΑ με \textlatin{BPTT} έτσι ώστε να το χρησιμοποιήσουμε ως σημείο αναφοράς και να συγκρίνουμε τα αποτελέσματα με τις νεότερες μεθόδους που δοκιμάζονται σε αυτήν τη διατριβή.

\subsection{Αρχικοποιήση}

Η ανάπτυξη του νευρωνικού δικτύου και η εκπαίδευσή του έγιναν χρησιμοποιώντας τη βιβλιοθήκη \textlatin{Norse} της python \cite{norse2021}. Το \textlatin{Norse} είναι μια βιβλιοθήκη που δημιουργήθηκε για την ανάπτυξη Νευρωνικών Δικτύων Αιχμών και δίνει την δυνατότητα επιλογής της δομής του δικτύου όπως σχετικά με τα στοιχεία των δικτύων π.χ. οι νευρώνες που θα το αποτελούν (\textlatin{IF}, \textlatin{LIF}, \textlatin{Izhikevich} και άλλοι νευρώνες έχουν υλοποιηθεί στην βιβλιοθήκη) ή τα σχήματα κωδικοποίησης εισόδου εάν αυτά είναι απαραίτητα (δεν είναι απαραίτητα για τον σκοπό μας) παρέχοντας ένα πλαίσιο για έναν εύκολο τρόπο ανάπτυξης ΝΔΑ και ελέγχου των υπερπαραμέτρων του. Το \textlatin{Norse} αποτελεί επέκταση του \textlatin{PyTorch} με τα προαναφερθέντα χαρακτηριστικά. Το πιο σημαντικό, μπορεί να χειριστεί σύνολα δεδομένων φορτωμένων από την βιβλιοθήκη \textlatin{Tonic}. Αυτό μας βοηθά στο να φορτώσουμε το σύνολο δεδομένων και να εφαρμόσουμε σε αυτό τις απαραίτητες μετατροπές για να το διαχειριστούμε και να το διαμορφώσουμε με τον επιθυμητό τρόπο για τους σκοπούς της δοκιμής μας.

Προκειμένου να εξαχθούν σημαντικές πληροφορίες, αναπτύσσονται πολλά δίκτυα, μεταβάλλοντας μία ή περισσότερες παραμέτρους του καθενός. Οι εφαρμοζόμενοι μετασχηματισμοί του \textlatin{Tonic} και η εύκολη χρήση αυτών των δεδομένων από το \textlatin{Norse} μας επέτρεψαν να πειραματιστούμε τόσο με τις παραμέτρους των νευρώνων, όπως το κατώφλι τάσης όσο και με χωροχρονικούς χειρισμούς στα δεδομένα για καλύτερη απόδοση στην ακρίβεια των δικτύων και την ταχύτητα των δοκιμών.

\subsection{Παραλλαγές Δικτύων}

Τα δίκτυα που αναπτύχθηκαν για την δοκιμή του \textlatin{BPTT} ήταν επαναλαμβανόμενα ΝΔΑ. Πειραματιζόμαστε με δίκτυα διαφορετικού μεγέθους και δεδομένου ότι ο μετασχηματισμός των δεδομένων έγινε από τη βιβλιοθήκη \textlatin{Tonic}, δοκιμάσαμε μια ποικιλία από αυτούς που τους εφαρμόσαμε στο σύνολο δεδομένων για να βρούμε τις καλύτερες τιμές μετασχηματισμών για τη συγκεκριμένη εργασία. Τα δομικά στοιχεία του δικτύου, όπως οι νευρώνες, άλλαξαν επίσης για να λάβουμε πληροφορίες σχετικά με την απόκριση και την απόδοση του δικτύου. Συνολικά, εστιάσαμε σε τρεις κύριες πτυχές της βελτιστοποίησης:

\begin{itemize}
    \item Βελτιστοποίηση προεπεξεργασίας
    \item Βελτιστοποίηση αρχιτεκτονικής δικτύου
    \item Βελτιστοποίηση εκπαίδευσης
\end{itemize}

Δεδομένου ότι τα δεδομένα που λαμβάνονται από τον αισθητήρα, λόγω του υψηλού ρυθμού λήψης, ήταν πολύ μεγάλα για να εκπαιδεύσουν το δίκτυο ή ακόμη και να χωρέσουν στη μνήμη, χρησιμοποιήσαμε χωροχρονικούς μετασχηματισμούς για να μειώσουμε το μέγεθος των δεδομένων. Συμπιέσαμε τα γεγονότα με συντελεστή στην περιοχή τιμών από 1000 έως 100000, που είναι το ίδιο με το να λέμε ότι το δίκτυο χειρίζεται πληροφορίες κάθε 1 έως 100 χιλιοστά του δευτερολέπτου (αφού ο ρυθμός καρέ του αισθητήρα είναι περίπου 100 \textlatin{kHz}). Αυτό προκαλεί επίσης τα γεγονότα που τροφοδοτούνται στο δίκτυο να είναι πιο κοντά στο χρόνο διατηρώντας τους χρονικούς συσχετισμούς τους χωρίς να αφήνουν την επίδρασή τους να εξαφανιστεί στο χρόνο. Ωστόσο, η υπερδειγματοληψία στη χρονική διάσταση μπορεί να προκαλέσει τη συσσώρευση πάρα πολλών συμβάντων σε ένα στιγμιότυπο εισόδου - χρονική σήμανση εισόδου και το δίκτυο να μην μπορεί να εξαγάγει σημαντικές πληροφορίες. Αυτός είναι ο λόγος που πειραματιζόμαστε με πολλαπλή μείωση δειγμάτων, για να επιτύχουμε την καλύτερη απόδοση με λογική ταχύτητα εκπαίδευσης.

Εξίσου σημαντική είναι και η μείωση της χωρικής διάστασης. Η είσοδος 128\textlatin{x}128 του αισθητήρα δίνει ένα αρκετά μεγάλο επίπεδο νευρώνων εισόδου, το οποίο είναι ακριβό από άποψη χωρητικότητας μνήμης και επίσης προκαλεί αραιά γεγονότα στη χωρική διάσταση. Αυτό μας επιτρέπει να μειώσουμε το μέγεθος εισόδου με τη δειγματοληψία των δεδομένων και την εξοικονόμηση μνήμης και χρόνου χωρίς σημαντική επίδραση στην απόδοση (με πιθανότητα αύξησης της απόδοσης με μείωση δείγματος). Για να δούμε την επίδραση της δειγματοληψίας των δεδομένων εισόδου στο δίκτυο, δοκιμάζουμε δίκτυα με μεγέθη εισόδου 128\textlatin{x}128, 64\textlatin{x}64 και ακόμη και 32\textlatin{x}32 και συγκρίνουμε τα αποτελέσματα.

Τα δομικά στοιχεία του δικτύου, οι νευρώνες, είναι αυτά που καθορίζουν το δίκτυο και την απόδοσή του. Εξοπλισμένοι με τις πληροφορίες που αποκτήθηκαν από την προηγούμενη ανάλυση στην παρούσα διπλωματική εργασία για τη δυναμική των νευρώνων, αποφασίσαμε επίσης να πειράξουμε τις υπερπαραμέτρους που επηρεάζουν την απόκριση τους. Τα πιο αξιοσημείωτα και σημαντικά, που δοκιμάστηκαν στα ακόλουθα πειράματα, είναι το κατώφλι τάσης και οι σταθερές πτώσης της μεμβράνης και της σύναψης. Το κατώφλι τάσης δοκιμάστηκε για τιμές 1 και 1.5 για να εξεταστεί η διαφορά στη δυναμική του δικτύου σε τέτοιες περιπτώσεις. Οι σταθερές πτώσης πειράχτηκαν για να μελετήσουν τη συμπεριφορά του δικτύου όταν ο νευρώνας τείνει να διατηρεί το δυναμικό της μεμβράνης (ενεργώντας περισσότερο σαν νευρώνας ολοκλήρωσης και πυροδότησης - \textlatin{Integrate and Fire}) και όταν η άμεση τροφοδοσία αιχμών είναι ζωτικής σημασίας για την πυροδότηση του νευρώνα.

Συνολικά, για να αντλήσουμε τις περισσότερες πληροφορίες σε ικανοποιητικό χρόνο εκπαίδευσης, δεδομένου ότι η βιβλιοθήκη \textlatin{Norse} δεν ήταν βελτιστοποιημένη για εργασίες τέτοιου μεγέθους (με προβλήματα ταχύτητας σε σύγκριση με το βελτιστοποιημένο \textlatin{DECOLLE}), εφαρμόσαμε 10 επαναλαμβανόμενα νευρωνικά δίκτυα, όπου οι υπερπαραμέτρές τους είναι τέτοιες, ώστε να μας δωθεί η δυνατότητα εξαγωγής πληροφοριών. Εφαρμόσαμε επίσης ορισμένες τροποποιήσεις όπως η προσθήκη συντελεστή πιθανότητας πτώσης (όπου τα γεγονότα αγνοούνται τυχαία, για αύξηση της ευρωστίας του δικτύου) και η αλλαγή του συντελεστή άλφα, μια τιμή που έχει να κάνει με τον υποκατάστατο υπολογισμό κλίσης. Παρόλο που δεν διερευνάται ολόκληρος ο χώρος αναζήτησης για την εύρεση καλύτερης ρύθμισης δικτύου, καταφέρνουμε να έχουμε σημαντικά αποτελέσματα από τις δοκιμές.

\subsection{Αποτελέσματα}

Η προεπεξεργασία του συνόλου δεδομένων φαίνεται να επηρεάζει σημαντικά το δίκτυο. Στην εικ. 6.3 και στην εικ. 6.4 βλέπουμε δύο πανομοιότυπα δίκτυα (ή θα μπορούσαν να θεωρηθούν ως το ίδιο δίκτυο) για την επεξεργασία του συνόλου δεδομένων εισόδου με χρονική δειγρατοληψία 100000 και 1000000 αντίστοιχα. Όπως βλέπουμε, η αυξημένη δειγματοληψία στη χρονική διάσταση έχει ως αποτέλεσμα χειρότερη απόδοση. Η χρονική υποδειγματοληψία προκαλεί τη συγκέντρωση συμβάντων στην ίδια χρονική σήμανση με αποτέλεσμα η είσοδος σε κάθε χρονική στιγμή να κατακλύζεται από γεγονότα που συμβαίνουν στην πραγματικότητα σε μεγάλες αποστάσεις στην διάσταση του χρόνου, με αποτέλεσμα την απώλεια πληροφοριών και χειρότερη απόδοση του δικτύου. Από την άλλη πλευρά, η μείωση της χρονικής δειγματοληψίας (αύξηση του συντελεστή πολλαπλασιασμού) οδηγεί επίσης σε κακές επιδόσεις (εικ. 6.11). Ενώ αυτό φαίνεται αντιφατικό, μπορεί να εξηγηθεί από το γεγονός ότι τα γεγονότα μπορεί να είναι πολύ αραιά στο χρόνο και ο συσχετισμός τους μεταξύ στην χρονική διάσταση να χανεται λόγω της άμεσης πυροδότησης των νευρώνων.

\begin{figure}
\centering
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Norse/correct_perc/acc_02.png}
  \captionof{figure}{Δίκτυο με χρονική υποδειγματοληψία 10\textsuperscript{5}}
  \label{fig:test1}
\end{minipage}
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Norse/correct_perc/acc_10.png}
  \captionof{figure}{Δίκτυο με χρονική υποδειγματοληψία 10\textsuperscript{6}}
  \label{fig:test2}
\end{minipage}
\end{figure}

Ένας άλλος τύπος δειγματοληψίας που έχει εφαρμοστεί είναι η χωρική δειγματοληψία. Τα αποτελέσματα δείχνουν ότι καθόλου ή χαμηλή χωροταξική δειγματοληψία (αφήνοντας το μέγεθος εισόδου σε δεκάδες χιλιάδες νευρώνες εισόδου - 16384 χωρίς δειγματοληψία) προκαλεί υψηλή αστάθεια και χαοτική συμπεριφορά (εικ. 6.5). Ωστόσο, μπορεί να φανεί ότι το δίκτυο αυξάνει την απόδοσή σε βάθος των εποχών. Αυτό μπορεί να μην είναι τόσο προφανές από το διάγραμμα ακρίβειας, αλλά μπορεί να φανεί όταν σχεδιάζουμε τη συνολική απώλεια του δικτύου για κάθε εποχή εικ. 6.3 - όπου χρησιμοποιήθηκε παράγοντας χωρικής μείωσης 0.25. Η φθίνουσα συνολική απώλεια που προκύπτει σε κάθε εποχή επισημαίνει ότι το δίκτυο όντως μαθαίνει. Ωστόσο, όταν συγκρίνουμε την απόδοση σε σύγκριση με το σχ. 6.5, η αύξηση της απώλειας δοκιμής της εικ. 6.3 (ή η μείωση της ακρίβειάς του) στις τελευταίες εποχές καθιστά την επιλογή μιας τέτοιας ρύθμισης μη ελκυστική, υποστηριζόμενη από το γεγονός ότι στις τελευταίες εποχές και τα δύο δίκτυα (εικ. 6.5, εικ. 6.3 ) έχουν επιτύχει παρόμοια αποτελέσματα, αν και θα πρέπει να γίνει περαιτέρω εκπαίδευση για πιο καταληκτικά αποτελέσματα. Αυτή η ασταθής συμπεριφορά του δικτύου σε αυτήν τη ρύθμιση μπορεί να οφείλεται στην αραιότητα των συμβάντων στο χώρο εισόδου. Ένα τεράστιο μέγεθος εισόδου σε ένα τόσο μικρό κρυφό επίπεδο, που αποτελείται μόνο από 50 νευρώνες, μπορεί να προκαλέσει αστάθεια, καθώς το κρυφό επίπεδο δεν μπορεί να διακρίνει μοτίβα από μια τόσο μεγάλη είσοδο, ή ίσως χρειάζεται περισσότερος χρόνος εκπαίδευσης για να επιτευχθεί η αναγνώριση του μοτίβου.

\begin{figure}[htp] %this figure will be at the right
    \centering
     \includegraphics[width=6cm]{plots/Norse/correct_perc/acc_01.png}
    \caption{Απουσία εφαρμογής χωρικής δειγματοληψίας συγκριτικά με την εφαρμογή παράγοντα 0.25 της εικ. 6.3}
    \label{fig:representation-methods}
\end{figure}

Στο σχ. 6.6, στο ίδιο δίκτυο δίνεται το σύνολο δεδομένων με χωρική και χρονική μείωση του δείγματος κατά 0,25 και 100000 αντίστοιχα, αλλά εφαρμόστηκε πιθανότητα πτώσης γεγονότος 0,25, που σημαίνει ότι τυχαία το ένα τέταρτο των συμβάντων θα απορριφθεί. Το αποτέλεσμα της εκπαίδευσης που προκύπτει παρουσιάζει χαμηλότερη ακρίβεια στην τελευταία εποχή και γενικά πιο αργή κλίση μάθησης στις πρώτες εποχές. Παρ 'όλα αυτά, δείχνει μια γενική αύξηση της ακρίβειας για τις τελευταίες εποχές που πραγματοποιήθηκε η εκπαίδευση χωρίς η πιθανότητα πτώσης να χάσει την παροχή πληροφοριών, κάτι που μπορεί να σημαίνει ότι το δίκτυο θα μπορούσε να επιτύχει υψηλότερα σκορ ακρίβειας με περισσότερη εκπαίδευση. Αυτές οι συμπεριφορές θα μπορούσαν να εξηγηθούν με τη λογική ότι η πτώση των γεγονότων οδηγεί σε μια πιο αραιή εσωτερική κατάσταση - αριθμό αιχμών - και οι νευρώνες μπορούν να εξαγάγουν χαρακτηριστικά με μεγαλύτερη ευκολία και να έχουν μια πιο σταθερή καμπύλη μάθησης. Ο παράγοντας πτώσης μπορεί επίσης να εξηγήσει την χαμηλότερη κλίση εκμάθησης στην αρχή, καθώς η πτώση αυτών των συμβάντων χάνει πληροφορίες για το δίκτυο.

\begin{figure}[htp] %this figure will be at the right
    \centering
     \includegraphics[width=6cm]{plots/Norse/correct_perc/acc_08.png}
    \caption{Αποτελέσματα μετά της εφαρμογή πιθανότητας πτώσης γεγονότων στο δίκτυο}
    \label{fig:representation-methods}
\end{figure}

Ένα παράδοξο αποτέλεσμα από τη δοκιμαστική εκτέλεση που έγινε ήταν το γεγονός ότι το μέγεθος κρυμμένου επιπέδου δεν φαίνεται να επηρεάζει καθόλου την απόδοση του δικτύου. Στο σχ. 6.3 και 6.7 βλέπουμε την ίδια αρχικοποίηση για 2 δίκτυα με μόνη διαφορά το μέγεθος του κρυφού επιπέδου. Το δίκτυο της εικ. 6.3 έχει 50 κρυμμένους νευρώνες, ενώ αυτό που φαίνεται στην εικ. 6.7 έχει 100. Ωστόσο, η απόδοση των δικτύων είναι παρόμοια, τόσο στην κλίση εκμάθησης όσο και στην τελική τους ακρίβεια ή ακόμη και σταθερότητα. Αυτό αποδεικνύει τις μεγάλες δυνατότητες των νευρωνικών δικτύων αιχμών αφού μόνο 50 ή και λιγότεροι νευρώνες λειτουργούν με ακρίβεια περίπου 70 τοις εκατό σε ένα τόσο δύσκολο χωροχρονικό έργο.

\begin{figure}[htp] %this figure will be at the right
    \centering
     \includegraphics[width=6cm]{plots/Norse/correct_perc/acc_07.png}
    \caption{Ακρίβεια δικτύου με διπλάσιους νευρώνες στο κρυφό επίπεδο συγκριτικά με το δίκτυο της εικ. 6.3}
    \label{fig:representation-methods}
\end{figure}

Η αστάθεια που παρουσιάζει το δίκτυο λόγω της απουσίας χωροταξικής δειγματοληψίας φαίνεται να μειώνεται ευρέως ή ακόμη και να εξαφανίζεται με μια προσαρμογή του ρυθμού μάθησης. Στο σχ. 6.8 μειώνουμε το ρυθμό εκμάθησης από το αρχικό 0.002 που είχε ως αποτέλεσμα το σχ. 6,4 στο 0,0002 (δέκα φορές λιγότερο). Όπως μπορούμε να δούμε από τα γραφήματα, η ακρίβεια αυξάνεται σταθερά χωρίς ξαφνικά άλματα και χωρίς μεγάλες διαφορές στην ακρίβεια μεταξύ των εποχών (που αντιπροσωπεύει τη σταθερότητα).Φαίνεται ότι ο χαμηλότερος ρυθμός εκμάθησης δίνει τη δυνατότητα στο δίκτυο να ρυθμίσει λεπτομερώς τον μεγάλο αριθμό βαρών που συνδέουν την είσοδο με το κρυφό επίπεδο, χωρίς μεγάλα άλματα στο χώρο αναζήτησης που προκαλούν την αστάθεια που βλέπουμε στο σχήμα. 6.4.

\begin{figure}[htp] %this figure will be at the right
    \centering
     \includegraphics[width=6cm]{plots/Norse/correct_perc/acc_06.png}
    \caption{Το αποτέλεσμα της εκπαίδευσης με χαμηλότερο ρυθμό εκμάθησης συγκριτικά με το δίκτυο της εικ. 6.4}
    \label{fig:representation-methods}
\end{figure}

Οι διαφορές στη μεταβλητή άλφα δεν φαίνεται να επηρεάζουν την απόδοση του δικτύου. Στην εικ. 6.9 το δίκτυο έχει τις ίδιες βελτιστοποιημένες τιμές χωροχρονικών παραμέτρων αλλά διαιρείται με 2 η τιμή άλφα. Οι μόνες πληροφορίες που μπορούμε να αντλήσουμε είναι ότι αυτή η μείωση της παραμέτρου αύξησε την αστάθεια και τις διαφορές ακρίβειας μεταξύ διαδοχικών εποχών. Αυτή η συμπεριφορά ήταν αναμενόμενη δεδομένου ότι το αντίστροφο της τιμής άλφα σχετίζεται με την τετραγωνική ρίζα του υπολογισμού της κλίσης με τη μέθοδο της υποκατάστατης κλίσης και έτσι η μείωση της οδηγεί στην αύξηση της τιμής της παραγώγου.

\begin{figure}[htp] %this figure will be at the right
    \centering
     \includegraphics[width=6cm]{plots/Norse/correct_perc/acc_09.png}
    \caption{Δίκτυο με προσαρμοσμένη τιμή άλφα}
    \label{fig:representation-methods}
\end{figure}

Τα αποτελέσματα του πειράματος μας λένε για την επιρροή της απόδοσης του δικτύου μέσω αλλαγών στα χαρακτηριστικά των νευρώνων. Θα πρέπει να σημειωθεί ότι οι ακόλουθες δοκιμές έγιναν με χρονική δειγματοληψία τιμής 10000, η οποία όπως είδαμε αποδίδει χειρότερα λόγω της αραιότητας στον χρόνο των δεδομένων εισόδου. Ωστόσο, οι κατάλληλες μεταβλητές μπορούν να δώσουν αποδεκτές επιδόσεις. Για παράδειγμα, τα δίκτυα στην εικ. 6.10 και εικ. 6.11 διαφέρουν στις αντίστροφες σταθερές συνάψεως και αντίστροφες σταθερές μεμβράνης με τα ζεύγη 100000, 100000 και 1000, 1000 αντίστοιχα. Η σταθερά συνάψεως ελέγχει την ανθεκτική περίοδο του νευρώνα, δηλαδή τον χρόνο που χρειάζεται για να επιστρέψει στην κατάσταση ηρεμίας μετά την εκπομπή μιας αιχμής, ενώ η σταθερά της μεμβράνης ελέγχει την πτώση τάσης του νευρώνα καθώς φορτίζεται με αιχμές εισόδου πριν φτάσει στον κατώφλι τάσης. Με τη χαμηλότερη χρονική δειγματοληψία, μπορούμε να δικαιολογήσουμε την καλύτερη απόδοση του δικτύου της εικ. 6.11 συγκριτικά με αυτό της εικ. 6.10 δεδομένου ότι οι χαμηλότερες αντίστροφες σταθερές τιμές κάνουν τους νευρώνες να λειτουργούν περισσότερο σαν νευρώνες \textlatin{Integrate and Fire (IF)}, διατηρώντας τη ληφθείσα αιχμή περισσότερο με την πάροδο του χρόνου και δίνοντας τη δυνατότητα να διατηρούνται και να εξάγονται οι χρονικές πληροφορίες. Οι μεγαλύτερες τιμές αυτών των παραμέτρων έχουν ως αποτέλεσμα οι νευρώνες να αποφορτίζονται γρηγορότερα από την άφιξη μιας νέας αιχμής (σε συνδυασμό με τη χαμηλότερη χρονική δειγματοληψία) με αποτέλεσμα να χάνονται οι χρονικές πληροφορίες και μερικοί νευρώνες να φτάνουν στην κατάσταση ηρεμίας πριν φτάσει η επόμενη αιχμή. Ενώ σε αυτό το πείραμα, οι εκτελέσεις δεν φαίνεται να δείχνουν βελτίωση στο αποτέλεσμα της απόδοσης του δικτύου, η εκ νέου ρύθμιση αυτών των παραμέτρων μπορεί να οδηγήσει σε υψηλότερη ακρίβεια.

\begin{figure}
\centering
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Norse/correct_perc/acc_03.png}
  \captionof{figure}{Αποτελέσματα αύξησης αντίστροφων σταθερών μεμβράνης}
  \label{fig:test1}
\end{minipage}
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Norse/correct_perc/acc_04.png}
  \captionof{figure}{Αποτελέσματα μείωσης αντίστροφων σταθερών μεμβράνης}
  \label{fig:test2}
\end{minipage}
\end{figure}

Με τον ίδιο χρονικό συντελεστή μείωσης, δοκιμάσαμε την επίδραση του κατωφλίου τάσης στο δίκτυο. Το δίκτυο φαίνεται να δείχνει χειρότερη συμπεριφορά για αυξημένο κατώφλι τάσης. Το δίκτυο που φαίνεται στην εικ. 6.12 έχει το κατώφλι στο 1.5 σε σύγκριση με το 1.0 που ήταν το προεπιλεγμένο για τα προηγούμενα δίκτυα. Το δίκτυο παρουσιάζει μια ασταθή συμπεριφορά που εμφανίζεται τόσο στο γράφημα ακρίβειας, όσο κυρίως στο γράφημα απώλειας δοκιμής 6.12. Αυτή η συμπεριφορά μπορεί να εξηγηθεί από το γεγονός ότι ένας νευρώνας με υψηλότερο κατώφλι πυροδότησης χρειάζεται περισσότερες αιχμές ως εισόδους και οι αλλαγές βάρους δεν κατάφεραν να προσαρμοστούν σε αυτήν την αλλαγή. Η χαμηλή χρονική δειγματοληψία μπορεί να απέτρεψε αυτή η ανάγκη να υλοποιηθεί και έτσι το δίκτυο λειτουργεί κατ' αυτό τον τρόπο.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{plots/Norse/correct_perc/acc_05.png}
  \caption{Ακρίβεια}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{plots/Norse/test_loss/test_loss_05.png}
  \caption{Απώλεια δοκιμής}
  \label{fig:sub2}
\end{subfigure}
\caption{Αποτελέσματα αύξησης κατωφλίου πυροδότησης σε σύγκριση με το δίκτυο της εικ. 6.3}
\label{fig:test}
\end{figure}

Συμπερασματικά, η προεπεξεργασία των δεδομένων φαίνεται να παίζει καθοριστικό ρόλο στην απόδοση του δικτύου με το πιο σημαντικό να είναι η χρονική δειγματοληψία. Για το συγκεκριμένο σύνολο δεδομένων, η τιμή 100000 φαίνεται να έχει την καλύτερη απόδοση. Η χωρική μείωση με 0.25 δείχνει καλύτερα αποτελέσματα συγκριτικά με μεγαλύτερες τιμές, παρόλο που ένας σωστός συνδυασμός χωρικής δειγματοληψίας και ρυθμού μάθησης φαίνεται να αντιμετωπίζει τα προβλήματα σταθερότητας και να αποδίδει καλά. Αυτό το πείραμα δείχνει ότι η απόρριψη τυχαίων συμβάντων δεν επηρεάζει ιδιαίτερα την τελική (μέγιστη) ακρίβεια, αλλά μπορεί και να την αυξήσει, όμως αυξάνει τον αριθμό των απαραίτητων εποχών για να αγγίξει το δίκτυο τη μέγιστη ακρίβεια. Τέλος, μέσω αυτού του πειράματος, αποδεικνύεται ότι οι χρονικές σταθερές του νευρώνα πρέπει να βαθμονομηθούν στη χρονική διαμόρφωση του συνόλου δεδομένων εισόδου. Εάν τα γεγονότα είναι πολύ μακριά μεταξύ τους στο χρόνο, είναι επιθυμητή για τον νευρώνα μια συμπεριφορά που προσομοιάζει νευρώνες \textlatin{Integrate and Fire}. Διαφορετικά, απαιτείται νευρώνας ταχύτερης πτώσης τάσης και ανθεκτικής περιόδου. Το κατώφλι τάσης θα πρέπει επίσης να προσαρμοστεί στη χρονική πυκνότητα των συμβάντων στο σύνολο δεδομένων, καθώς μπορεί να επηρεάσει την ευκολία παραγωγής της αιχμής στο κρυφό επίπεδο που θα πρέπει είτε να αυξηθεί σε σύνολα δεδομένων αραιών γεγονότων είτε να μειωθεί σε πυκνά.

\section{Εκπαίδευση με τον αλγόριθμο \textlatin{Decolle}}

Ο αλγόριθμος \textlatin{DECOLLE} \cite{kaiser2020} επιτρέπει διαδικτυακή εκπαίδευση εστιάζοντας σε τοπικές απώλειες που μπορούν να υπολογιστούν χωρίς την ανάγκη του δικτύου να υπολογίσει το σφάλμα της εξόδου του και να το διαδώσει πίσω στα υπόλοιπα επίπεδα. Από την άποψη της μηχανικής, είναι ενδιαφέρουσα η παρατήρηση των αποτελεσμάτων ενός τέτοιου σχήματος καθώς η διαδικτυακή του ικανότητα μάθησης υπόσχεται υψηλές ταχύτητες. Η μέτριση της ακρίβειας σε τόσο δύσκολες εργασίες θα αναδείξει τις δυνατότητες αυτού του προγράμματος μάθησης.

\subsection{Αρχικοποίηση}

Δεδομένου ότι το \textlatin{Decolle} έχει ήδη δοκιμαστεί στο ίδιο σύνολο δεδομένων από τους συγγραφείς του αλγορίθμου, επικεντρωθήκαμε στη δοκιμή διαφορετικών παραλλαγών του δικτύου, ώστε να επωφεληθούμε πλήρως από τις δυνατότητες της μεθόδου εκμάθησης και να βρούμε τις κατάλληλες παραμέτρους δικτύου, ανακαλύπτοντας κατά την διάρκεια μοτίβα σχετικά με την απόδοση του δικτύου σε συγκεκριμένες υπερπαραμέτρους. Οι συγγραφείς είχαν ήδη εφαρμόσει την προεπεξεργασία των δεδομένων και τα τροποποίησαν με τη μορφή που είναι κατάλληλη για μέγιστη απόδοση μέσω του συγκεκριμένου σχήματος εκπαίδευσης. Σε αυτήν την περίπτωση, δεν χρησιμοποιήσαμε τη βιβλιοθήκη \textlatin{Tonic} για τη φόρτωση των δεδομένων, καθώς ο απαιτούμενος κώδικας υπήρχε ήδη. Επιπλέον, οι μετασχηματισμοί που ισχύουν για τις άλλες μεθόδους εκπαίδευσης, για να επιταχυνθεί ο χρόνος της συνολικής διαδικασίας, δεν χρειάζονταν σε αυτή την περίπτωση, καθώς αυτό το σύνολο δεδομένων, προεπεξεργασμένο ειδικά για αυτό το έργο, επιτρέπει η εκπαίδευση να είναι αρκετά γρήγορη. Επιπλέον, θα πρέπει να παρατηρήσουμε ότι τα δεδομένα, παρόλο που δεν μετασχηματίστηκαν από εμάς, είχαν ήδη χωρική υποδειγματοληψία από την εφαρμοζόμενη διαδικασία προεπεξεργασίας. Η χωρική διάσταση διαιρέθηκε δια του 4 σε κάθε άξονα με αποτέλεσμα μια εικόνα εισόδου 32\textlatin{x}32.

\subsection{Παραλλαγές Δικτύου}

Σε αυτό το πείραμα, εστιάσαμε στην εύρεση των ιδανικών υπερπαραμέτρων του δικτύου. Ο βελτιστοποιητής που χρησιμοποιείται είναι ο βελτιστοποιητής \textlatin{Adamax} ο οποίος έχει μελετηθεί καλά και έτσι δεν θα επικεντρωθούμε στη βελτιστοποίηση του \textlatin{Adamax}. Ως αποτέλεσμα, εκπαιδεύσαμε 14 διαφορετικά δίκτυα για 200 εποχές στις ακόλουθες παραμέτρους:

\begin{itemize}
    \item Αριθμός και τύπος στρώματος: Χρησιμοποιούμε συνδυασμούς επιπέδων αναγνωριστών (\textlatin{perceptron layers}) και συνελικτικών επιπέδων (\textlatin{convolutional layers}) για να επιτύχουμε βαθιά μάθηση
    \item Μέγεθος πυρήνα για τα συνελικτικά επίπεδα
    \item Οι σταθερές των νευρώνων όπως η ανθεκτική περίοδος του νευρώνα
    \item Μέγεθος κομματιού εκπαίδευσης: Το μέγεθος των δεδομένων εκπαίδευσης όντας πολύ μεγάλο από άποψη τόσο της μνήμης \textlatin{GPU} όσο και του χρόνου εκπαίδευσης, ορίστηκε χειροκίνητα σε μια σταθερή τιμή και κάθε αντικείμενο δεδομένων χωρίστηκε σε κομμάτια του καθορισμένου μεγέθους.
    \item Συντελεστής πτώσης ρυθμού μάθησης: καθορίζει τη μείωση του ποσοστού μάθησης καθώς συνεχίζεται η εκπαίδευση
    \item Όροι κανονικοποίησης: Αυτή η παράμετρος ελέγχει τη συχνότητα αιχμής των νευρώνων, διατηρώντας την (ανάλογα με την τιμή της) σε λογικές τιμές - χωρίς να αφήνει τους νευρώνες να σιωπούν ή να πυροδοτούνται συνεχώς. Οι τιμές κανονικοποίησης ενδέχεται να διαφέρουν σε κάθε επίπεδο του δικτύου.
\end{itemize}

Η επιλογή των παραπάνω υπερπαραμέτρων έγινε αφού πιστεύαμε ότι παίζουν καθοριστικό ρόλο στην απόδοση του δικτύου. Οι δημιουργοί του κανόνα μάθησης παρέχουν αποτελέσματα μόνο από ένα δίκτυο - χωρίς να αναφέρονται σε δοκιμές που γίνονται σε άλλες αρχιτεκτονικές δικτύου. Το δίκτυό τους αποτελείται από τρία συνελικτικά επίπεδα με μέγεθος πυρήνα 5. Θέλοντας να διερευνήσουμε τις δυνατότητες, επικεντρωθήκαμε σε βαθιά νευρωνικά δίκτυα με συνδυασμούς αναγνωστών και συνελικτικών επιπέδων. Αποφασίσαμε να αναπτύξουμε 4 διαφορετικές αρχιτεκτονικές δικτύων που μπορεί να παρουσιάσουν ενδιαφέροντα αποτελέσματα. Στον παρακάτω πίνακα 6.1, παρουσιάζουμε την αρχιτεκτονική αυτών των δικτύων. Θα πρέπει να παρατηρήσουμε ότι τα συνελικτικά στρώματα ήταν πάντα πρώτα στο δίκτυο, πριν από τα επίπεδα αναγνωστών στο πείραμά μας.

\begin{table}[htbp]
\centering
\begin{tabular}{||c||c|c|c||}
\hline
{\textit{\textbf{\textlatin{Network ID}}}} & {\textit{\textbf{\textlatin{Conv. layers}}}} & {\textit{\textbf{\textlatin{MLP layers}}}} \tabularnewline \hline
1 & 64-128-128 & -\tabularnewline \hline
2 & 128-512-256 & 128\tabularnewline \hline
3 & 128-512-256 & 128-128\tabularnewline \hline
4 & - & 512-256-128-128\tabularnewline \hline
\end{tabular}
\caption{Αναπτυγμένη αρχιτεκτονική δικτύων. Η σειρά που ακολουθούν τα επίπεδα ξεκινάει από αριστερά προς τα δεξιά.}
\label{table:economicSchools2}   
\end{table}

Με τις προτεινόμενες υπερπαραμέτρους να τροποποιούνται για τον έλεγχο της απόδοσης του δικτύου, ο συνολικός αριθμός δικτύων που θα πρέπει να δοκιμαστούν είναι εκθετικός. Για να μειώσουμε τον υπολογιστικό χρόνο αυτής της εργασίας και να συγκεντρώσουμε ακόμα τις πληροφορίες που θα μας οδηγήσουν σε χρήσιμα συμπεράσματα, χρησιμοποιούμε ζεύγη δικτύων που διαφέρουν σε ένα στοιχείο - παράμετρο. Με αυτόν τον τρόπο, μπορούμε να αξιολογήσουμε την επίδραση που θα έχει αυτή η αλλαγή στη συγκεκριμένη εγκατάσταση και πιθανώς γενικά σε ολόκληρο το σύνολο των δικτύων. Αν και συνειδητοποιούμε ότι αυτή η μέθοδος δεν θα μας δώσει σίγουρα αποτελέσματα, καταφέρνουμε να μειώσουμε το σύνολο των δικτύων που θα δοκιμαστούν σε μόλις 14. Επιπλέον, οι πληροφορίες που αποκτώνται από τις διαφορές στα ζεύγη δικτύων θα δείξουν προς τη σωστή κατεύθυνση για την ολική ιδανική επιλογή υπερπαραμέτρων στον χώρο αναζήτησης.

Ως αφετηρία, χρησιμοποιήσαμε τις τιμές των παραμέτρων που πρότειναν οι συντάκτες του \textlatin{DECOLLE}. Με βάση αυτές τις τιμές - μέγεθος πυρήνα = 7, \textlatin{alpha} = 0.97, \textlatin{alpharp} = 0.65, μέγεθος κομματιού = 500, παράγοντας πτώσης = 5, κανονικοποίηση = 0 - και τις αρχιτεκτονικές των δικτύων που αναπτύχθηκαν, δημιουργούμε τα ακόλουθα ζεύγη με μικρές διαφορές για να λάβουμε πληροφορίες σχετικά με τις αλλαγές στις στοχευμένες τιμές. Δοκιμάσαμε το πρώτο δίκτυο με μέγεθος κομματιού 500 και 1500 για να δούμε τη διαφορά στην απόδοση που προκαλεί το χρονικό κομμάτι εισόδου, αφού οι συγγραφείς είχαν ήδη εργαστεί με την ίδια αρχιτεκτονική, πιθανώς πειράζοντας τις υπερπαραμέτρους του. Το δεύτερο δίκτυο επικεντρώθηκε στην επίδραση της κανονικοποίησης. Μια παραλλαγή εκτελέστηκε με τις προτεινόμενες υπερπαραμέτρους, ενώ σε μια άλλη ορίσαμε την κανονικοποίηση ως 0,1 για κάθε ένα από τα επίπεδα. Σε άλλες δύο εκδόσεις, μειώσαμε την τιμή του \textlatin{deltat}, της μεταβλητής που καθορίζει τη διαφορά χρόνου σε δύο διαδοχικά χρονικά βήματα, από 1000 σε 500. Η μία από αυτές τις δύο εκδόσεις δοκιμάστηκε με κανονικοποίηση 0 και η άλλη είχε φθίνουσα μορφή [0,2, 0,15, 0,1, 0,05, 0] για κάθε στρώμα, με την ιδέα να ελέγχονται οι αιχμές εισόδου έτσι ώστε να αποτρέπεται η συνεχής πυροδότηση και σαν αποτέλεσμα η παραμόρφωση ή και απώλεια πληροφορίας. Τέσσερις παραλλαγές του τρίτου δικτύου (\textlatin{ID} = 3) εφαρμόστηκαν επίσης. Το πρώτο ήταν η αρχιτεκτονική που παρουσιάστηκε με τις προτεινόμενες παραμέτρους. Το δεύτερο είχε διαφορετικές υπερπαραμέτρους νευρώνων. Η πτώση δυναμικού της μεμβράνης των νευρώνων ορίστηκε σε 0,99 αντί 0,97 και η ανθεκτική περίοδος μεταβλήθηκε από 0,65 σε 0,55 για να διατηρηθούν οι νευρώνες πιο ενεργοί ώστε να αντιπαραβάλλουν την αραιότητα των δεδομένων. Οι άλλες δύο εκδόσεις δοκιμάστηκαν με πυρήνα μεγέθους 3 για τα συνελικτικά στρώματα και ο συντελεστής πτώσης ορίστηκε 5 και 10 αντίστοιχα σε κάθε δίκτυο. Αυτή η μεταβλητή διαιρεί το ρυθμό εκμάθησης μετά από έναν καθορισμένο από τον χρήστη αριθμό εποχών, εδώ 30. Η τέταρτη αρχιτεκτονική δοκιμάστηκε με τις παραμέτρους που ορίστηκαν από τους συντάκτες του κανόνα και συγκρίθηκε με ένα αντίγραφο που χρησιμοποιεί το βελτιστοποιητή \textlatin{Adam}. Επιπλέον, εφαρμόστηκαν 2 ακόμη εκδόσεις της ίδιας αρχιτεκτονικής, όπου και οι δύο έχουν μια αυξανόμενη κανονικοποίηση για τα στρώματα με την ιδέα να μην αφήσουν τα τελευταία στρώματα να σωπάσουν και να χάσουν πληροφορίες κατά την εκπαίδευση, με τη διαφορά ότι μία από τις εκδόσεις είχε 0 κανονικοποίηση στο τελευταίο επίπεδο για να μην επηρεαστεί το τελικό επίπεδο εξόδου με αιχμές που δεν παράγονται από το δίκτυο. Οι τιμές κανονικοποίησης για τα επίπεδα κάθε δικτύου είναι τα ακόλουθα: [0, 0.04, 0.08, 0.12, 0.16] και [0, 0.05, 0.1, 0.15, 0].

\subsection{Αποτελέσματα}

Αναλύοντας τα αποτελέσματα των δοκιμών, παρατηρούμε ότι τα δίκτυα που εκπαιδεύονται με το \textlatin{Decolle} έχουν υψηλή ακρίβεια από τις πρώτες εποχές. Σε σύγκριση με τις υπόλοιπες μεθόδους μάθησης που δοκιμάστηκαν σε αυτήν τη διατριβή, αυτή η διαφορά απόδοσης μπορεί να οφείλεται στο γεγονός ότι ο κώδικας του προγράμματος μάθησης \textlatin{Decolle} κατασκευάστηκε με τέτοιο τρόπο ώστε να χειρίζεται μόνο τέτοια σύνολα δεδομένων. Οι συγγραφείς, όπως αναφέρθηκε προηγουμένως, χειρίζονται το σύνολο δεδομένων με τέτοιο τρόπο ώστε αυτό να τροφοδοτείται στο αναπτυγμένο σχήμα υλοποίησης του δικτύου τους με τρόπο τέτοιο, χάνοντας την καθολικότητα, αλλά στοχεύοντας σε καλύτερη απόδοση όπως αυτή που βλέπουμε από αυτό το πείραμα.

Γενικά, από τις τέσσερις οικογένειες των ανεπτυγμένων δικτύων (οι τέσσερις διαφορετικές αρχιτεκτονικές), παρατηρούμε διαφορετικές συμπεριφορές. Δίκτυα που αποτελούνται από τρία συνελικτικά στρώματα εικ. 6.13, εικ. 6.14, τα πρώτα στρώματα φαίνεται να δυσκολεύονται να μάθουν τους τοπικούς κανόνες μάθησης. Το πρώτο στρώμα τείνει να παραμείνει σε ακρίβεια περίπου 25 τοις εκατό σε όλη τη διαδικασία εκμάθησης, ενώ τα υπόλοιπα στρώματα αυξάνουν αναλογικά την ακρίβειά τους. Στις τελευταίες εποχές, φαίνεται ότι το επίπεδο εξόδου αποδίδει με την ίδια ακρίβεια με το τελευταίο συνελικτικό στρώμα. Ωστόσο, σε αντίθεση με τις υπόλοιπες οικογένειες, το επίπεδο εξόδου για τις πρώτες εποχές βαθμολογείται χαμηλότερα από το τρίτο επίπεδο και φαίνεται ότι χρειάζεται μερικές εποχές για να φτάσει σε παρόμοιο επίπεδο. Μια πιθανή εξήγηση θα μπορούσε να είναι ότι τα συνελικτικά επίπεδα επικεντρώνονται στην εξαγωγή χαρακτηριστικών παρά στον τοπικό κανόνα εκμάθησης (και για αυτόν τον λόγο, βαθμολογούνται χαμηλότερα) και το επίπεδο εξόδου, καθώς αποτελεί το μόνο στρώμα αναγνωστών χρειάζεται λίγο χρόνο για να μάθει τα χαρακτηριστικά από το τρίτο συνελικτικό επίπεδο, και έτσι εκπαιδεύεται πιο αργά.

\begin{figure}
\centering
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Decolle/correct_perc/acc_01.png}
  \captionof{figure}{Δίκτυο που αποτελείται από τρία συνελικτικά επίπεδα}
  \label{fig:test1}
\end{minipage}
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Decolle/correct_perc/acc_08.png}
  \captionof{figure}{Δίκτυο που αποτελείται από τρία συνελικτικά επίπεδα, με αυξημένο μέγεθος κομματιού εισόδου}
  \label{fig:test2}
\end{minipage}
\end{figure}

Δίκτυα με 3 συνελικτικά επίπεδα και 1 επίπεδο αναγνωστών παρουσιάζουν την καλύτερη συμπεριφορά (εικ. 6.15, εικ. 6.16.) Με εξαίρεση το πρώτο επίπεδο, τα υπόλοιπα φαίνεται να φτάνουν σε ακρίβειες περίπου στο 90 τοις εκατό. Ωστόσο, το πρώτο στρώμα σε σύγκριση με την πρώτη οικογένεια φαίνεται να μαθαίνει από τους τοπικούς κανόνες φθάνοντας σε ακρίβεια πάνω από 50 τοις εκατό. Το στρώμα 2 φαίνεται ότι χρειάζεται κάποιες εποχές για να φτάσει τα επόμενα, αλλά τελικά φτάνει στους δείκτες ακρίβειας των τελευταίων επιπέδων. Μια άλλη σημαντική παρατήρηση είναι η σταθερότητα που παρουσιάζει το δίκτυο μέσα στις εποχές, δηλαδή οι διαφορές στην ακρίβεια μεταξύ των διαδοχικών εποχών παραμένουν χαμηλές.
\begin{figure}
\centering
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Decolle/correct_perc/acc_02.png}
  \captionof{figure}{Δίκτυο που αποτελείται από τρία συνελικτικά επίπεδα και ένα επίπεδο αναγνωστών με παράμετρο δέλτα ρυθμισμένη στο 1000}
  \label{fig:test1}
\end{minipage}
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Decolle/correct_perc/acc_06.png}
  \captionof{figure}{Δίκτυο που αποτελείται από τρία συνελικτικά επίπεδα και ένα επίπεδο αναγνωστών με παράμετρο δέλτα ρυθμισμένο στο 500}
  \label{fig:test2}
\end{minipage}
\end{figure}

Όταν προσθέσαμε ένα άλλο επίπεδο αναγνωστών στο δίκτυο (τρία συνελικτικά επίπεδα και 2 επίπεδα αναγνωστών), το δίκτυο δεν βελτιώθηκε (εικ. 6.17, εικ.6.18). Παρατηρούμε αυξημένη αστάθεια σε όλα τα επίπεδα σε σύγκριση με την τελευταία ρύθμιση. Επιπλέον, η προσθήκη του στρώματος αναγωστών φαίνεται να μειώνει την απόδοση του δεύτερου στρώματος. Γενικά, μπορούμε να συμπεράνουμε ότι μόνο τα τρία τελευταία στρώματα του δικτύου (έξοδος και τα 2 τελευταία κρυφά επίπεδα) μπορεί να έχουν μεγάλη ακρίβεια. Δεδομένου ότι η ακρίβεια της εξόδου δεν αυξήθηκε και σε ορισμένες περιπτώσεις μειώθηκε, η προσθήκη πάρα πολλών επιπέδων μπορεί να προκαλέσει προβλήματα αστάθειας και απόδοσης.

\begin{figure}
\centering
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Decolle/correct_perc/acc_03.png}
  \captionof{figure}{Δίκτυο που αποτελείται από τρία συνελικτικά επίπεδα και δύο επίπεδα αναγνωστών με παράμετρο δέλτα ρυθμισμένο στο 1000}
  \label{fig:test1}
\end{minipage}
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Decolle/correct_perc/acc_07.png}
  \captionof{figure}{Δίκτυο που αποτελείται από τρία συνελικτικά επίπεδα και δύο επίπεδα αναγνωστών με παράμετρο δέλτα ρυθμισμένο στο 200}
  \label{fig:test2}
\end{minipage}
\end{figure}

Η τέταρτη αρχιτεκτονική που περιέχει μόνο 4 στρώματα αναγνωστών παρουσίασε διαφορετική συμπεριφορά σε σύγκριση με τα υπόλοιπα (εικ. 6.19, εικ. 6.20). Αυτά τα δίκτυα σημείωσαν χαμηλότερη βαθμολογία από τα δίκτυα τόσο με μόνο συνελικτικά επίπεδα όσο και με επίπεδα αναγνωστών, φθάνοντας σε ακρίβεια μικρότερη από 90 τοις εκατό. Επιπλέον, παρουσιάζουν μια πιο ασταθή συμπεριφορά με σημαντικές διακυμάνσεις στην ακρίβεια μεταξύ των εποχών. Ωστόσο, μια αξιοσημείωτη παρατήρηση είναι η ακρίβεια του πρώτου στρώματος. Σε αντίθεση με τις άλλες οικογένειες δικτύων, όπου το πρώτο επίπεδο δεν μπορούσε να υπερβεί το 60 τοις εκατό σε ακρίβεια, εδώ παρατηρούμε την ακρίβεια του επιπέδου ένα να ακολουθήσει στενά τα υπόλοιπα επίπεδα, φθάνοντας τιμές πάνω από 80 τοις εκατό (εικ. 6.19). Τέλος, μπορούμε να παρατηρήσουμε ότι όλα τα επίπεδα έχουν υψηλή βαθμολογία ακόμη και από την πρώτη εποχή (τις περισσότερες φορές πάνω από 50 τοις εκατό για κάθε στρώμα). Η αυξημένη ακρίβεια του πρώτου στρώματος αναγνωστών μπορεί να οφείλεται στην εστίαση στον κανόνα τοπικής μάθησης σε σύγκριση με την εξαγωγή χαρακτηριστικών που εκτελούν τα συνελικτικά στρώματα.

\begin{figure}
\centering
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Decolle/correct_perc/acc_04.png}
  \captionof{figure}{Δίκτυο που αποτελείται από 4 επίπεδα αναγνωστών}
  \label{fig:test1}
\end{minipage}
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Decolle/correct_perc/acc_11.png}
  \captionof{figure}{Δίκτυο που αποτελείται από 4 επίπεδα αναγνωστών με εφαρμογή κανονικοποίησης πυροδότησης}
  \label{fig:test2}
\end{minipage}
\end{figure}

Η δοκιμή αύξησης του χρονικού μήκους της ακολουθίας εισόδου στο δίκτυο (με την ιδέα ότι το δίκτυο λαμβάνει ένα μεγαλύτερο κομμάτι και μπορεί να μάθει καλύτερα με κάθε εκτέλεση της μάθησης) έδειξε ότι αυτός ο χειρισμός προκαλεί πτώση της απόδοσης (εικ. 6.13, εικ. 6.14). Αυτό υποδηλώνει την αδυναμία του συστήματος εκμάθησης να αντιμετωπίσει μεγάλες χρονικές ακολουθίες όπως πρότειναν οι ίδιοι οι συγγραφείς. Αυτή η αδυναμία μπορεί να είναι το αποτέλεσμα της έλλειψης ίχνους νευρώνων για την ενημέρωση των νεότερων νευρώνων για γεγονότα στο πιο μακρινό παρελθόν.

Ο χειρισμός του ρυθμού μάθησης και του ποσοστού πτώσης του φαίνεται να έχει μεγάλο αντίκτυπο στην απόδοση του δικτύου. Συγκρίνοντας δύο πανομοιότυπα δίκτυα (εικ. 6.19, εικ. 6.21) με διαφορετικούς ρυθμούς εκμάθησης - 10 \textuperscript{-9} για το αριστερό και 10\textuperscript {-8} για το δεξί - παρατηρούμε ότι η αύξηση του ποσοστού μάθησης οδήγησε το δίκτυο στο να γίνει ασταθές και να φτάσει συνολικά σε χαμηλότερη ακρίβεια. Αυτό μπορεί να εξηγηθεί λόγω των μεγάλων αλμάτων στον χώρο αναζήτησης με αποτέλεσμα την αδυναμία του δικτύου να φτάσει στο τοπικό ή μέγιστο ελάχιστο. Η ίδια συμπεριφορά παρατηρείται όταν συγκρίνονται 2 δίκτυα με διαφορετικούς παράγοντες πτώσης του ρυθμού μάθησης (εικ. 6.18, εικ. 6.22). Ο συντελεστής πτώσης του ρυθμού μάθησης \textlatin{(LRDF)} διαιρεί το ρυθμό μάθησης με την τιμή της παραμέτρου κάθε 30 εποχές (το διάστημα των 30 εποχών διατηρήθηκε το ίδιο για όλα τα δίκτυα αφού φαίνεται λογικό αυτό το διάστημα διαίρεσης για τον αριθμό των επιλεγμένων εποχών). Το μειωμένο \textlatin{(LRDF)} προκάλεσε το δίκτυο εικ. 6.22 για να πετύχει χαμηλότερη ακρίβεια. Η αιτία αυτού θα μπορούσε να είναι ότι σε μεταγενέστερες εποχές που το δίκτυο θα πρέπει να εκτελέσει μικρορυθμίσεις (μετά την εκμάθηση των γενικών χαρακτηριστικών), ο ρυθμός εκμάθησης είναι ακόμα υψηλός και το δίκτυο κάνει μεγάλα άλματα στο χώρο αναζήτησης, με αποτέλεσμα την αδυναμία να φτάσει τοπικό ελάχιστο (αδυναμία εφαρμογής μικρορυθμίσεων - \textlatin{fine tuning}).

\begin{figure}
\centering
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Decolle/correct_perc/acc_05.png}
  \captionof{figure}{Αυξημένος ρυθμός μάθησης σε σύκγριση με την εικ. 6.19}
  \label{fig:test1}
\end{minipage}
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Decolle/correct_perc/acc_14.png}
  \captionof{figure}{Πτώση στην τιμή του \textlatin{(LRDF)} του δικτύου της εικ. 6.18}
  \label{fig:test2}
\end{minipage}
\end{figure}

Η προσθήκη κανονικοποίησης στα δίκτυα φαίνεται να σταθεροποιεί γενικά τη μαθησιακή διαδικασία (εικ. 6.19, εικ. 6.20). Η εφαρμογή της αυξανόμενης κανονικοποίησης σε βαθύτερα στρώματα δεν έδειξε να παράγει διαφορετικά αποτελέσματα όσον αφορά την ακρίβεια σε σύγκριση με τη φθίνουσα ή σταθερή (ίδια για όλα τα δίκτυα) κανονικοποίηση. Ωστόσο, οι τιμές κανονικοποίησης δεν ξεπέρασαν ποτέ το 0,2 για το πείραμά μας και υψηλότερες τιμές μπορεί να επηρεάσουν το δίκτυο με απρόβλεπτους τρόπους.

Το πείραμα αποκάλυψε επίσης ότι η επιλογή της χρονικής σταθεράς της μεμβράνης και η ανθεκτική περίοδος του νευρώνα επηρεάζει σημαντικά την απόδοση του δικτύου και μικρές αλλαγές έχουν ως αποτέλεσμα σημαντικές διαφορές. Η χρονική σταθερά της μεμβράνης ελέγχει τον ρυθμό πτώσης του νευρώνα (πόσο γρήγορα απόφορτίζεται μεταξύ των αιχμών εισόδου στον χρόνο) και η ανθεκτική περίοδος, τον χρόνο που χρειάζεται ο νευρώνας για να είναι έτοιμος να χειριστεί τις εισόδους μετά την πυροδότηση αιχμή του. Οι εικ. 6.23 και εικ. 6.24 δείχνουν το ίδιο δίκτυο όπου αυτό στην εικ. 6.23 έχει το ζεύγος παραμέτρων 0,97 και 0,65 ενώ το αυτό της εικ. 6.24 έχει τις παραμέτρους ορισμένες σε 0.99 και 0.55, καθιστώντας τον νευρώνα «πιο αιχμηρό», συντομεύοντας το χρονικό διάστημα και των δύο προαναφερθέντων ενεργειών. Ως αποτέλεσμα αυτής της αλλαγής, το δίκτυο απέδωσε χειρότερα και έγινε πιο ασταθές. Επιπλέον, το επίπεδο εξόδου δεν έλαβε τόσο καλές βαθμολογίες όσο τα μεσαία στρώματα. Αυτό μπορεί να είναι το αποτέλεσμα των νευρώνων που ξεχνούν ταχύτερα γεγονότα από παλαιότερες εποχές (λόγω της αλλαγής στη σταθερά της μεμβράνης) προκαλώντας πτώση του ρυθμού πυροδότησης και το δίκτυο δυσκολεύεται να εξαγάγει πληροφορίες και να βρει μια σταθερή διαμόρφωση για τους πιο ασταθείς νευρώνες (για τη δεδομένη ρύθμιση).

\begin{figure}
\centering
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Decolle/correct_perc/acc_03.png}
  \captionof{figure}{Απόδοση δικτύου με σταθερή συμπεριφορά νευρώνων}
  \label{fig:test1}
\end{minipage}
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/Decolle/correct_perc/acc_10.png}
  \captionof{figure}{Αποτέλεσμα της αύξησης της αιχμηρότητας των νευρώνων}
  \label{fig:test2}
\end{minipage}
\end{figure}

Η ανάλυση των αποτελεσμάτων των δικτύων με διαφορετικές παραμέτρους δέλτα αποκάλυψε ότι η τιμή του επηρεάζει την απόδοση του δικτύου. Σύγκριση των δικτύων των εικ. 6.15, εικ. 6.16 και 6.17, 6.18 αποκαλύπτει ένα μοτίβο. Η μειωμένη τιμή του δέλτα προκαλεί αστάθεια στο δίκτυο και φαίνεται να μειώνει ελαφρώς την ακρίβειά του. Η παράμετρος δέλτα ελέγχει τα χρονικά διαστήματα στα οποία οι νευρώνες λαμβάνουν αιχμές και κατά μία έννοια ελέγχει τη συχνότητα στην οποία λειτουργεί το δίκτυο. Ο καθορισμός χαμηλής τιμής σημαίνει ότι ρίχνουμε τη συχνότητα και ως αποτέλεσμα, χάνουμε πληροφορίες. Ιδανικά, η παράμετρος πρέπει να ρυθμιστεί βάση της συχνότητας εισόδου, μια διαδικασία που γίνεται αυτόματα σε μια νευρομορφική πλατφόρμα.

Τέλος, οι δοκιμασμένες τιμές μεγέθους πυρήνα που εφαρμόστηκαν (3, 5 και 7) δεν φαίνεται να έχουν κάποια αξιοσημείωτη επίδραση στην απόδοση του δικτύου.

Συνολικά, με βάση τα αποτελέσματα του πειράματος, καταλήγουμε στο συμπέρασμα ότι τα καλύτερα αποτελέσματα για τη συγκεκριμένη εργασία παράγονται από ένα δίκτυο αποτελούμενο από τρία συνελικτικά επίπεδα και ένα επίπεδο αναγνωστών (\textlatin{perceptron}). Η χρήση μόνο συνελικτικών επιπέδων ή επιπέδων αναγνωστών επιτυγχάνει χειρότερες βαθμολογίες και το ίδιο κάνει και η προσθήκη επιπλέον στρωμάτων αναγνωστών στην προτεινόμενη αρχιτεκτονική. Μια σταθερή απόδοση απαιτεί ένα αρκετά χαμηλό ποσοστό εκμάθησης και έναν υψηλό συντελεστή πτώσης για να αποφευχθούν μεγάλα άλματα και σκέδαση στο χώρο αναζήτησης. Η προσθήκη κανονικοποίησης στα επίπεδα φαίνεται να βοηθά το δίκτυο να σταθεροποιηθεί, ενώ η εσωτερική δομή του νευρώνα χρειάζεται εκ νέου ρύθμιση για κάθε σύνολο δεδομένων καθώς φαίνεται να παίζει σημαντικό ρόλο στην απόδοση του δικτύου. Μια σημαντική παρατήρηση που έγινε μέσω του πειράματος ήταν η επίδραση που έχει το παράθυρο του χρόνου εισόδου στην απόδοση. Τα αποτελέσματα κλίνουν προς την κατεύθυνση της αδυναμίας του μαθησιακού σχήματος να επεξεργαστεί μεγάλα χρονικά διαστήματα από τη φύση του.

\section{Εκπαίδευση με τον αλγόριθμο \textlatin{E-prop}}

Ο κανόνας εκμάθησης \textlatin{E-prop} προσελκύει μεγάλο ενδιαφέρον από νευροεπιστημονική σκοπιά, καθώς φαίνεται να μιμείται τη συμπεριφορά του εγκεφάλου τόσο από τη δομή του δικτύου (\textlatin{RSNN}), καθώς και από τα δομικά στοιχεία που αποτελούν τον ίδιο τον κανόνα, δηλαδή το ίχνος επιλεξιμότητας και το σήμα μάθησης. Ως αποτέλεσμα, η απόδοση αυτού του σχεδίου μάθησης σε ένα τέτοιο σύνολο δεδομένων παρουσιάζει μεγάλο ενδιαφέρον τόσο από μηχανικής όσο και από νευροεπιστημονικής πλευράς.

\subsection{Αρχικοποίηση}

Η προεπεξεργασία του συνόλου δεδομένων, όπως οι απαιτούμενοι μετασχηματισμοί, έγινε χρησιμοποιώντας τη βιβλιοθήκη \textlatin{Tonic} που αναφέρθηκε προηγουμένως. Οι μετασχηματισμοί που εφαρμόζονται στο σύνολο δεδομένων είναι η υποδειγματοληψία χωρικών συντεταγμένων, από 128\textlatin{x}128 σε 32\textlatin{x}32 έτσι ώστε να συντομευθεί το μέγεθος εισόδου. Επιπλέον, δεδομένου ότι τα γεγονότα συμβαίνουν με σημαντική χρονική διαφορά, πράγμα που σημαίνει ότι ένα μπλοκ 4\textlatin{x}4 εικονοστοιχείων είναι απίθανο να πυροδοτείται ταυτόχρονα, αυτό η υποδειγματοληψία μπορεί να δικαιολογηθεί και η επίδρασή της στην μαθησιακή διαδικασία να είναι αμελητέα. Ωστόσο, αυτή η αραιότητα των γεγονότων τόσο στη χωρική όσο και στη χρονική διάσταση μας δίνει τη δυνατότητα να εφαρμώσουμε υποδειγματοληψία και στη χρονική διάσταση, η οποία θα συμπιέζει τα γεγονότα χωρίς καμία απώλεια πληροφοριών. Η κλίμακα της δειγματοληψίας στο χρόνο ορίζεται στο 0,001. Με αυτόν τον τρόπο, η διαδικασία εκμάθησης επιταχύνεται σημαντικά (η διαδικασία εκμάθησης του \textlatin{e-prop} είναι πιο χρονοβόρα από το τυπικό \textlatin{BPTT} όπως έχουν δηλώσει οι ίδιοι οι συγγραφείς \cite{Bellec2020}).

Το λογισμικό που χρησιμοποιείται για την εκπαίδευση είναι το \textlatin{Neko} \cite{Brasoveanu2020}, μια βιβλιοθήκη γλώσσας \textlatin{python} για κανόνες νευρομορφικής μάθησης, στην οποία έχει υλοποιηθεί και η μέθοδος \textlatin{e-prop}. Οι συγγραφείς έχουν υλοποιήσει την ανάπτυξη ενός δικτύου \textlatin{RNN} με μια ποικιλία υποστρωμάτων (\textlatin{backends}) όπως \textlatin{PyTorch} και \textlatin{TensorFlow}, κανόνες εκμάθησης όπως οι \textlatin{BPTT} και \textlatin{e-prop} και μια ποικιλία μοντέλων νευρώνων για το δίκτυο, τα οποία είναι το απλό επαναλαμβανόμενο κύτταρο, το επαναλαμβανόμενο \textlatin{LIF} και το προσαρμοστικό \textlatin{LIF ( A-LIF)} επαναλαμβανόμενο κύτταρο. Τα ακόλουθα πειράματα χρησιμοποιούν το υπόστρωμα \textlatin{PyTorch} και τη μέθοδο \textlatin{e-prop}, αφού το \textlatin{BPTT} έχει ήδη δοκιμαστεί. Ωστόσο, ο αλγόριθμος δεν αναπτύχθηκε έτσι ώστε να χειρίζεται δεδομένα από τη βιβλιοθήκη \textlatin{Tonic}. Το πρόβλημα ήταν στη μορφή δεδομένων των αντικειμένων \textlatin{Tonic}, τα οποία περιέχουν λίστες 4 στοιχείων, καθένα από τα οποία αντιπροσωπεύει τη χρονική σήμανση, τον άξονα \textlatin{Χ}, τον άξονα \textlatin{Υ} και την πολικότητα αντίστοιχα, ενώ η εκπαίδευση απαιτούσε την είσοδο να είναι ένας τένσορας 32\textlatin{x}32 = 1024 τριαδικών τιμών ως είσοδοι για κάθε έναν από τους νευρώνες του επιπέδου εισόδου. Η εφαρμογή του παραπάνω μετασχηματισμού έγινε για κάθε παρτίδα εισόδου (\textlatin{batch}) ξεχωριστά χρησιμοποιώντας τη βιβλιοθήκη \textlatin{Numpy} για ταχύτερη εκτέλεση. Ωστόσο, φαινόταν να έχει υψηλό υπολογιστικό κόστος. Πρέπει να σημειωθεί ότι μια τέτοια συμπεριφορά μπορεί να αποφευχθεί με τη χρήση δεδομένων \textlatin{DVS} σε πραγματικό χρόνο ή τη σωστή/κατάλληλα εφαρμοσμένη πλατφόρμα.

\subsection{Παραλλαγές Δικτύου}

Όπως και σε προηγούμενα πειράματα, εδώ επικεντρωθήκαμε σε τρεις τύπους χειριστικών κλάσεων. Πρώτον, ελήφθη υπόψη η προεπεξεργασία των δεδομένων έτσι ώστε να επιτευχθεί ένα ιδανικό μείγμα ταχύτητας και ακρίβειας, καθώς, όπως αναφέρθηκε προηγουμένως, το \textlatin{Neko} δεν έχει υλοποιηθεί για να χειρίζεται σύνολα δεδομένων \textlatin{Tonic}. Στη συνέχεια, η αρχιτεκτονική του δικτύου επιλέχθηκε σαν δοκιμαστική παράμετρος. Στο πείραμα αναπτύξαμε επαναλαμβανόμενα ΝΔΑ και δοκιμάσαμε διαφορετικά μεγέθη του δικτύου. Ο \textlatin{Neko} εφαρμόζει μια χρήσιμη παραμετροποίηση στον τύπο των νευρώνων και την κανονικοποίηση της πυροδότησης αιχμών του δικτύου, την οποία χρησιμοποιήσαμε στο πείραμά μας. Παρόλο που δεν προσφέρει τα εσωτερικά χαρακτηριστικά των νευρώνων όπως οι τιμές των σταθερών μεμβράνης και σύναψης ως παράμετροι (εκτός από το κατώφλι τάσης), δοκιμάσαμε διαφορετικές τιμές αυτών των παραμέτρων αλλάζοντας τον κώδικα των συναρτήσεων της βιβλιοθήκης. Τέλος, το ίδιο το εκπαιδευτικό σχήμα δίνει τρεις τρόπους σύνδεσης των νευρώνων εξόδου με τους επαναλαμβανόμενους για τη μεταφορά του μαθησιακού σήματος (κεφ. 4.3.6), το οποίο έχει υλοποιήσει ο \textlatin{Neko} και που χρησιμοποιήσαμε σε αυτό το πείραμα.

Ο τρόπος με τον οποίο ο \textlatin{Neko} υλοποιεί ΝΔΑ και την μάθηση \textlatin{E-prop} έχει ως αποτέλεσμα την ανάγκη ολόκληρης της χρονικής διάρκειας της εισόδου να τροφοδοτηθεί στο δίκτυο. Παρόλο που έχει καθιερωθεί στη θεωρία ότι η λειτουργία ενημέρωσης του δικτύου και η εκμάθηση μπορούν να γίνουν διαδικτυακά (χωρίς την ανάγκη για ολόκληρο το σήμα εισόδου στον χρόνο), προκειμένου η βιβλιοθήκη να εκμεταλλευτεί τους τένσορες του \textlatin{PyTorch} και την επιτάχυνση της \textlatin{GPU}, χρησιμοποιείται τένσορες που περιέχουν όλα τα χρονικά βήματα (ακόμη και αυτά που δεν έχουν αιχμές). Αυτό οδηγεί σε τεράστια ανάγκη για \textlatin{RAM} (μερικά \textlatin{TeraBytes} στη μνήμη), καθώς το σύνολο δεδομένων μπορεί να περιέχει εκατοντάδες χιλιάδες συμβάντα ανά χειρονομία, το καθένα πολλαπλασιασμένο με 16384 για να αντιπροσωπεύει το μέγεθος εισόδου. Για να ξεπεραστεί αυτό το πρόβλημα, μειώσαμε τη χρονική διάσταση κατά 1000, (αφού οι χαμηλότερες δοκιμασμένες τιμές είχαν ως αποτέλεσμα προβλήματα κατανομής μνήμης). Δεν προχωρήσαμε σε χωρική μείωση ή σε περαιτέρω χρονική δειγματοληψία, καθώς το σύνολο δεδομένων ήταν ήδη συμπιεσμένο σε όλο του το μέγεθος κατά συντελεστή 1000.

Το \textlatin{E-prop} δεν απαιτεί ξεδίπλωση του δικτύου στον χρόνο, αλλά όπως αναφέρθηκε προηγουμένως, το \textlatin{Neko} διατηρεί στη μνήμη προηγούμενα γεγονότα και καταστάσεις νευρώνων προκειμένου να ενημερώνεται και να παράγει εξόδους. Αυτό αυξάνει την πολυπλοκότητα χωρικής μνήμης του αλγορίθμου καθιστώντας τον, στην πράξη, ίδιο με το \textlatin{BPTT}. Ως αποτέλεσμα, το πιθανό μέγεθος του δικτύου περιορίστηκε για να μην ξεπεράσει τη χωρητικότητα της μνήμης, ενώ σε κατάλληλα εφαρμοσμένο \textlatin{e-prop} (όπως σε εφαρμογή σε νευρομορφική πλατφόρμα) τέτοια προβλήματα δεν θα εμφανιστούν. Δοκιμάσαμε δίκτυα με μεγέθη 64, 128, 256, 512 και 1024 νευρώνων πριν ξεπεράσουμε τη χωρητικότητα μνήμης, για 200 εποχές. Ο τύπος των νευρώνων επιλέχθηκε να είναι \textlatin{LIF} ή προσαρμοστικός \textlatin{LIF (A-LIF)} για το δίκτυο, ένας τύπος νευρώνα που οι προγραμματιστές κωδικοποίησαν βασισμένοι στην εργασία \cite{Bellec2020}. Οι νευρώνες \textlatin{A-LIF} έχουν μια υπερπαράμετρο, την προσαρμοστικότητα του νευρώνα, η οποία ελέγχει το ρυθμό που προσαρμόζεται ο νευρώνας - δηλαδή μαθαίνει να φτάνει στις κατάλληλες τιμές. Εφαρμόσαμε τρεις διαφορετικές τιμές 1.0, 2.0 και 4.0 για να μεταβούμε στη βέλτιστη τιμή. Όσον αφορά τις αποκρίσεις των νευρώνων, δοκιμάσαμε την αλλαγή των σταθερών χρόνου της μεμβράνης μεταξύ των τιμών 0,001, 0,02 και 0,2 και του ορίου τάσης της μεμβράνης μεταξύ 0,5 και 1,0. Το όριο τάσης δεν δοκιμάστηκε περαιτέρω με βάση την ιδέα ότι μια αιχμή εισόδου της τιμής 1 θα προκαλέσει την πυροδότηση του νευρώνα με ένα κατώτατο όριο τάσης, ενώ τα βάρη του δικτύου θα προσαρμοστούν και θα καταλήξουν σε αναλογικές τιμές για οποιαδήποτε διαφορά κατωφλίου τάσης μεμβράνης (δηλαδή τα βάρη απλά θα διπλασιαστούν για ένα διπλάσιο κατώφλι τάσης και το δίκτυο θα έχει την ίδια συμπεριφορά). Έχοντας αυτό κατά νου, δοκιμάσαμε με τις δύο προαναφερθείσες τιμές ώστε να μειώσουμε το μέγεθος της εκπαίδευσης και να συλλέξουμε πληροφορίες σχετικά με το αν η εικασία μας ισχύει ή όχι.

Στα πρώτα πειράματα που κάναμε, παρατηρήσαμε ότι το δίκτυο τείνει να σταθεροποιηθεί σε μια συχνότητα πυροδότησης περίπου 100 αιχμών ανά δευτερόλεπτο για κάθε νευρώνα. Έχοντας αυτό υπόψη, δοκιμάσαμε την επίδραση της παραμέτρου κανονικοποίησης στην απόδοση του δικτύου, ορίζοντάς την στις τιμές 80 και 150 με συντελεστές 0,00001 και 0,001 αντίστοιχα. Τέλος, εφαρμόσαμε κάθε υλοποιημένη λειτουργία \textlatin{e-prop} για τα βάρη του σήματος εκμάθησης όπως παρουσιάζεται στο κεφ. 4.3.6, τα οποία είναι τυχαία, συμμετρικά και προσαρμοστικά.

\begin{figure}[htp] %this figure will be at the right
    \centering
     \includegraphics[width=14cm]{Experiments/table.png}
    \caption{Παράμετροι προς βελτιστοποίηση και οι αντίστοιχες τιμές τους}
    \label{fig:representation-methods}
\end{figure}

Ο Πίνακας 6.2 δείχνει τον συνολικό χώρο αναζήτησης του πειράματός μας σχετικά με τον αλγόριθμο \textlatin{e-prop}. Ωστόσο, η πρόσβαση σε ολόκληρο τον χώρο αναζήτησης απαιτεί τη δοκιμή 360 διαφορετικών δικτύων. Δεδομένου ότι μια τέτοια εργασία θα απαιτούσε ένα τεράστιο χρονικό διάστημα, αντιμετωπίζουμε το πρόβλημα χρησιμοποιώντας τη μέθοδο που εφαρμόσαμε στο \textlatin{Decolle}. Δηλαδή, συγκρίναμε ζεύγη δικτύων με τροποποιημένες πολλαπλές μεταβλητές που δεν επηρεάζουν το ένα το άλλο για να συλλέξουμε πληροφορίες σε πολύ μικρότερο χώρο αναζήτησης (δηλαδή τρία δίκτυα δοκιμάζουν τρεις διαφορετικές μεταβλητές και κάθε ζεύγος παρουσιάζει πληροφορίες για κάθε μεταβλητή). Αν και, όπως αναφέρθηκε προηγουμένως, αυτή η μέθοδος δεν εγγυάται την τέλεια αναζήτηση (αφού η μη ζευγαρωμένη μεταβλητή μπορεί να επηρεάσει με απρόβλεπτο τρόπο την απόδοση), μειώνει μαζικά το χώρο αναζήτησης. Χρησιμοποιώντας αυτήν την ιδέα, αναπτύξαμε και δοκιμάσαμε 16 διαφορετικά δίκτυα θεωρώντας ότι είναι μια ανεκτή ισορροπία μεταξύ της συλλογής πληροφοριών και των δοκιμών.

\subsection{Αποτελέσματα}

Εκτελώντας τις παραπάνω διαμορφώσεις του δικτύου στο προτεινόμενο σύνολο δεδομένων, αποθηκεύουμε τις πληροφορίες της δοκιμής και της συνολικής απώλειας και την ακρίβεια του δικτύου για κάθε μία από τις 200 εποχές, καθώς και την πραγματική και προβλεπόμενη έξοδο της τελευταίας δοκιμαστικής λειτουργίας, όπως κάναμε στα προηγούμενα πειράματα (\textlatin{BPTT} και \textlatin{Decolle}). Με αυτές τις πληροφορίες σχεδιάζουμε τα αντίστοιχα γραφήματα (συνολική απώλεια, απώλεια δοκιμής και ακρίβεια) και τους πίνακες σύγχυσης και ακρίβειας ανά κλάση για την τελευταία εποχή. Με αυτήν την αναπαράσταση των συγκεντρωμένων πληροφοριών αναλύουμε τα παραγόμενα αποτελέσματα και συμπεραίνουμε την επίδραση κάθε υπερπαραμέτρου στο δίκτυο και την αιτία αυτής.

Μια γρήγορη παρατήρηση που μπορεί να γίνει από την απόδοση των δικτύων είναι ο συσχετισμός μεταξύ του αυξημένου αριθμού νευρώνων και της αστάθειας της εκπαίδευσης. Οι γραφικές παραστάσεις στο παρακάτω σχήμα παρουσιάζουν την ακρίβεια δύο πανομοιότυπων δικτύων με τη διαφορά του μεγέθους του κρυφού επιπέδου. Στην εικ. 6.25 το δίκτυο έχει 64 νευρώνες και στην εικ. 6.26, 512. Το μοτίβο ακολουθεί με δίκτυα που έχουν μεγαλύτερα μεγέθη κρυφών επιπέδων να αποδίδουν χειρότερα και να είναι πιο ασταθή από τα μικρότερα. Η χαοτική συμπεριφορά μπορεί να εξηγηθεί από την αδυναμία του δικτύου να αντλήσει σημαντικές πληροφορίες όταν πολλοί νευρώνες πρέπει να συνεργαστούν για τη δεδομένη ρύθμιση και εργασία. Επιπλέον, το \textlatin{E-prop} είναι κατασκευασμένο κατά τρόπο ώστε να μην λαμβάνεται υπόψη η μελλοντική επίδραση μιας αιχμής στο δίκτυο για την ελαχιστοποίηση των σφαλμάτων. Ένα μεγαλύτερο δίκτυο παράγει περισσότερες αιχμές συνολικά (όπως είναι εύκολα κατανοητό) και αυτές οι αιχμές, ενώ τελικά θα έχουν σημαντικό ρόλο στην έξοδο που θα παράγει το δίκτυο, δεν λαμβάνονται υπόψη. Αυτό μπορεί να υποδικνύει ότι η προτεινόμενη μέθοδος μπορεί να μην λειτουργεί καλά με μεγαλύτερα δίκτυα ή ακόμη και μεγάλα χρονικά δεδομένα.

\begin{figure}
\centering
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/E-prop/correct_perc/acc_01.png}
  \captionof{figure}{Ακρίβεια δικτύου με 64 νευρώνες στο κρυφό επίπεδο}
  \label{fig:test1}
\end{minipage}
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/E-prop/correct_perc/acc_02.png}
  \captionof{figure}{Ακρίβεια δικτύου με 512 νευρώνες στο κρυφό επίπεδο}
  \label{fig:test2}
\end{minipage}
\end{figure}

Τα μοντέλα των εικ. 6.25 και εικ. 6.26 έχουν αναπτυχθεί με προσαρμοζόμενους νευρώνες ολοκλήρωσης και πυροδότησης (\textlatin{A-LIF}). Η αλλαγή του τύπου νευρώνα του δικτύου σε \textlatin{LIF} παράγει αποτελέσματα όπως αυτά στην εικόνα 6.27. Το δίκτυο της εικ. 6.27 χρησιμοποιεί νευρώνες τύπου \textlatin{LIF} και πρέπει να σημειωθεί ότι έχει 512 νευρώνες στο κρυφό στρώμα. Αυτό δείχνει ένα σημαντικό αποτέλεσμα, δηλαδή, ότι οι νευρώνες τύπου \textlatin{LIF} μπορεί να παρουσιάζουν σταθερά αποτελέσματα παρά το μέγεθος του δικτύου. Αυτή η πρόσθετη σταθερότητα κάνει το δίκτυο να έχει αυξημένη ακρίβεια και λόγω του αυξημένου αριθμού νευρώνων του, να επιτυγχάνει υψηλή ακρίβεια από τις πρώτες κι όλας εποχές.

\begin{figure}[htp] %this figure will be at the right
    \centering
     \includegraphics[width=6cm]{plots/E-prop/correct_perc/acc_09.png}
    \caption{Δίκτυο με 512 νευρώνες \textlatin{LIF}}
    \label{fig:representation-methods}
\end{figure}

Συγκρίνοντας την κακή απόδοση του δικτύου που φαίνεται στο σχήμα 6.25 με ένα δίκτυο 512 νευρώνvn στο κρυφό στρώμα και χαμηλότερη τιμή προσαρμογής για τον νευρώνα \textlatin{A-LIF} (η ακρίβειά του φαίνεται στο σχήμα 6.28) φαίνεται ότι το ζήτημα της σταθερότητας προκαλείται πιθανώς από την τιμή προσαρμογής των νευρώνων. Υψηλές τιμές μπορεί να προκαλέσουν την υπερεκπαίδευση του αυξημένου αριθμού νευρώνων κατά τη διαδικασία επίτευξης ενός καθολικού ελάχιστου και να προκαλέσουν ταλάντωση του δικτύου. Ως αποτέλεσμα, μια προτεινόμενη μέθοδος θα μπορούσε να είναι ότι ένα μεγαλύτερο κρυφό επίπεδο απαιτεί μικρότερες τιμές προσαρμογής για το δίκτυο.

\begin{figure}[htp] %this figure will be at the right
    \centering
     \includegraphics[width=6cm]{plots/E-prop/correct_perc/acc_13.png}
    \caption{Αποτελέσματα της μείωσης των τιμών προσαρμογής των νευρώνων \textlatin{A-LIF} - σε σύγκριση με το σχήμα 6.25}
    \label{fig:representation-methods}
\end{figure}

Όσον αφορά τα χαρακτηριστικά των νευρώνων, μέσω του πειράματός μας, προέκυψε το ακόλουθο μοτίβο για τον αλγόριθμο \textlatin{e-prop}. Η μείωση των σταθερών μεμβράνης των νευρώνων στα κρυμμένα στρώματα και στα στρώματα εξόδου μπορεί να φαίνεται ότι αυξάνει ελαφρώς την απόδοση του δικτύου. Η αιτία αυτού του αποτελέσματος θα μπορούσε να είναι ότι, με την εφαρμοζόμενη χρονική μείωση, τα γεγονότα που τροφοδοτούνται στο δίκτυο είναι κοντά μεταξύ τους στην χρονική διάσταση. Αυτό θα μπορούσε να έχει ως αποτέλεσμα το δυναμικό της μεμβράνης των νευρώνων να μην αποφορτίζεται αρκετά γρήγορα και οι χρονικές πληροφορίες να μην συμβάλλουν τόσο πολύ. Η σταθερά της μεμβράνης ελέγχει τον ρυθμό πτώσης του δυναμικού της μεμβράνης του νευρώνα και η μείωση του προκαλεί ταχύτερο ρυθμό αποφόρτισης (καθώς ο αρνητικός αντίστροφος της μεταβλητής χρησιμοποιείται ως εκθέτης). Αυτό θα μπορούσε να έχει ως αποτέλεσμα οι αιχμές να είναι πιο αραιές στη χρονική διάσταση μέσα στο κρυφό στρώμα και στο δίκτυο να εξάγει καλύτερες πληροφορίες με αποτέλεσμα την απόδοση που παρουσιάζεται. Ομοίως, η αυξημένη τιμή των σταθερών της μεμβράνης, που σημαίνει ότι ο νευρώνας ενεργεί περισσότερο σαν ένας \textlatin{Integrate-and-Fire} νευρώνας (μοντέλο νευρώνα \textlatin{IF}) μπορεί να κατακλύσει το μοντέλο με αιχμές με αποτέλεσμα την αδυναμία του μοντέλου να μάθει επίσης σημαντικά πρότυπα. Ωστόσο, αυτές οι επηροές φαίνεται να είναι μικρές και ακόμη και οι αλλαγές σε τάξεις μεγέθους (πολλαπλασιασμός ή διαίρεση των παραμέτρων με το 10) δεν παίζουν κάποιο αισθητά σημαντικό ρόλο στην απόδοση του δικτύου. Το Σχ. 6.29 δείχνει τις τιμές ακρίβειας ενός τέτοιου δικτύου με αυξημένη σταθερά μεμβράνης νευρώνα, παρουσιάζοντας μια μορφή αστάθειας, αν και το δίκτυο υλοποιήθηκε με νευρώνες \textlatin{A-LIF} που αυτοί μπορεί να είναι η αιτία αυτής της συμπεριφοράς. Οι εφαρμοζόμενες αλλαγές στο κατώφλι τάσης δεν φαίνεται να επηρεάζουν το δίκτυο σημαντικά ή παρατηρήσιμα, μέσω αυτού του πειράματος, θετικά ή αρνητικά, και έτσι η προεπιλεγμένη τιμή του 1,0 θα μπορούσε να είναι μια αποδεκτή επιλογή.

\begin{figure}[htp] %this figure will be at the right
    \centering
     \includegraphics[width=6cm]{plots/E-prop/correct_perc/acc_14.png}
    \caption{Αποτελέσματα μετά από αύξηση των σταθερών χρόνου των νευρώνων}
    \label{fig:representation-methods}
\end{figure}

Η επιλογή για τη λεγόμενη "λειτουργία \textlatin{e-prop}" μεταξύ τυχαίου, συμμετρικού και προσαρμοστικού (που αφορά τα βάρη για το σήμα εκμάθησης), παίζει σημαντικό ρόλο στην απόδοση του δικτύου. Τα προσαρμοστικά βάρη φαίνεται να ακολουθούν το πρότυπο της "μάθησης μαζί με το δίκτυο". Τα δίκτυα που υλοποιήθηκαν με αυτήν τη λειτουργία είχαν μια ομαλή αυξανόμενη καμπύλη όσον αφορά την ακρίβεια (εικ. 6.28). Φαίνεται επίσης ότι χρειάζονται περισσότερες εποχές μέχρι να φτάσουν στη μέγιστη δυνατή ακρίβεια. Ο λόγος για αυτήν τη συμπεριφορά θα μπορούσε να είναι το γεγονός ότι το βάρος προσαρμογής χρειάζεται αρκετές εποχές για να φτάσει στις βέλτιστες τιμές και να βοηθήσει το δίκτυο να σταθεροποιηθεί. Η τυχαία λειτουργία ακολουθεί την ίδια συμπεριφορά (εικ.6.30) με τη διαφορά ότι χρειάζεται περισσότερες εποχές για να επιτευχθεί υψηλή ακρίβεια αφού τα βάρη των μαθησιακών σημάτων δεν αλλάζουν με την πάροδο του χρόνου για να αυξήσουν την ταχύτητα εκμάθησης. Τέλος, η συμμετρική φαίνεται να δείχνει την ταχύτερη ικανότητα εκμάθησης αφού, βάση του γραφήματος απώλειας δοκιμής στου σχήματος 6.31, ένα βελτιστοποιημένο συμμετρικό δίκτυου ρίχνει την απώλεια δοκιμής στο ελάχιστο από τις πρώτες εποχές σε σύγκριση με τη συνάρτηση απώλειας δοκιμής των προσαρμοστικών (6.32) ή τυχαίων (εικ. 6.33) δικτύων, αλλά η αύξηση της τιμής του που βλέπουμε να εμφανίζεται σε μετέπειτα εποχές μπορεί να οφείλεται σε υπερβολική προσαρμογή που η προσαρμοστική λειτουργία θα μπορούσε δυνητικά να την έχει αποφύγει. Θα πρέπει να σημειωθεί ότι η τυχαία λειτουργία του σχήματος (6.33) δείχνει επίσης μια μικρή αύξηση της απώλειας δοκιμής στις τελευταίες εποχές που μπορεί να προκαλείται από τον ίδιο λόγο που αναφέρθηκε προηγουμένως, ενώ η προσαρμοστική λειτουργία δεν φαίνεται να παρουσιάζει τέτοια συμπεριφορά.

\begin{figure}[htp] %this figure will be at the right
    \centering
     \includegraphics[width=6cm]{plots/E-prop/correct_perc/acc_04.png}
    \caption{Ακρίβεια δικτύου που δημιουργήθηκε με "τυχαία λειτουργία \textlatin{e-prop}"}
    \label{fig:representation-methods}
\end{figure}

\begin{figure}
\centering
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/E-prop/test_loss/test_loss_06.png}
  \captionof{figure}{Απώλεια δοκιμής της συμμετρικής λειτουργίας \textlatin{e-prop}}
  \label{fig:test1}
\end{minipage}
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/E-prop/test_loss/test_loss_09.png}
  \captionof{figure}{Απώλεια δοκιμής της προσαρμοστικής λειτουργίας \textlatin{e-prop}}
  \label{fig:test2}
\end{minipage}
\begin{minipage}{.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{plots/E-prop/test_loss/test_loss_04.png}
  \captionof{figure}{Απώλεια δοκιμής της τυχαίας λειτουργίας \textlatin{e-prop}}
  \label{fig:test1}
\end{minipage}
\end{figure}

Συνοψίζοντας, οι πιο σημαντικές πληροφορίες που αντλήσαμε από αυτό το πείραμα είναι το γεγονός ότι για δίκτυα μεγάλου μεγέθους ο αλγόριθμος \textlatin{e-prop} παρουσιάζει ζητήματα αστάθειας. Η αιτία αυτού θα μπορούσε να είναι το γεγονός ότι κατά τη διάρκεια της εκμάθησης η μελλοντική επιρροή των αιχμών δεν λαμβάνεται υπόψη και τα μεγάλα δίκτυα, που παράγουν αυξημένο αριθμό αιχμών, επηρεάζονται σημαντικά. Τα ζητήματα αστάθειας θα μπορούσαν να είναι πιο δραματικά με μια ακατάλληλη επιλογή της τιμής προσαρμογής για τον νευρώνα \textlatin{ALIF}, αλλά προσεκτικά επιλεγμένες τιμές μπορεί να ξεπεράσουν τέτοια προβλήματα, ενώ οι νευρώνες \textlatin{LIF} δεν φαίνεται να παρουσιάζουν αυτά τα ζητήματα σε τέτοιο. Η επιλογή της λειτουργίας \textlatin{e-prop} είναι πολύ σημαντική και όπως φαίνεται το τυχαίο αποδίδει χειρότερα από τα άλλα δύο. Η συμμετρική λειτουργία φτάνει σε υψηλή ακρίβεια νωρίς, αλλά φαίνεται να υποφέρει από υπερβολική προσαρμογή, ενώ η προσαρμοστική λειτουργία, ενώ χρειάζεται περισσότερες εποχές για να συγκλίνει στο μέγιστο, φαίνεται να ξεπερνά το πρόβλημα της υπερεκπαίδευσης μέσω της προσαρμογής των βαρών. Τα χαρακτηριστικά των εσωτερικών νευρώνων δεν φαίνεται να επηρεάζουν σημαντικά την απόδοση του δικτύου.

\chapter{Σύνοψη}
Φτάσαμε στο τελευταίο τμήμα της διπλωματικής μας εργασίας. Αυτή η διατριβή είναι το αποτέλεσμα μηνών έρευνας, συλλογής, επεξεργασίας και προσαρμογής πληροφοριών για την προσέγγιση προβλημάτων με τη χρήση νευρωνικών δικτύων και ώστε να μας δωθεί η ικανότητα ταξινόμησης δεδομένων που έχουν καταγραφεί από αισθητήρες δυναμικής όρασης. Αρχικά, ήταν απαραίτητο να κατανοήσουμε τα θεμελιώδη στοιχεία του ανθρώπινου εγκεφάλου: νευρώνες, συνάψεις και άλλα δομικά στοιχεία που ενώνονται για να χτίσουν μαζικές δομές δικτύου που υποστηρίζουν γνωστικές διαδικασίες. Απαιτήθηκε επίσης να κατανοήσουμε τις εξισώσεις που χαρακτηρίζουν τους νευρώνες (μοντέλα νευρώνων) και πώς η δυνατότητα δημιουργίας αιχμών τους επιτρέπει να στέλνουν πληροφορίες μέσω συνάψεων.

Η κατανόηση της βιολογίας στην οποία βασίζονται τα νευρωνικά δίκτυα αιχμών ήταν εξαιρετικά επωφελής για την συνειδητοποίηση της τεράστιας πολυπλοκότητας του ανθρώπινου εγκεφάλου, ότι τα νευρικά δίκτυα είναι απλώς μια απλοποίηση αυτού που έχει επιτύχει η εξέλιξη και ότι έχουμε ακόμα πολύ δρόμο να διανύσουμε πριν επιτύχουμε τεχνητή νοημοσύνη σε ανθρώπινο επίπεδο . Ωστόσο, η εμβάθυνση στην έρευνα του εγκεφάλου μπορεί να οδηγήσει σε νέες προόδους στην έρευνα μηχανικής μάθησης με την καλύτερη κατανόηση των συστημάτων του ανθρώπινου εγκεφάλου που βοηθούν στην οπτική επεξεργασία και μάθηση. Οι νευρωνικές συνθέσεις και οι ταλαντώσεις των νευρώνων, σε συνδυασμό με την έννοια της πλαστικότητας, είναι δύο βασικές πτυχές του ανθρώπινου εγκεφάλου που βρήκαμε ενδιαφέρουσες.

Οι νευρωνικές συνθέσεις βελτιώνουν την αποτελεσματικότητα, τη δυναμική και την ευελιξία επεξεργασίας πληροφοριών και δεν έχει βρεθεί καμία ερευνητική δημοσίευση που να χρησιμοποιεί νευρωνικές συνθέσεις για κάποιο πρόβλημα μηχανικής μάθησης. Όπως αποδείχθηκε προηγουμένως, οι ταλαντώσεις μπορούν να χρησιμοποιηθούν για τον έλεγχο της δραστηριότητας ενός δικτύου ή πολλαπλών δικτύων, τον συγχρονισμό (αισθητηριακή είσοδος από δύο αυτιά στην κουκουβάγια) και τη σωστή δραστηριότητα των νευρώνων (σακκαδική κίνηση των ματιών ώστε να δημιουργείτε οπτική απεικόνιση του περιβάλλοντος του οργανισμού). Σε ορισμένες δημοσιεύσεις, οι ταλαντώσεις έχουν χρησιμοποιηθεί για την παραγωγή εσωτερικής δραστηριότητας δικτύου για προβλήματα όπως η χωρική πλοήγηση και η ομιλία \cite{vincent2020}, καθώς και σε συνδυασμό με το \textlatin{STDP} \cite{deco2009} ως μια γενικότερη μέθοδο εκμάθησης.

Ως συγγραφείς αυτής της διατριβής, ελπίζουμε να εμπνεύσουμε άλλους ακαδημαϊκούς να εμβαθύνουν στην έρευνα του εγκεφάλου, προκειμένου να βρουν τρόπο να ενσωματώσουν τις συνθέσεις νευρώνων και τις ταλαντώσεις σε προβλήματα μηχανικής μάθησης, οι οποίες θα μπορούσαν να οδηγήσουν σε καλύτερες επιδόσεις. Επιπλέον, θεωρήσαμε ότι ήταν σημαντικό να μάθουμε για τα υπάρχοντα νευρομορφικά υπολογιστικά συστήματα καθώς και την τρέχουσα πορεία της έρευνας σε νευρομορφικά συστήματα που μπορούν να χρησιμοποιηθούν ως πλατφόρμες για τη μοντελοποίηση της δραστηριότητας ενός νευρωνικού δικτύου. Ήταν επίσης σημαντικό να κατανοήσουμε τις θεμελιώδεις αρχές λειτουργίας των αισθητήρων δυναμικής όρασης προκειμένου να κάνουμε την κατάλληλη προεπεξεργασία στο \textlatin{dataset}. Δεδομένου του περιορισμένου χρόνου, αποφασίσαμε να επικεντρωθούμε και να πειραματιστούμε με υπάρχοντες κανόνες μάθησης για την ταξινόμηση χειρονομιών χρησιμοποιώντας το σύνολο δεδομένων \textlatin{DVS Gestures} της \textlatin{IBM}.
\bibliographystyle{IEEEtran}
\selectlanguage{english}
\bibliography{library}
\end{document}
